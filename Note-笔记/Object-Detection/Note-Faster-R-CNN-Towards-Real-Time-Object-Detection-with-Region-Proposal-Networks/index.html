<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css" integrity="sha256-/4UQcSmErDzPCMAiuOiWPVVsNN2s3ZY/NsmXNcj0IFc=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"xzhewei.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.15.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Abstract 目前先进的目标检测网络依赖区域推荐算法来推测目标位置。SSPnet和Fast R-CNN在减少网络运行时间的基础上发现，区域推荐是计算瓶颈。本文提出Region Proposal Network（RPN）区域推荐网络与检测网络的共享全图卷积特征，从而实现接近0成本的区域推荐。RPN作为一个全卷积网络能够在图像各个位置预测物体的边界和得分。RPN采用端到端的训练方法实现高质量的区域">
<meta property="og:type" content="article">
<meta property="og:title" content="论文笔记《Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks》">
<meta property="og:url" content="http://xzhewei.com/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks/index.html">
<meta property="og:site_name" content="Zavix | Intellisoftx">
<meta property="og:description" content="Abstract 目前先进的目标检测网络依赖区域推荐算法来推测目标位置。SSPnet和Fast R-CNN在减少网络运行时间的基础上发现，区域推荐是计算瓶颈。本文提出Region Proposal Network（RPN）区域推荐网络与检测网络的共享全图卷积特征，从而实现接近0成本的区域推荐。RPN作为一个全卷积网络能够在图像各个位置预测物体的边界和得分。RPN采用端到端的训练方法实现高质量的区域">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2017-03-30T19:14:08.000Z">
<meta property="article:modified_time" content="2023-03-30T11:43:21.423Z">
<meta property="article:author" content="Zavix">
<meta property="article:tag" content="Object Detection">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://xzhewei.com/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://xzhewei.com/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks/","path":"Note-笔记/Object-Detection/Note-Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks/","title":"论文笔记《Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks》"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>论文笔记《Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks》 | Zavix | Intellisoftx</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Zavix | Intellisoftx</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">IntelliSoftx</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a></li><li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>公益 404</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-number">1.</span> <span class="nav-text">1 Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Related-Work"><span class="nav-number">2.</span> <span class="nav-text">2 Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E5%8C%BA%E5%9F%9F%E6%8E%A8%E8%8D%90%E7%BD%91%E7%BB%9C"><span class="nav-number">3.</span> <span class="nav-text">3 区域推荐网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B3%E7%A7%BB%E4%B8%8D%E5%8F%98Anchors"><span class="nav-number">3.1.</span> <span class="nav-text">平移不变Anchors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8C%BA%E5%9F%9F%E6%8E%A8%E8%8D%90%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">3.2.</span> <span class="nav-text">区域推荐的损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96"><span class="nav-number">3.3.</span> <span class="nav-text">优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8C%BA%E5%9F%9F%E6%8E%A8%E8%8D%90%E5%92%8C%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%85%B1%E4%BA%AB%E5%8D%B7%E7%A7%AF%E7%89%B9%E5%BE%81"><span class="nav-number">3.4.</span> <span class="nav-text">区域推荐和目标检测共享卷积特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="nav-number">3.5.</span> <span class="nav-text">实现细节</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E5%AE%9E%E9%AA%8C"><span class="nav-number">4.</span> <span class="nav-text">4 实验</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-number">4.1.</span> <span class="nav-text">实验结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VGG-16%E7%9A%84%E6%A3%80%E6%B5%8B%E7%B2%BE%E5%BA%A6%E5%92%8C%E8%BF%90%E8%A1%8C%E6%97%B6%E9%97%B4"><span class="nav-number">4.2.</span> <span class="nav-text">VGG-16的检测精度和运行时间</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E6%9E%90IoU%E5%92%8CRecall%E5%85%B3%E7%B3%BB"><span class="nav-number">4.3.</span> <span class="nav-text">分析IoU和Recall关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#one-stage%E6%A3%80%E6%B5%8B-vs-two-stage%E5%8C%BA%E5%9F%9F%E6%8E%A8%E8%8D%90-%E6%A3%80%E6%B5%8B"><span class="nav-number">4.4.</span> <span class="nav-text">one-stage检测 vs two-stage区域推荐+检测</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E7%BB%93%E8%AE%BA"><span class="nav-number">5.</span> <span class="nav-text">5 结论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">6.</span> <span class="nav-text">Reference</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zavix</p>
  <div class="site-description" itemprop="description">一个平平无奇的AI从业者。</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">32</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://xzhewei.com/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zavix">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zavix | Intellisoftx">
      <meta itemprop="description" content="一个平平无奇的AI从业者。">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="论文笔记《Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks》 | Zavix | Intellisoftx">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          论文笔记《Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks》
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-03-31 03:14:08" itemprop="dateCreated datePublished" datetime="2017-03-31T03:14:08+08:00">2017-03-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-30 19:43:21" itemprop="dateModified" datetime="2023-03-30T19:43:21+08:00">2023-03-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Note-%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">Note 笔记</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Note-%E7%AC%94%E8%AE%B0/Object-Detection/" itemprop="url" rel="index"><span itemprop="name">Object Detection</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p><strong>Abstract</strong></p>
<p>目前先进的目标检测网络依赖区域推荐算法来推测目标位置。SSPnet和Fast R-CNN在减少网络运行时间的基础上发现，区域推荐是计算瓶颈。本文提出Region Proposal Network（RPN）区域推荐网络与检测网络的共享全图卷积特征，从而实现接近0成本的区域推荐。RPN作为一个全卷积网络能够在图像各个位置预测物体的边界和得分。RPN采用端到端的训练方法实现高质量的区域推荐，Fast R-CNN作为检测器。通过简单的联合优化，RPN和Fast R-CNN可以贡献卷积特征进行训练。对于VGG16模型，我们的检测系统在GPU的速度能够实现5fps。在每个图片提取300个候选区的情况下 ，PASCAL VOC 2007上精度为73.2% mAP，PASCAL VOC  2012精度为70.4% mAP。</p>
<span id="more"></span>

<ul>
<li>Faster R-CNN</li>
<li>微软</li>
<li>v-shren, kahe, rbg, jiansun</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ShaoqingRen/faster_rcnn">code</a></li>
<li><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">pdf</a></li>
</ul>
<p><strong>BibTex</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">@inproceedings&#123;ren2015faster,</span><br><span class="line">  title=&#123;Faster r-cnn: Towards real-time object detection with region proposal networks&#125;,</span><br><span class="line">  author=&#123;Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian&#125;,</span><br><span class="line">  booktitle=&#123;Advances in neural information processing systems&#125;,</span><br><span class="line">  pages=&#123;91--99&#125;,</span><br><span class="line">  year=&#123;2015&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>理解</strong></p>
<p>本文主要提出了一个two-stage的目标检测框架，将原有的目标检测分为区域推荐+目标检测两个步骤，同时公用特征提取部分的卷积操作。RPN网络区别于语义分割为，RPN提供了bounding box，同时在一个图像尺度下进行bounding box回归，避免了之前采用图像金字塔的方式，极大的减少了计算量。</p>
<p>同时该方法还是一个通用的框架，不限于具体使用的网络模型，例如ZF，VGG都行。只是在最后一个卷积层上分别采用不同的处理方式。一边采用两层卷积层进行进一步特征提取，然后做bounding box回归和目标得分，另一边基于提出的bunding box对原卷积层特征做全连接的分类处理。</p>
<p>同时，由于该架构基于统一的卷积网络，因此通过GPU加速能够实现快速检测，而传统方法SS只能基于CPU运算，其效率远远低于GPU。</p>
<p>**RPN-BF那篇文章主要采用RPN的思想来推荐候选区域，但是对卷积层的共享这部分没有采用，主要原因是他想采用boosting进行困难负样本提取。那是否有方法可以在卷积神经网络的架构下进行困难负样本提取呢？如果可以那基于RPN+ClassifyNet的架构下，同时利用了最好的神经网络模型可能实现更好的结果，例如F-DNN，就是利用了GoogLeNet和ResNet实现了更好的精度。RPN-Boosted DNN架构 **</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>目前采用区域推荐方法和基于区域的卷积神经网络（R-CNN）实现了较好的目标检测效果。尽管基于区域的CNN计算开销巨大，但是通过共享卷积特征能够显著的降低计算量。目前Fast R-CNN在不考虑区域推荐的情况下能够实现接近实时的检测性能。因此区域推荐的计算开销称为目前检测系统的主要瓶颈。</p>
<p>区域推荐方法一般只需要inexpensive特征和简单的推理框架，但是相比于检测器效率仍然很低</p>
<ul>
<li>Selective Search是一种流行的框架，其对低级的特征进行融合。但是相比于检测网络，要慢一个数量级，2s&#x2F;img on CPU</li>
<li>EdgeBoxes是目前平衡精度和计算时间最后的方法，0.2s&#x2F;img</li>
</ul>
<p>但是需要注意到基于CNN的区域推荐网络受益于GPU优势，而一般的区域推荐方法采用CPU计算，因此计算时间不具可比性。因此在GPU上实现区域推荐算法，能够有效的提升运行效率。但是仅仅在GPU上实现区域推荐方法，会忽略计算贡献的重要性。</p>
<p>本文，我们显示了一种高效的解决方法，使得区域推荐的计算成本几乎为0。RPN网络与目标检测网络贡献卷积层，因此在检测任务上，区域推荐的边际成本只有10ms&#x2F;img。</p>
<p>我们在卷积特征上构造RPN，增加了两个卷积层：一个将卷积图每个位置编码到一个256维的特征向量；另一个是在每个卷积图位置输出一个是目标的得分和回归k个候选区域边界（一般k&#x3D;9）。</p>
<p>RPN是一种全卷积网络，可以使用端到端的训练方法。为了联合RPN和Fast R-CNN，我们提出一个简单的训练框架，在fine-tuning 区域推荐任务和fine-tuning目标检测任务之间切换。这个方法能够很快收敛并得到共用的卷积特征。</p>
<p>我们在PASCAL VOC检测任务上评估我们的方法的精度比Selective Search + Fast R-CNN要高。并且计算效率非常高。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h2><p>最近的论文提出了几种专门用于定位和矩形框估计的方法。</p>
<ul>
<li>OverFeat，训练fc层用于预测box的坐标。然后将fc层变为卷积层用于检测特定类别的目标</li>
<li>MultiBox，fc层生成推荐区域，同时预测多个（800）boxes，然后用R-CNN进行目标检测。区域推荐网络应用于单一图像，或者多个大截取区域</li>
</ul>
<p>共享卷积计算的方法正却来越受关注：</p>
<ul>
<li>OverFear计算图像金字塔的卷积特征用于分类、定位、检测</li>
<li>SPP在共享卷积特征图上进行自适应尺寸池化，提升基于区域的目标检测和语义分割</li>
<li>Fast R-CNN能够在共享卷积特征上进行端到端训练，提升精度和速度</li>
</ul>
<h2 id="3-区域推荐网络"><a href="#3-区域推荐网络" class="headerlink" title="3 区域推荐网络"></a>3 区域推荐网络</h2><p>RPN的输入为一个图像（任何尺寸），输出为一系列的矩形目标区域和每个区域的得分。由于我们的目的是让RPN和Fast R-CNN共享计算资源，因此我们假设两个网络共用一些卷积层。在我们的实验组，我们评估了ZF模型[^23] (5个可共享卷积层)，VGG模型（13个可共享卷积层）。</p>
<p>![](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster</a> R-CNN Towards Real-Time Object Detection with Region Proposal Networks_Attachments\f1.png)</p>
<p>为了生成区域推荐，我们在最后一个共享卷积层输出的特征图上使用一个小网络进行滑动。该网络是一个$n \times n$的空间窗口。每次滑动窗口都映射为一个低维向量（256-d对ZF，512-d对VGG）。每个向量输入给两个平行的全连接层：一个box回归层（reg），一个box分类层（cls）。本文中n&#x3D;3，其有效的感知域在输入图像上是很大的（171pixel for ZF，228pixel for VGG）。滑动窗口通过256&#x2F;512个$n \times n$的卷积层实现，后面两个分别用$1 \times 1$的卷积层实现。在$n \times n$后使用ReLU。</p>
<h3 id="平移不变Anchors"><a href="#平移不变Anchors" class="headerlink" title="平移不变Anchors"></a>平移不变Anchors</h3><ul>
<li>每个滑窗的位置，我们同时预测k个区域推荐</li>
<li>reg层输出4k个坐标</li>
<li>cls层输出2k个分数，分别估计是目标和不是目标的概率</li>
<li>k个推荐区域参数化为k个参考box，称为anchors</li>
<li>每个anchors的中心是否在滑窗中心，与尺度和宽高比相关</li>
<li>我们采用3种尺度、3中宽高比，因此有9中anchors</li>
<li>对于$W \times N$的特征图，总共有WHk个anchors</li>
<li>我们的方法是平移不变的</li>
</ul>
<p>作为对比：</p>
<ul>
<li>MultiBox使用k-means生成800种anchors，不具有平移不变性</li>
<li>MultiBox需要$（4+1）\times 800$-维输出层（由于不具有平移不变形，不太理解为什么）</li>
<li>我们的方法只需要$(4+2)\times 9$-维输出</li>
<li>我们的方法需要的参数更少，因此在小数据集上的过拟合风险更小</li>
</ul>
<h3 id="区域推荐的损失函数"><a href="#区域推荐的损失函数" class="headerlink" title="区域推荐的损失函数"></a>区域推荐的损失函数</h3><p>训练RPN网络使用二元类标。标记为positive的anchors需要满足两个条件：1）与GT的IoU最高；2）IoU至少为0.7以上。因此一个Ground Truth可能会给多个anchors分配positive类标。当IoU低于0.3时标为negative。没有类标的anchors不进行训练。</p>
<p>对于这样多任务的损失函数，一个图片的损失函数定义为：</p>
<p>$$L({p_i},{t_i})&#x3D;\frac{1}{N_{cls}}\sum_i L_{cls}(p_i,p_i^*)+\lambda\frac{1}{N_{reg}}\sum_i p_i^<em>L_{reg}(t_i,t_i^</em>)$$</p>
<p>$i​$ 是mini-batch中第i个anchor，$p_i​$是预测类标，$p_{i}^*​$ 是gt标签（0&#x2F;1），$t_i​$ 是预测的bb的坐标，$t_i^*​$ 是gt。 $L_{cls}​$ 是分类损失函数， $L_{reg}&#x3D;R(t_i-t_i^{*})​$ 是bb回归损失函数,$R​$ 是鲁棒损失函数（smooth L1）[^5]。 $p_i^{*} L_{reg}​$ 表示只考虑positive anchor的回归损失， $\lambda​$ 是平衡参数，目前设为10。 $N_{cls}​$ 跟mini-batch有关，目前为256， $N_{reg}​$ 大约为2400。</p>
<p>为了进行回归，采用参数化坐标[^6]：</p>
<p>$$<br>t_x&#x3D;(x-x_a)&#x2F;w_a, t_y&#x3D;(y-y_a)&#x2F;h_a, t_w&#x3D;log(w&#x2F;w_a),t_h&#x3D;log(h&#x2F;h_a)<br>$$</p>
<p>$$<br>t_x^*&#x3D;(x^*-x_a)&#x2F;w_a, t_y^*&#x3D;(y^*-y_a)&#x2F;h_a, t_w&#x3D;log(w^*&#x2F;w_a),t_h&#x3D;log(h^*&#x2F;h_a)<br>$$</p>
<p>$x,y,w,h$是box的中心和宽高，$x,x_a,x^*$分别是预测box，anchor box，Ground truth。可以看作是从一个anchor box到附近的Ground truth box的bounding box回归。</p>
<p>在[^5][^7]中，在任意尺寸区域的池化特征上进行bb回归，所有的区域共享回归权重。我们的方法，使用相同的尺寸的特征图，为了适应变化的尺寸，我们为k种bb都学习一个回归器，不共享权重。这样在固定尺寸特征图上我们仍然可以预测不同尺寸的bb。</p>
<h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>RPN是一个全卷积神经网络，可以通过反向传播和随机梯度下降实现端到端训练。我们遵循“image-centric”[^5]训练该网络。一张图片是一个mini-batch，但是由于负样本比正样本多，会造成数据偏移。因此我们在一个mini-batch里采用随机的选择256个anchor进行训练，正负样本1:1。如果正样本少于128，则由负样本补齐。</p>
<p>参数的初始化：</p>
<ul>
<li>新增的卷积层，使用随机高斯分布（均值为0，标准差为0.01）进行参数初始化</li>
<li>其他网络层（共享卷积层）使用ImageNet分类任务预训练的参数，一般都这么做</li>
<li>对ZF我们fine-tune所有层，对VGG只fine-tune conv3_1及以上的网络，来节省内存</li>
<li>学习率：前60k mini-batch是0.001，后20k是0.0001</li>
<li>动量为0.9，权重衰减0.0005（<strong>这个是caffe上标准做法</strong>）[^11]</li>
</ul>
<h3 id="区域推荐和目标检测共享卷积特征"><a href="#区域推荐和目标检测共享卷积特征" class="headerlink" title="区域推荐和目标检测共享卷积特征"></a>区域推荐和目标检测共享卷积特征</h3><p>至此我们描述了如何训练一个区域推荐网络，但是没有考虑目标检测CNN如果利用这些推荐区域。下面我们将描述采用Fast R-CNN作为目标检测网络如何训练。</p>
<p>RPN和Fast R-CNN如果分开训练，则卷积层的参数将差别很大，因此为了共享卷积层，我们需要同时进行训练。然而，单一网络同时包含RPN和Fast R-CNN进行反向传播优化并不容易。因为Fast R-CNN的训练需要依赖固定的目标区域，如果在训练过程中目标的生成机制也在变化，能否收敛则不得而知。联合优化是一个非常有趣的研究方向，<strong>我们采取更务实的4步训练算法，通过轮换寻优（alternating optimization ）来训练共享特征。</strong></p>
<ol>
<li>RPN使用ImageNet预训练模型，端到端的fine-tune训练区域推荐任务</li>
<li>使用RPN推荐的区域训练Fast R-CNN（之前也是用ImageNet预训练）</li>
<li>使用Fast R-CNN的参数来初始化RPN，并且固定共享卷积层参数，只fine-tune RPN增加的卷积层参数。</li>
<li>固定共享卷积层，fine-tune Fast R-CNN的fc层</li>
</ol>
<p>通过上面四步，两个网络就共用了卷积层，形成统一的网络。</p>
<h3 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h3><p>图片的设置</p>
<ul>
<li>训练和测试区域推荐和目标检测网络，均使用单一尺度图像</li>
<li>缩放图像到同一尺寸，短边600 pixels</li>
<li>提取多尺度特征可能会改善精度，但是会损失效率</li>
<li>最后一层卷积层的步长反推到原图上是16 pixels，在这种情况下也能得到较好的结果</li>
</ul>
<p>Anchor设置</p>
<ul>
<li>3种尺度的面积，$128^2,256^2,512^2$，3种宽高比1:1，1:2，2:1（这里好像没有什么依据）</li>
<li>我们发现anchor box的面积比感知野大也是可以的，这样如果一个目标只有中间部分出现，粗略的估计目标的其他范围也是可行的*(即可以估计被遮挡目标的实际大小)*</li>
<li>因此，我们的方法不需要多尺度特征和滑窗来预测大区域</li>
<li>下表是使用ZF net学习的推荐区域平均尺寸</li>
</ul>
<p>![](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster</a> R-CNN Towards Real-Time Object Detection with Region Proposal Networks_Attachments\t1.png)</p>
<p>Anchor超过图像边界的处理</p>
<ul>
<li>忽略所有超过边界的anchor</li>
<li>典型的1000x600的图像有大约20k（60*40*9）个anchor，排除超过边界的，剩下越6k个用于训练</li>
<li>如果不排除那些anchor，很难正确评估损失值，而不收敛</li>
<li>在测试时，RPN仍会产生跨边界的anchor，我们按图片边缘进行裁剪</li>
</ul>
<p>Anchor的NMS</p>
<ul>
<li>RPN的推荐区域有高度重叠，为了减少，基于cls得分进行NWS</li>
<li>IoU采用0.7，这样剩下大概2k个区域</li>
<li>NMS不会损失精度，同时极大减少计算量</li>
<li>我们的区域取前N个，训练时使用2K个，测试时使用另一个数量</li>
</ul>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4 实验"></a>4 实验</h2><ul>
<li>数据集 PASCAL VOC 2007</li>
<li>训练5k，测试5k，20个目标类别</li>
<li>ImageNet 预训练，fast版ZF，5个卷积，3个fc；VGG-16，13个卷积，3个fc</li>
<li>mAP作为评价，平均精度</li>
</ul>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>![](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster</a> R-CNN Towards Real-Time Object Detection with Region Proposal Networks_Attachments\t2.png)</p>
<p>首先，实验采用自身提出的Proposal训练分类网络，使用RPN+ZF共享卷积层的方法mAP为59.9%</p>
<p>Ablation Experiments 分解实验：</p>
<ul>
<li>卷积层共享的影响：未进行卷积层共享的精度降低，可能的原因是共享卷积层后，检测器fine-tuned的特征用于RPN fine-tune，区域提取的质量提高</li>
<li>RPN对Fast R-CNN训练的影响，使用SS的Proposal对分类器训练，然后使用RPN对分类器测试。从结果看训练和测试的Proposal的一致性还是很重要的。另外6k个Proposal与100个Proposal的差异不大，说明NMS对精度的影响是很小的。</li>
<li>cls和reg的作用。测试时没有cls，则没有NMS和排序，随机选择N个Proposal，1K时差别不大，但是100时差别很大，说明cls得分对选择Proposal是有用的。测试时没有reg，即Proposal变成了anchor box，精度也下降了，说明高质量的Proposal主要取决于位置回归。</li>
<li>其他更好的网络模型对RPN的影响。我们使用VGG-16训练RPN，进行Proposal，但是分类器仍然使用原来的SS+ZF，精度从56.8%提升到59.2%。说明RPN+VGG提供的Proposal效果更好。</li>
</ul>
<h3 id="VGG-16的检测精度和运行时间"><a href="#VGG-16的检测精度和运行时间" class="headerlink" title="VGG-16的检测精度和运行时间"></a>VGG-16的检测精度和运行时间</h3><p>![](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster</a> R-CNN Towards Real-Time Object Detection with Region Proposal Networks_Attachments\t3.png)</p>
<p>![](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster</a> R-CNN Towards Real-Time Object Detection with Region Proposal Networks_Attachments\t4.png)</p>
<ul>
<li>表2和表3分别是在PASCAL VOC2007和PASCAL VOC2012上的结果</li>
<li>分类器均为Fast R-CNN，主要比较RPN的模型差异</li>
<li>训练集：07表示采用VOC 2007训练集，12表示采用VOC 2012做训练集，07+12表示用上面两个一起做训练集，07++12表示用07的训练和测试集+12的训练集做为训练集</li>
<li>训练时Proposal都是2k</li>
<li>可以发现RPN+VGG比SS更精确，且速度更快</li>
</ul>
<p>![](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster</a> R-CNN Towards Real-Time Object Detection with Region Proposal Networks_Attachments\t5.png)</p>
<p>表4总结了目标检测器的运行时间</p>
<ul>
<li>运行平台K40，SS运行在CPU上</li>
<li>SS的时间大约为1-2s</li>
<li>Fast R-CNN基于VGG-16模型在2k SS上需要320ms</li>
<li>我们的整个系统在VGG上只需198ms</li>
<li>region-wise包括：NMS, pooling, fc, softmax</li>
<li>由于共享了卷积层Proposal的计算只有10ms</li>
<li>由于更少的Proposal数量，所以region-wise时间更少</li>
<li>ZF模型更简单，因此可实现17fps的效率</li>
</ul>
<h3 id="分析IoU和Recall关系"><a href="#分析IoU和Recall关系" class="headerlink" title="分析IoU和Recall关系"></a>分析IoU和Recall关系</h3><p>![](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster</a> R-CNN Towards Real-Time Object Detection with Region Proposal Networks_Attachments\f2.png)</p>
<p>图2：Recall和IoU重叠率的关系</p>
<ul>
<li>proposal数量分别为300，1000，2000</li>
<li>RPN对于proposal数量减少不敏感，主要归功于cls</li>
<li>SS和EB会随着proposal数量的减少迅速下降</li>
</ul>
<h3 id="one-stage检测-vs-two-stage区域推荐-检测"><a href="#one-stage检测-vs-two-stage区域推荐-检测" class="headerlink" title="one-stage检测 vs two-stage区域推荐+检测"></a>one-stage检测 vs two-stage区域推荐+检测</h3><p>![](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster</a> R-CNN Towards Real-Time Object Detection with Region Proposal Networks_Attachments\t6.png)</p>
<p>OverFeat[^18]提出一种在卷积特征图上滑窗通过回归器和分类器进行检测。该方法是one-stage，特定类别的检测流程，我们的方法是two-stage级联不定类别推荐和特定类别检测。OverFeat中，region-wise特征从由一个宽高比的尺度金字塔滑窗获得。RPN中特征从一个方向滑窗和相对不同宽高比、尺度的anchor预测区域。虽然两个方法都用到了滑窗，但是RPN中区域推荐任务只是第一个阶段，Fast R-CNN用来进一步细化他们。在第二阶段里，region-wise特征在推荐的box中进一步池化，而从能够更加专注与该区域的特征。我们详细这将导致更精确的检测。</p>
<p>为了比较，one-stage和two-stage系统，我们进行了大量实验。如表5所示。5 scales是图像金字塔的尺度。结果看two-stage的效果更好，且计算更快。</p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h2><p>我们提出了一种RPN用于高效精确的区域推荐。通过共享卷积层特征，RPN可以做到几乎计算开销0成本。我们的方法可以一个5-17fps的深度学习目标检测系统。学习得到的RPN改进了RP的质量因此提升了整体目标检测的精度。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[^5]: R. Girshick. Fast R-CNN. arXiv:1504.08083, 2015.<br>[^6]: R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.<br>[^7]: K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV. 2014.<br>[^11]: A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.<br>[^18]: P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014. </p>
<p>[^23]: M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional neural networks. In ECCV 2014.</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Zavix
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://xzhewei.com/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks/" title="论文笔记《Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks》">http://xzhewei.com/Note-笔记/Object-Detection/Note-Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Object-Detection/" rel="tag"># Object Detection</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/Note-%E7%AC%94%E8%AE%B0/Pedestrian-Detection/Note-Is-Faster-R-CNN-Doing-Well-for-Pedestrian-Detection/" rel="prev" title="论文笔记《Is Faster R-CNN Doing Well for Pedestrian Detection?》">
                  <i class="fa fa-chevron-left"></i> 论文笔记《Is Faster R-CNN Doing Well for Pedestrian Detection?》
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/Note-%E7%AC%94%E8%AE%B0/Pedestrian-Detection/Note-CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection/" rel="next" title="论文笔记《CityPersons: A Diverse Dataset for Pedestrian Detection》">
                  论文笔记《CityPersons: A Diverse Dataset for Pedestrian Detection》 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zavix</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  





</body>
</html>
