<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css" integrity="sha256-/4UQcSmErDzPCMAiuOiWPVVsNN2s3ZY/NsmXNcj0IFc=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"xzhewei.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.15.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="![1508501321087](http:&#x2F;&#x2F;zavix-image.oss-cn-shenzhen.aliyuncs.com&#x2F;note&#x2F;Rethink Atrous&#x2F;1508501321087.png) Abstract本文我们回顾了atrous 卷积，一个调整filter感知域并控制特征图分辨率的强大工具。为了处理多尺度目标分割，我们的模型使用不同空率（atrous rates）的">
<meta property="og:type" content="article">
<meta property="og:title" content="论文笔记《Rethinking Atrous Convolution for Semantic Image Segmentation》">
<meta property="og:url" content="http://xzhewei.com/Note-%E7%AC%94%E8%AE%B0/Semantic-Segmentation/Note-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/index.html">
<meta property="og:site_name" content="Zavix | Intellisoftx">
<meta property="og:description" content="![1508501321087](http:&#x2F;&#x2F;zavix-image.oss-cn-shenzhen.aliyuncs.com&#x2F;note&#x2F;Rethink Atrous&#x2F;1508501321087.png) Abstract本文我们回顾了atrous 卷积，一个调整filter感知域并控制特征图分辨率的强大工具。为了处理多尺度目标分割，我们的模型使用不同空率（atrous rates）的">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2017-10-21T05:28:35.000Z">
<meta property="article:modified_time" content="2023-03-30T11:43:21.442Z">
<meta property="article:author" content="Zavix">
<meta property="article:tag" content="Atrous Convolution">
<meta property="article:tag" content="Image Segmentation">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://xzhewei.com/Note-%E7%AC%94%E8%AE%B0/Semantic-Segmentation/Note-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://xzhewei.com/Note-%E7%AC%94%E8%AE%B0/Semantic-Segmentation/Note-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/","path":"Note-笔记/Semantic-Segmentation/Note-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/","title":"论文笔记《Rethinking Atrous Convolution for Semantic Image Segmentation》"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>论文笔记《Rethinking Atrous Convolution for Semantic Image Segmentation》 | Zavix | Intellisoftx</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Zavix | Intellisoftx</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">IntelliSoftx</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a></li><li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>公益 404</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Introduction"><span class="nav-number">2.</span> <span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Related-Work"><span class="nav-number">3.</span> <span class="nav-text">2. Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Methods"><span class="nav-number">4.</span> <span class="nav-text">3. Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E8%86%A8%E8%83%80%E5%8D%B7%E7%A7%AF%E7%94%A8%E4%BA%8EDense-Feature-%E6%8F%90%E5%8F%96"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 膨胀卷积用于Dense Feature 提取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Going-Deeper-with-Atrous-Convolution"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 Going Deeper with Atrous Convolution</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-%E5%A4%9A%E7%BD%91%E6%A0%BC"><span class="nav-number">4.2.1.</span> <span class="nav-text">3.2.1 多网格</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Atrous-Spatial-Pyramid-Pooling%EF%BC%88ASPP%EF%BC%89"><span class="nav-number">4.3.</span> <span class="nav-text">3.3 Atrous Spatial Pyramid Pooling（ASPP）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Experimental-Evaluation"><span class="nav-number">5.</span> <span class="nav-text">4. Experimental Evaluation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95"><span class="nav-number">5.1.</span> <span class="nav-text">4.1 训练方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E4%BD%BF%E7%94%A8%E8%86%A8%E8%83%80%E5%8D%B7%E7%A7%AF%E7%9A%84%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C"><span class="nav-number">5.2.</span> <span class="nav-text">4.2 使用膨胀卷积的深度网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-ASPP"><span class="nav-number">5.3.</span> <span class="nav-text">4.3 ASPP</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">6.</span> <span class="nav-text">Reference</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zavix</p>
  <div class="site-description" itemprop="description">一个平平无奇的AI从业者。</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">32</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/xzhewei" title="Github → https:&#x2F;&#x2F;github.com&#x2F;xzhewei" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>Github</a>
      </span>
      <span class="links-of-author-item">
        <a href="/xzhewei@gmail.com" title="E-Mail → xzhewei@gmail.com" rel="noopener me"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://xzhewei.com/Note-%E7%AC%94%E8%AE%B0/Semantic-Segmentation/Note-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zavix">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zavix | Intellisoftx">
      <meta itemprop="description" content="一个平平无奇的AI从业者。">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="论文笔记《Rethinking Atrous Convolution for Semantic Image Segmentation》 | Zavix | Intellisoftx">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          论文笔记《Rethinking Atrous Convolution for Semantic Image Segmentation》
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-10-21 13:28:35" itemprop="dateCreated datePublished" datetime="2017-10-21T13:28:35+08:00">2017-10-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-30 19:43:21" itemprop="dateModified" datetime="2023-03-30T19:43:21+08:00">2023-03-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Note-%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">Note 笔记</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Note-%E7%AC%94%E8%AE%B0/Semantic-Segmentation/" itemprop="url" rel="index"><span itemprop="name">Semantic Segmentation</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>![1508501321087](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508501321087.png)</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文我们回顾了atrous 卷积，一个调整filter感知域并控制特征图分辨率的强大工具。为了处理多尺度目标分割，我们的模型使用不同空率（atrous rates）的atrous卷积进行级联或平行，以此来获得不同尺度的语义信息。并且，我们提出Atrous Spatial Pyramid Pooling (ASPP) 孔空间池化模型，探索多尺度上卷机特征具有图像级的全局特征编码并提升性能。本文提出的 <em>DeepLabv3</em> 比之前的版本提升，并且没有DenseCRF后处理，在VOC 2012图像分割benchmark上，实现与其他先进方法类似的性能。</p>
<span id="more"></span>

<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>DCNN用于语义分割任务面临两个挑战：</p>
<ul>
<li>池化和卷积步长会减小特征分辨率，妨碍需要丰富空间信息的任务，例如分割</li>
<li>目标的尺度多样性</li>
</ul>
<p>解决方法：</p>
<ul>
<li>使用 atrous 卷积 （也称为 膨胀（dilated） 卷积）来改变ImageNet预训练模型的功能，在网络的最后几层取消将采样操作，通过在filter的中间增加孔等同于提高卷积核尺寸。这样在不增加网络参数的情况下提升了特征图的分辨率。</li>
<li>图2显示了几种处理多尺度的方法：<ul>
<li>图像金字塔</li>
<li>编码-解码，先提取特征，然后恢复空间分辨率</li>
<li>使用atrous级联获得高分辨率和大感知域特征</li>
<li>空间金字塔池化[^10][^80] 在特征图上使用不同比率的池化和filter操作得到不同的感受域</li>
</ul>
</li>
</ul>
<p>本文，我们使用atrous卷积增加感受域获得不同尺度的语义信息。我们提出的模型包含多种尺寸的atrous，并且发现batch normolization层对于训练很重要。我们发现使用3$\times$3的atrous卷积，如果空隙过大也不行，主要因为图像边界效应。ASPP中简单的退化为1$\times$1卷积来获得图像级的特征。后面我们会分享训练的经验机械，包括简单有效的bootstrapping方法来处理稀少和精细的标注目标。最后，我们的方法在PASCAL VOC 2012上实现了85.7%的精度，并不适用DenseCRF后处理。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>![1508487635972](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508487635972.png)</p>
<p>(a) 图像金字塔</p>
<p>(b) 编码-解码器</p>
<p>(c) 语义模型</p>
<p>(d) 空间金字塔池化</p>
<p><strong>膨胀卷积</strong> 最近基于膨胀卷积的模型在语义分割领域很流行。[^73]研究了孔率（atrous rates）对获得大范围（long-rang）信息的影响。[^72]采用hyper孔率应用域ResNet最后两个Block。[^16]尝试学习可变形卷积。膨胀卷积也应用于目标检测[^56][^15][^31]</p>
<h2 id="3-Methods"><a href="#3-Methods" class="headerlink" title="3. Methods"></a>3. Methods</h2><p>本节回顾了如何应用膨胀卷积提取高分辨率特征用于语义分割。后面讨论了，将膨胀卷积用于级联和平行模型中。</p>
<h3 id="3-1-膨胀卷积用于Dense-Feature-提取"><a href="#3-1-膨胀卷积用于Dense-Feature-提取" class="headerlink" title="3.1 膨胀卷积用于Dense Feature 提取"></a>3.1 膨胀卷积用于Dense Feature 提取</h3><p>DCNN由于池化和卷积步长导致特征分辨率降低。因此有些方法尝试用反卷积来恢复特征分辨率。但是，我们提倡使用膨胀卷积。膨胀卷积的公式如下：</p>
<p>![1508500253840](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508500253840.png)</p>
<ul>
<li>$x$ 输入特征</li>
<li>$i$ 特征位置</li>
<li>$r$ 孔率（atrous rates）</li>
<li>$k$ filter尺寸</li>
<li>$w$ 权重</li>
</ul>
<p>膨胀卷积能够调整filter的感知域，如图1所示。因此，其能够让我们显式的控制全卷机网络中的特征响应尺寸。这里我们用<em>output_stride</em> 来表示输出的空间分辨率比例。假设一个DCNN的会缩小32倍，取消最后一个池化层或卷积层的步长设为1，然后后面的孔率设为2。这样在不增加任何参数的情况下提高特征分辨率。具体细节看[^10]</p>
<h3 id="3-2-Going-Deeper-with-Atrous-Convolution"><a href="#3-2-Going-Deeper-with-Atrous-Convolution" class="headerlink" title="3.2 Going Deeper with Atrous Convolution"></a>3.2 Going Deeper with Atrous Convolution</h3><p>![1508502437271](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508502437271.png)</p>
<p>级联模型在ResNet的block4开始，使用atrous。本来使用stride和pooling是为了得到大范围的信息，最终缩小到低分辨率的特征图上。然而，我们发现，这样的stride丢失的信息会损害语义分割。因此采样atrous来保持分辨率。</p>
<h4 id="3-2-1-多网格"><a href="#3-2-1-多网格" class="headerlink" title="3.2.1 多网格"></a>3.2.1 多网格</h4><p>受到之前一些研究对不同层级采用不同网格尺寸的启发，我们对block4到block7采用不同的孔率。$Multi\_Grid&#x3D;(r_1,r_2,r_3)$ ，我们采用$rates&#x3D;2\cdot(1,2,4)&#x3D;(2,4,8)$。（这里不太懂是级联的用不同孔率，还是在一层上就用不同的空率）</p>
<h3 id="3-3-Atrous-Spatial-Pyramid-Pooling（ASPP）"><a href="#3-3-Atrous-Spatial-Pyramid-Pooling（ASPP）" class="headerlink" title="3.3 Atrous Spatial Pyramid Pooling（ASPP）"></a>3.3 Atrous Spatial Pyramid Pooling（ASPP）</h3><p>![1508504434479](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508504434479.png)</p>
<p>借鉴了SPP Net的平行空间分辨率的方法。这样能形成不同的感受域，更好的获得多尺度信息。但是，当孔率增加是，有效的filter权重数量也在变少（filter的权重用于特征区域，而不是padding）。当对一个65$\times$65的特征图应用不同孔率时，各个有效权重膨胀卷积操作的比例。当孔率增加，我们看到有效权重的数量减少，极端情况下，只有中心的fitler权重是有效权重。之后还有图像级池化。</p>
<p>![1508505311787](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508505311787.png)</p>
<h2 id="4-Experimental-Evaluation"><a href="#4-Experimental-Evaluation" class="headerlink" title="4. Experimental Evaluation"></a>4. Experimental Evaluation</h2><p>网络模型使用ImageNet预训练的ResNet。在TensorFlow框架下实现。如果希望输出output _ stride&#x3D;8，那就从block3开始用rate&#x3D;2，block4的rate&#x3D;4。</p>
<p>我们在PASCAL VOC 2012 语义分割benchmark上评估模型。评估指标为像素交并集。</p>
<h3 id="4-1-训练方法"><a href="#4-1-训练方法" class="headerlink" title="4.1 训练方法"></a>4.1 训练方法</h3><p><strong>学习率</strong> 我们采用“poly”学习率，对初始学习率乘以$(1-\frac{iter}{max\_iter})^{power}$  ，$power&#x3D;0.9$。</p>
<p><strong>Crop Size</strong> 尺寸为512</p>
<p><strong>Batch normolization</strong> 对ResNet增加模块全部包含batch normolization。后面这段是具体的训练细节。</p>
<p><strong>Upsampling logits</strong> 对最后的结果进行上采样，来保证对原图进行反向传播，不损失原始的标注细节。</p>
<p><strong>Data augmentation</strong> 对原图进行随机缩放（0.5~2.0），然后随机左右翻转。</p>
<h3 id="4-2-使用膨胀卷积的深度网络"><a href="#4-2-使用膨胀卷积的深度网络" class="headerlink" title="4.2 使用膨胀卷积的深度网络"></a>4.2 使用膨胀卷积的深度网络</h3><p>我们先实验了级联膨胀卷积的模型</p>
<p><strong>ResNet-50：</strong> 表1，我们看见output _ stride影响，output _ stride的增加伴随着信号的损失，当采用膨胀卷积后，效果从20.29%提升到75.18%，说明膨胀卷积对级联block进行语义分割的重要性。</p>
<p>![1508509468319](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508509468319.png)</p>
<p><strong>Res-50 vs. Res-101</strong> 两个网络模型，增加级联的block，其性能持续改善，但是提升逐渐减少。</p>
<p>![1508510227795](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508510227795.png)</p>
<p><strong>Multigrid</strong> 表3显示了不同孔率搭配的结果。</p>
<p>![1508510702238](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508510702238.png)</p>
<p><strong>Inference Strategy on val set</strong> 我们还尝试使用output _ stride&#x3D;8的结果。然后对输入图像进行多尺度处理，左右翻转，也能进一步提升，最后的结果是对每个结果probability求平均。</p>
<p>![1508549190998](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508549190998.png)</p>
<h3 id="4-3-ASPP"><a href="#4-3-ASPP" class="headerlink" title="4.3 ASPP"></a>4.3 ASPP</h3><p><strong>ASPP</strong> 表5我们实验了ASPP和应用于block4的Multigrid配合。应该是一个block中有多个卷积层，使用不同的rates，还有图像级的Image Pooling</p>
<p>![1508512107326](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508512107326.png)</p>
<p><strong>Inference strategy on val set</strong> 同上。发现ASPP比级联的稍好。</p>
<p><strong>与DeepLabv2比较</strong> 在没有DenseCRF和MS-COCO预训练的情况下已经比v2好。主要是改进因为fine-tuning batch normalization，以及更好的编码多尺度上下文。</p>
<p><strong>Block5 + ASPP</strong> 我们尝试在组合级联和平行模型，在Block5上的效果却不好。</p>
<p><strong>量化结果</strong> 在PASCAL VOC 2012测试集上的结果mIOU&#x3D;85.7<br>![1508564060949](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508564060949.png)</p>
<p><strong>Bootstrapping</strong> 我们不是像其他人挖掘困难负样本，而是复制包含困难类别的图像。这样简单的Bootstrapping策略也能得到较好的结果。</p>
<p>![1508513867319](<a target="_blank" rel="noopener" href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508513867319.png)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@article&#123;Chen2017Rethinking,</span><br><span class="line">author = &#123;Chen, Liang Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig&#125;,</span><br><span class="line">title = &#123;&#123;Rethinking Atrous Convolution for Semantic Image Segmentation&#125;&#125;,</span><br><span class="line">journal=&#123;arXiv preprint arXiv:1706.05587&#125;</span><br><span class="line">year = &#123;2017&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[^10]: L.-C. Chen, G. Papandreou, I.Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. arXiv:1606.00915, 2016.<br>[^15]: J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection via region-based fully convolutional networks. arXiv:1605.06409, 2016.<br>[^16]: J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y.Wei. Deformable convolutional networks. arXiv:1703.06211, 2017.<br>[^31]: J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z.Wojna, Y. Song, S. Guadarrama, and K. Murphy. Speed&#x2F;accuracy trade-offs for modern convolutional object detectors. In CVPR, 2017.<br>[^56]: G. Papandreou, I. Kokkinos, and P.-A. Savalle. Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection. In CVPR, 2015.<br>[^72]: P.Wang, P. Chen, Y. Yuan, D. Liu, Z. Huang, X. Hou, and G. Cottrell. Understanding convolution for semantic segmen- tation. arXiv:1702.08502, 2017.<br>[^73]: Z. Wu, C. Shen, and A. van den Hengel. Bridging category-level and instance-level semantic image segmen- tation. arXiv:1605.06885, 2016.<br>[^80]: H. Zhao, J. Shi, X. Qi, X.Wang, and J. Jia. Pyramid scene parsing network. arXiv:1612.01105, 2016.</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Zavix
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://xzhewei.com/Note-%E7%AC%94%E8%AE%B0/Semantic-Segmentation/Note-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/" title="论文笔记《Rethinking Atrous Convolution for Semantic Image Segmentation》">http://xzhewei.com/Note-笔记/Semantic-Segmentation/Note-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Atrous-Convolution/" rel="tag"># Atrous Convolution</a>
              <a href="/tags/Image-Segmentation/" rel="tag"># Image Segmentation</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/Note-%E7%AC%94%E8%AE%B0/Pedestrian-Detection/Note-A-unified-multi-scale-deep-convolutional-neural-network-for-fast-object-detection/" rel="prev" title="论文笔记《A unified multi-scale deep convolutional neural network for fast object detection》">
                  <i class="fa fa-chevron-left"></i> 论文笔记《A unified multi-scale deep convolutional neural network for fast object detection》
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-Soft-NMS-%E2%80%93-Improving-Object-DetectionWith-One-Line-of-Code/" rel="next" title="论文笔记《Soft-NMS – Improving Object Detection With One Line of Code》">
                  论文笔记《Soft-NMS – Improving Object Detection With One Line of Code》 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zavix</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  





</body>
</html>
