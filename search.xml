<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>论文笔记《Is Faster R-CNN Doing Well for Pedestrian Detection?》</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Pedestrian-Detection/Note-Is-Faster-R-CNN-Doing-Well-for-Pedestrian-Detection/</url>
    <content><![CDATA[<p><strong>Abstract</strong></p>
<p>行人检测是目标检测的特例。尽管近年Fast&#x2F;Faster R-CNN在目标检测上效果显著，但是在行人检测上效果缺一般。之前的较好的行人检测算法一般是将手工特征和深度卷积特征相结合。本文，研究Faster R-CNN在行人检测上的问题。我们发现Faster R-CNN中的Region Proposal Network（RPN）区域推荐网络可以作为一个效果较好的独立的行人检测器，但是通过下游的检测器后其结果居然变差了。我们认为有两个原因：1）对于处理小目标，特征图的分辨率太小；2）缺少boostrapping策略来提取困难负样本。针对上述问题，我们提出使用RPN+boosted forests共享高分辨卷积特征图的方法。该方法的效果在多个benchmark上进行了评估（Caltech，INRIA，ETH，KITTI），其精度和速度均较好。</p>
<span id="more"></span>

<ul>
<li>中山大学</li>
<li>何凯明</li>
<li>RPN-BF</li>
<li>行人检测</li>
<li><a href="https://github.com/zhangliliang/RPN_BF">code</a></li>
<li><a href="http://ss.sysu.edu.cn/~ll/files/ECCV2016_Pedestrian.pdf">pdf</a></li>
</ul>
<p><strong>理解</strong></p>
<p>本文提出了一种RPN+BF的行人检测方法，其创新性主要体现在：</p>
<ul>
<li>使用卷积神经网络来进行区域推荐</li>
<li>使用卷积神经网络的特征图作为特征进行行人分类</li>
<li>证明了特征图的分辨率对行人检测的影响</li>
<li>证明了困难负样本的提取对检测率的提升的重要性</li>
</ul>
<p>其本质上是利用了两个分类器，一个做区域推荐，其目的是高召回率，同时排除掉大部分的背景区域，另一个做精细的行人分类，把行人与非行人分类。这样做的好处是简化了两个分类器的任务，因为高检测率和低虚警是矛盾的，那么分为两个检测器后，就可以降低每个分类器的设计难度，同时使得后面那个分类器更加的专注，不用考虑特殊的背景干扰。</p>
<p><strong>BibTex</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Zhang2016a,</span><br><span class="line">author = &#123;Zhang, Liliang and Lin, Liang and Liang, Xiaodan and He, Kaiming&#125;,</span><br><span class="line">booktitle = &#123;European Conference on Computer Vision&#125;,</span><br><span class="line">mendeley-groups = &#123;Ped Detection&#125;,</span><br><span class="line">pages = &#123;443--457&#125;,</span><br><span class="line">publisher = &#123;Springer&#125;,</span><br><span class="line">title = &#123;&#123;Is Faster R-CNN Doing Well for Pedestrian Detection?&#125;&#125;,</span><br><span class="line">year = &#123;2016&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>虽然近几年深度学习特征在计算机视觉的一些领域有重大突破，但是在行人识别中，手工特征仍然扮演着重要的角色。</p>
<p>**Faster R-CNN[^11]**在一般目标检测任务上效果较好，基于纯CNN模型，没有手工特征，其包含两个部分:</p>
<ul>
<li>一个全卷机RPN，用于推荐候选区域</li>
<li>一个Fast R-CNN分类器</li>
</ul>
<p>我们发现，PRN的行人检测效果，比后面加了Fast R-CNN的效果还好，可能有如下两个原因：</p>
<p>1）Fast R-CNN用于检测小目标的特征图太小</p>
<p>一般情况行人的尺寸是28x70 (caltech)。ROI的池化层（通常为16 pixel步长）导致低分辨率的特征图特征抹平。从而使得后面的分类器没有足够的特征进行分别。而一般手工特征得到的特征分辨率都较高。</p>
<p>我们解决的思路是在浅层池化而不是高分辨率层，然后通过<strong>hole algorithm（“&#96; a trous”[^16] or filter rarefaction[^17] ）</strong>来增加特征图的尺寸。</p>
<p>2）存在大量困难负样本</p>
<p>行人检测中困难负样本对行人检测的影响很大，这与目标检测中多类分类分问题不太一样。</p>
<p>为了解决这个问题我们采用**cascaded Boosted Forest (BF) [^18][^19]**，来提取困难负样本，然后对样本进行赋予权重。</p>
<p>与以前的方法采用手工特征训练BF不同，我们采用RPN的卷积特征训练BF，这样不仅能够节省计算时间，而且特征比手工特征更具表现力。</p>
<p>最后，我们的方法在多个数据集上进行了验证。特别的是，我们的方法对行人的定位更准，因为当IoU为0.7时，相比于其他方法有40%的提升。并且每个图像的处理时间是0.5s，与其他方法也较接近。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h2><p>集成通道特征Integrate Channel Feature（ICF）是从Viola Jones的框架扩展。ICF中涉及了金字塔和boosted分类器。ICF的改进版，如ACF，LDCF，SCF[^9]，boosting算法仍然是行人检测中的关键基础。</p>
<p>由于R-CNN在目标检测领域的成果，目前有一系列方法采用两步法用于行人检测。</p>
<ul>
<li>文献[^1]使用SCF做区域推荐+R-CNN分类器；</li>
<li>TA-CNN使用ACF做区域推荐+R-CNN类方法，联合语义任务进行行人检测；</li>
<li>DeepParts方法使用LDCF做区域推荐，然后使用神经网络做检测</li>
</ul>
<p>可以发现，上述方法都是用一个单独的手工特征+boosted分类器作为区域提取。</p>
<ul>
<li>CompACT方法学习了一个手工特征和深度卷积特征结合的boosted分类器</li>
<li>CCF是与我们方法最相似的一个，它是一个boosted分类器，使用深度卷积特征金字塔，但是没有区域推荐。</li>
<li>我们的方法没有金字塔，并且比CCF更快更精确。（用卷积特征）</li>
</ul>
<h2 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3 Approach"></a>3 Approach</h2><p>我们的方法由两部分组成：</p>
<ul>
<li>RPN生成候选框和卷积特征图</li>
<li>Boosted Forest作为分类器使用卷积特征</li>
</ul>
<p>![pipline](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Is">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Is</a> Faster R-CNN Doing Well for Pedestrian Detectio_Attachments&#x2F;1pipline.png)</p>
<h3 id="3-1-用于行人检测的区域推荐网络"><a href="#3-1-用于行人检测的区域推荐网络" class="headerlink" title="3.1 用于行人检测的区域推荐网络"></a>3.1 用于行人检测的区域推荐网络</h3><p>Faster R-CNN的RPN主要是用于在多类目标检测场景中解决多类推荐问题。对于单一类别检测，RPN自然的可以只考虑一个类别。因此我们对RPN也进行了一定简化。</p>
<p>我们采用的参考框（anchors）是单一宽高比0.41（这个caltech里有统计过）。原始的RPN的宽高比是变化的。<strong>我们采用9个不同尺度的anchors，从40pixel高开始，1.3倍递增。</strong>因此我们不需要使用特征金字塔。</p>
<p>我们采用VGG-16预训练网络（ImageNet）作为主网络。RPN建立在Conv5_3之上，在他们之间有一个3x3卷积层和两个1x1卷积层用于分类和bounding box回归 **(more details in [11])**。 RPN的回归boxs具有16pixels的步长，分类层得到的置信度得分用于预测boxes，并且也作为Boosted Forest级联的初始化得分。</p>
<p>我们使用”a trous”技巧来提高分辨率，减少步长，但是RPN还是使用16 pixels步长，这个策略只用来提取特征，而不是用来fine-tuning。</p>
<h3 id="3-2-特征提取"><a href="#3-2-特征提取" class="headerlink" title="3.2 特征提取"></a>3.2 特征提取</h3><p>从RPN提取的区域，我们使用**RoI池化[^12]**提取固定长度的特征。不同于Faster R-CNN的fc层需要固定的特征纬度，BF没有这个限制。例如，面向Conv3_3，Conv4_3，提特征，虽然他们的特征图不一样，但是通过池化可以固定为7x7。不同层的特征可以简单的连接在一起，而深度分类器的特征归一化需要小心的处理。</p>
<p>由于BF对于特征纬度没有限制，因此我们可以增加特征的分辨率。原来RPN的各层的步长是（Conv3，步长4，Conv4，步长8，Conv5，步长16），我们使用”a trous”技巧得到更高的分辨率。例如，我们设pool3的步长是1，膨胀所有Conv4卷积2，Con4步长是4。<strong>不同于之前方法中会fine-tune膨胀之后的卷积核，我们的方法只是来提特征而不进行fine-tune。</strong>(这里好像有问题，你改变了卷积操作，而不fine-tune，那如何保证其效果比不fine-tune的效果好？有可能fine-tune之后RPN的整体效果下降了，但是可能提取高分辨特征的能力提升了)</p>
<p>我们的ROI分辨率于Faster R-CNN相同（7x7），但是特征图用的是更高的分辨率。如果输入的ROI尺寸小于7x7，池化会将特征抹平导致无法分别。</p>
<h3 id="3-3-Boosted-Forest"><a href="#3-3-Boosted-Forest" class="headerlink" title="3.3 Boosted Forest"></a>3.3 Boosted Forest</h3><p>RPN网络产生推荐区域，置信度得分，特征，我们使用这些来训练级联Boosted Forest分类器。我们使用RealBoost算法[^18]，超参采用借鉴[^4]。</p>
<ul>
<li>boostrap 训练6个stage，每次有{64，128，256，512，1024，1536}个树</li>
<li>开始训练样本包括，然后随机从推荐的区域中提取相同数量的负样本</li>
<li>经过每个stage训练后增加困难负样本（正样本数量的10%）</li>
<li>最后通过所有stage得到一个2048个树的森林</li>
<li>实现是在pdollar的工具包基础上</li>
</ul>
<p>RPN类似一个stage-0的分类器，通过它我们可以得推荐区域的置信度得分s，我们令$f_0&#x3D;\frac{1}{2} log\frac{s}{1-s}$，后面接RealBoost分类器。其他的阶段就是标准的RealBoost。</p>
<h3 id="3-4-实现细节"><a href="#3-4-实现细节" class="headerlink" title="3.4 实现细节"></a>3.4 实现细节</h3><ul>
<li>我们采用单一尺度进行训练和测试</li>
<li>检测框与GT的IoU超过0.5则认为是检测到</li>
<li>采用image-centric训练框架[^11][^12]</li>
<li>每个mini-batch包含1个图像和120个随机矩形框，正负样本比为1：5</li>
<li>RPN的超参采用文献[^11]采用的</li>
<li>我们发现[^11]在fine-tune中不使用边界交叉的矩形框，但是我们保留下来能提供精度</li>
<li>推荐区域NMS的阈值为0.7</li>
<li>推荐区域按得分排序，每个图片只取前1000个区域</li>
<li>树的深度对Caltech和KITTI是5，对INRIA和ETH是2</li>
<li>测试的时候我们RPN的前100个区域给BF进行分类</li>
</ul>
<h2 id="4-实验和分析"><a href="#4-实验和分析" class="headerlink" title="4 实验和分析"></a>4 实验和分析</h2><h3 id="4-1-数据集"><a href="#4-1-数据集" class="headerlink" title="4.1 数据集"></a>4.1 数据集</h3><ul>
<li>我们在四个数据集上机械能了测试，Caltech、INRIA、ETH、KITTI。IoU设为0.5。</li>
<li>Caltech数据集，其中有4024个图像是用于测试，测试标准为50 pixel高，65%可见。</li>
<li>INRIA和ETH用于验证模型的泛化能力。</li>
<li>KITTI包含立体数据，我们用左相机的7481个图像进行训练</li>
</ul>
<h3 id="4-2-实验"><a href="#4-2-实验" class="headerlink" title="4.2 实验"></a>4.2 实验</h3><p>这个部分主要在Caltech数据集上进行。</p>
<h4 id="RPN对行人检测足够好么？"><a href="#RPN对行人检测足够好么？" class="headerlink" title="RPN对行人检测足够好么？"></a>RPN对行人检测足够好么？</h4><p>我们测试了RPN在不同IoU和不同区域1、4、100数量时的召回率。如图所示，可以发现RPN相比于传统特征ACF、LDCF、Checkerboards的效果较好。100个推荐区域在IoU&#x3D;0.7时的召回率大于95%。</p>
<p>![3 recall vs IoU](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Is">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Is</a> Faster R-CNN Doing Well for Pedestrian Detectio_Attachments&#x2F;3 recall vs IoU.png)</p>
<p>RPN单独作为行人检测器时，MR为14.9%。</p>
<h4 id="特征分辨率的影响"><a href="#特征分辨率的影响" class="headerlink" title="特征分辨率的影响"></a>特征分辨率的影响</h4><p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Is">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Is</a> Faster R-CNN Doing Well for Pedestrian Detectio_Attachments&#x2F;t1.png)</p>
<ul>
<li>RPN + R-CNN的效果比RPN好，R-CNN使用的是原始的像素信息，因此特征分辨率的影响较小。</li>
<li>RPN + Fast R-CNN的效果就变差了，但是通过a trous技巧提升分辨率后，MR就减小了</li>
<li>RPN + BF，特征图的分辨率与MR有直接关系</li>
</ul>
<p>如果我们确定了RPN + BF的框架，那么应该选择哪些卷积层特征呢？作者做了比较。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Is">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Is</a> Faster R-CNN Doing Well for Pedestrian Detectio_Attachments&#x2F;t2.png)</p>
<p>从表2可知，分辨率可以降低MR，但是卷积层的增加可以提供特征描述能力，最好发现Conv3_3和Conv4_3(a trous)的效果最好。并且计算效率也比较高。Tesla K40 GPU平台，0.5s处理一张图像。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Is">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Is</a> Faster R-CNN Doing Well for Pedestrian Detectio_Attachments&#x2F;t4.png)</p>
<h4 id="Bootstrapping重要么？"><a href="#Bootstrapping重要么？" class="headerlink" title="Bootstrapping重要么？"></a>Bootstrapping重要么？</h4><p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Is">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Is</a> Faster R-CNN Doing Well for Pedestrian Detectio_Attachments&#x2F;t3.png)</p>
<p>从上图中可以发现，使用了bootstrapping之后的效果会提升，而且采用Fast R-CNN和BF的差别不大，可以认为bootstrapping对检测率的提升很重要。</p>
<h3 id="4-3-与目前先进的方法比较"><a href="#4-3-与目前先进的方法比较" class="headerlink" title="4.3 与目前先进的方法比较"></a>4.3 与目前先进的方法比较</h3><p>这里作者在各种数据集上和很多检测器的效果进行了比较，总之就是各种好。</p>
<h2 id="5-结论和讨论"><a href="#5-结论和讨论" class="headerlink" title="5 结论和讨论"></a>5 结论和讨论</h2><p>我们提出了一组非常有效的的行人检测方法，基于RPN和BF。PRN用于区域推荐和计算特征，BF分类器更加灵活：1）结合不同卷积层的不同分辨率的特征。2）配合bootstrapping进行困难负样本提取。这两个特征有效克服了Faster R-CNN对行人检测的限制。</p>
<p>我们发现有趣的是bootstrapping是一个非常关键的组件，甚至对深度神经网络也是。目前已经有人开展这方面的工作，称作**Online Hard Example Mining（OHEM）[^32]**来训练Fast R-CNN用于目标检测。比较端到端的在线挖掘方法和多阶段的级联bootstrapping方法是一个非常有趣的方向。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[^1]: Hosang, J., Omran, M., Benenson, R., Schiele, B.: Taking a deeper look at pedestrians. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015)<br>[^4]: Cai, Z., Saberian, M., Vasconcelos, N.: Learning complexity-aware cascades for deep pedestrian detection. In: IEEE International Conference on Computer Vision (ICCV) (2015)<br>[^9]: Benenson, R., Omran, M., Hosang, J., Schiele, B.: Ten years of pedestrian detection, what have we learned? In: Agapito, L., Bronstein, M.M., Rother, C. (eds.) ECCV 2014 Workshops. LNCS, vol. 8926, pp. 613–627. Springer, Heidelberg (2015)<br>[^11]: Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object detection with region proposal networks. In: Neural Information Processing Systems (NIPS) (2015)<br>[^12]: Girshick, R.: Fast R-CNN. In: IEEE International Conference on Computer Vision (ICCV) (2015)<br>[^16]: Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic image segmentation with deep convolutional nets, fully connected CRFs (2014). arXiv :1412.7062<br>[^17]: Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015)<br>[^18]: Friedman, J., Hastie, T., Tibshirani, R., et al.: Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors). Ann. Stat. 28, 337–407 (2000)<br>[^19]: Appel, R., Fuchs, T., Doll´ ar, P., Perona, P.: Quickly boosting decision trees-pruning underachieving features early. In: International Conference on Machine Learning (ICML) (2013)<br>[^32]: Shrivastava, A., Gupta, A., Girshick, R.: Training region-based object detectors with online hard example mining (2016). arXiv:1604.03540 </p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Pedestrian Detection</category>
      </categories>
      <tags>
        <tag>Pedestrian Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记《Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks》</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-Faster-R-CNN-Towards-Real-Time-Object-Detection-with-Region-Proposal-Networks/</url>
    <content><![CDATA[<p><strong>Abstract</strong></p>
<p>目前先进的目标检测网络依赖区域推荐算法来推测目标位置。SSPnet和Fast R-CNN在减少网络运行时间的基础上发现，区域推荐是计算瓶颈。本文提出Region Proposal Network（RPN）区域推荐网络与检测网络的共享全图卷积特征，从而实现接近0成本的区域推荐。RPN作为一个全卷积网络能够在图像各个位置预测物体的边界和得分。RPN采用端到端的训练方法实现高质量的区域推荐，Fast R-CNN作为检测器。通过简单的联合优化，RPN和Fast R-CNN可以贡献卷积特征进行训练。对于VGG16模型，我们的检测系统在GPU的速度能够实现5fps。在每个图片提取300个候选区的情况下 ，PASCAL VOC 2007上精度为73.2% mAP，PASCAL VOC  2012精度为70.4% mAP。</p>
<span id="more"></span>

<ul>
<li>Faster R-CNN</li>
<li>微软</li>
<li>v-shren, kahe, rbg, jiansun</li>
<li><a href="https://github.com/ShaoqingRen/faster_rcnn">code</a></li>
<li><a href="https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">pdf</a></li>
</ul>
<p><strong>BibTex</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;ren2015faster,</span><br><span class="line">  title=&#123;Faster r-cnn: Towards real-time object detection with region proposal networks&#125;,</span><br><span class="line">  author=&#123;Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian&#125;,</span><br><span class="line">  booktitle=&#123;Advances in neural information processing systems&#125;,</span><br><span class="line">  pages=&#123;91--99&#125;,</span><br><span class="line">  year=&#123;2015&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>理解</strong></p>
<p>本文主要提出了一个two-stage的目标检测框架，将原有的目标检测分为区域推荐+目标检测两个步骤，同时公用特征提取部分的卷积操作。RPN网络区别于语义分割为，RPN提供了bounding box，同时在一个图像尺度下进行bounding box回归，避免了之前采用图像金字塔的方式，极大的减少了计算量。</p>
<p>同时该方法还是一个通用的框架，不限于具体使用的网络模型，例如ZF，VGG都行。只是在最后一个卷积层上分别采用不同的处理方式。一边采用两层卷积层进行进一步特征提取，然后做bounding box回归和目标得分，另一边基于提出的bunding box对原卷积层特征做全连接的分类处理。</p>
<p>同时，由于该架构基于统一的卷积网络，因此通过GPU加速能够实现快速检测，而传统方法SS只能基于CPU运算，其效率远远低于GPU。</p>
<p>**RPN-BF那篇文章主要采用RPN的思想来推荐候选区域，但是对卷积层的共享这部分没有采用，主要原因是他想采用boosting进行困难负样本提取。那是否有方法可以在卷积神经网络的架构下进行困难负样本提取呢？如果可以那基于RPN+ClassifyNet的架构下，同时利用了最好的神经网络模型可能实现更好的结果，例如F-DNN，就是利用了GoogLeNet和ResNet实现了更好的精度。RPN-Boosted DNN架构 **</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>目前采用区域推荐方法和基于区域的卷积神经网络（R-CNN）实现了较好的目标检测效果。尽管基于区域的CNN计算开销巨大，但是通过共享卷积特征能够显著的降低计算量。目前Fast R-CNN在不考虑区域推荐的情况下能够实现接近实时的检测性能。因此区域推荐的计算开销称为目前检测系统的主要瓶颈。</p>
<p>区域推荐方法一般只需要inexpensive特征和简单的推理框架，但是相比于检测器效率仍然很低</p>
<ul>
<li>Selective Search是一种流行的框架，其对低级的特征进行融合。但是相比于检测网络，要慢一个数量级，2s&#x2F;img on CPU</li>
<li>EdgeBoxes是目前平衡精度和计算时间最后的方法，0.2s&#x2F;img</li>
</ul>
<p>但是需要注意到基于CNN的区域推荐网络受益于GPU优势，而一般的区域推荐方法采用CPU计算，因此计算时间不具可比性。因此在GPU上实现区域推荐算法，能够有效的提升运行效率。但是仅仅在GPU上实现区域推荐方法，会忽略计算贡献的重要性。</p>
<p>本文，我们显示了一种高效的解决方法，使得区域推荐的计算成本几乎为0。RPN网络与目标检测网络贡献卷积层，因此在检测任务上，区域推荐的边际成本只有10ms&#x2F;img。</p>
<p>我们在卷积特征上构造RPN，增加了两个卷积层：一个将卷积图每个位置编码到一个256维的特征向量；另一个是在每个卷积图位置输出一个是目标的得分和回归k个候选区域边界（一般k&#x3D;9）。</p>
<p>RPN是一种全卷积网络，可以使用端到端的训练方法。为了联合RPN和Fast R-CNN，我们提出一个简单的训练框架，在fine-tuning 区域推荐任务和fine-tuning目标检测任务之间切换。这个方法能够很快收敛并得到共用的卷积特征。</p>
<p>我们在PASCAL VOC检测任务上评估我们的方法的精度比Selective Search + Fast R-CNN要高。并且计算效率非常高。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h2><p>最近的论文提出了几种专门用于定位和矩形框估计的方法。</p>
<ul>
<li>OverFeat，训练fc层用于预测box的坐标。然后将fc层变为卷积层用于检测特定类别的目标</li>
<li>MultiBox，fc层生成推荐区域，同时预测多个（800）boxes，然后用R-CNN进行目标检测。区域推荐网络应用于单一图像，或者多个大截取区域</li>
</ul>
<p>共享卷积计算的方法正却来越受关注：</p>
<ul>
<li>OverFear计算图像金字塔的卷积特征用于分类、定位、检测</li>
<li>SPP在共享卷积特征图上进行自适应尺寸池化，提升基于区域的目标检测和语义分割</li>
<li>Fast R-CNN能够在共享卷积特征上进行端到端训练，提升精度和速度</li>
</ul>
<h2 id="3-区域推荐网络"><a href="#3-区域推荐网络" class="headerlink" title="3 区域推荐网络"></a>3 区域推荐网络</h2><p>RPN的输入为一个图像（任何尺寸），输出为一系列的矩形目标区域和每个区域的得分。由于我们的目的是让RPN和Fast R-CNN共享计算资源，因此我们假设两个网络共用一些卷积层。在我们的实验组，我们评估了ZF模型[^23] (5个可共享卷积层)，VGG模型（13个可共享卷积层）。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster</a> R-CNN Towards Real-Time Object Detection with Region Proposal Networks_Attachments\f1.png)</p>
<p>为了生成区域推荐，我们在最后一个共享卷积层输出的特征图上使用一个小网络进行滑动。该网络是一个$n \times n$的空间窗口。每次滑动窗口都映射为一个低维向量（256-d对ZF，512-d对VGG）。每个向量输入给两个平行的全连接层：一个box回归层（reg），一个box分类层（cls）。本文中n&#x3D;3，其有效的感知域在输入图像上是很大的（171pixel for ZF，228pixel for VGG）。滑动窗口通过256&#x2F;512个$n \times n$的卷积层实现，后面两个分别用$1 \times 1$的卷积层实现。在$n \times n$后使用ReLU。</p>
<h3 id="平移不变Anchors"><a href="#平移不变Anchors" class="headerlink" title="平移不变Anchors"></a>平移不变Anchors</h3><ul>
<li>每个滑窗的位置，我们同时预测k个区域推荐</li>
<li>reg层输出4k个坐标</li>
<li>cls层输出2k个分数，分别估计是目标和不是目标的概率</li>
<li>k个推荐区域参数化为k个参考box，称为anchors</li>
<li>每个anchors的中心是否在滑窗中心，与尺度和宽高比相关</li>
<li>我们采用3种尺度、3中宽高比，因此有9中anchors</li>
<li>对于$W \times N$的特征图，总共有WHk个anchors</li>
<li>我们的方法是平移不变的</li>
</ul>
<p>作为对比：</p>
<ul>
<li>MultiBox使用k-means生成800种anchors，不具有平移不变性</li>
<li>MultiBox需要$（4+1）\times 800$-维输出层（由于不具有平移不变形，不太理解为什么）</li>
<li>我们的方法只需要$(4+2)\times 9$-维输出</li>
<li>我们的方法需要的参数更少，因此在小数据集上的过拟合风险更小</li>
</ul>
<h3 id="区域推荐的损失函数"><a href="#区域推荐的损失函数" class="headerlink" title="区域推荐的损失函数"></a>区域推荐的损失函数</h3><p>训练RPN网络使用二元类标。标记为positive的anchors需要满足两个条件：1）与GT的IoU最高；2）IoU至少为0.7以上。因此一个Ground Truth可能会给多个anchors分配positive类标。当IoU低于0.3时标为negative。没有类标的anchors不进行训练。</p>
<p>对于这样多任务的损失函数，一个图片的损失函数定义为：</p>
<p>$$L({p_i},{t_i})&#x3D;\frac{1}{N_{cls}}\sum_i L_{cls}(p_i,p_i^*)+\lambda\frac{1}{N_{reg}}\sum_i p_i^<em>L_{reg}(t_i,t_i^</em>)$$</p>
<p>$i​$ 是mini-batch中第i个anchor，$p_i​$是预测类标，$p_{i}^*​$ 是gt标签（0&#x2F;1），$t_i​$ 是预测的bb的坐标，$t_i^*​$ 是gt。 $L_{cls}​$ 是分类损失函数， $L_{reg}&#x3D;R(t_i-t_i^{*})​$ 是bb回归损失函数,$R​$ 是鲁棒损失函数（smooth L1）[^5]。 $p_i^{*} L_{reg}​$ 表示只考虑positive anchor的回归损失， $\lambda​$ 是平衡参数，目前设为10。 $N_{cls}​$ 跟mini-batch有关，目前为256， $N_{reg}​$ 大约为2400。</p>
<p>为了进行回归，采用参数化坐标[^6]：</p>
<p>$$<br>t_x&#x3D;(x-x_a)&#x2F;w_a, t_y&#x3D;(y-y_a)&#x2F;h_a, t_w&#x3D;log(w&#x2F;w_a),t_h&#x3D;log(h&#x2F;h_a)<br>$$</p>
<p>$$<br>t_x^*&#x3D;(x^*-x_a)&#x2F;w_a, t_y^*&#x3D;(y^*-y_a)&#x2F;h_a, t_w&#x3D;log(w^*&#x2F;w_a),t_h&#x3D;log(h^*&#x2F;h_a)<br>$$</p>
<p>$x,y,w,h$是box的中心和宽高，$x,x_a,x^*$分别是预测box，anchor box，Ground truth。可以看作是从一个anchor box到附近的Ground truth box的bounding box回归。</p>
<p>在[^5][^7]中，在任意尺寸区域的池化特征上进行bb回归，所有的区域共享回归权重。我们的方法，使用相同的尺寸的特征图，为了适应变化的尺寸，我们为k种bb都学习一个回归器，不共享权重。这样在固定尺寸特征图上我们仍然可以预测不同尺寸的bb。</p>
<h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>RPN是一个全卷积神经网络，可以通过反向传播和随机梯度下降实现端到端训练。我们遵循“image-centric”[^5]训练该网络。一张图片是一个mini-batch，但是由于负样本比正样本多，会造成数据偏移。因此我们在一个mini-batch里采用随机的选择256个anchor进行训练，正负样本1:1。如果正样本少于128，则由负样本补齐。</p>
<p>参数的初始化：</p>
<ul>
<li>新增的卷积层，使用随机高斯分布（均值为0，标准差为0.01）进行参数初始化</li>
<li>其他网络层（共享卷积层）使用ImageNet分类任务预训练的参数，一般都这么做</li>
<li>对ZF我们fine-tune所有层，对VGG只fine-tune conv3_1及以上的网络，来节省内存</li>
<li>学习率：前60k mini-batch是0.001，后20k是0.0001</li>
<li>动量为0.9，权重衰减0.0005（<strong>这个是caffe上标准做法</strong>）[^11]</li>
</ul>
<h3 id="区域推荐和目标检测共享卷积特征"><a href="#区域推荐和目标检测共享卷积特征" class="headerlink" title="区域推荐和目标检测共享卷积特征"></a>区域推荐和目标检测共享卷积特征</h3><p>至此我们描述了如何训练一个区域推荐网络，但是没有考虑目标检测CNN如果利用这些推荐区域。下面我们将描述采用Fast R-CNN作为目标检测网络如何训练。</p>
<p>RPN和Fast R-CNN如果分开训练，则卷积层的参数将差别很大，因此为了共享卷积层，我们需要同时进行训练。然而，单一网络同时包含RPN和Fast R-CNN进行反向传播优化并不容易。因为Fast R-CNN的训练需要依赖固定的目标区域，如果在训练过程中目标的生成机制也在变化，能否收敛则不得而知。联合优化是一个非常有趣的研究方向，<strong>我们采取更务实的4步训练算法，通过轮换寻优（alternating optimization ）来训练共享特征。</strong></p>
<ol>
<li>RPN使用ImageNet预训练模型，端到端的fine-tune训练区域推荐任务</li>
<li>使用RPN推荐的区域训练Fast R-CNN（之前也是用ImageNet预训练）</li>
<li>使用Fast R-CNN的参数来初始化RPN，并且固定共享卷积层参数，只fine-tune RPN增加的卷积层参数。</li>
<li>固定共享卷积层，fine-tune Fast R-CNN的fc层</li>
</ol>
<p>通过上面四步，两个网络就共用了卷积层，形成统一的网络。</p>
<h3 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h3><p>图片的设置</p>
<ul>
<li>训练和测试区域推荐和目标检测网络，均使用单一尺度图像</li>
<li>缩放图像到同一尺寸，短边600 pixels</li>
<li>提取多尺度特征可能会改善精度，但是会损失效率</li>
<li>最后一层卷积层的步长反推到原图上是16 pixels，在这种情况下也能得到较好的结果</li>
</ul>
<p>Anchor设置</p>
<ul>
<li>3种尺度的面积，$128^2,256^2,512^2$，3种宽高比1:1，1:2，2:1（这里好像没有什么依据）</li>
<li>我们发现anchor box的面积比感知野大也是可以的，这样如果一个目标只有中间部分出现，粗略的估计目标的其他范围也是可行的*(即可以估计被遮挡目标的实际大小)*</li>
<li>因此，我们的方法不需要多尺度特征和滑窗来预测大区域</li>
<li>下表是使用ZF net学习的推荐区域平均尺寸</li>
</ul>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster</a> R-CNN Towards Real-Time Object Detection with Region Proposal Networks_Attachments\t1.png)</p>
<p>Anchor超过图像边界的处理</p>
<ul>
<li>忽略所有超过边界的anchor</li>
<li>典型的1000x600的图像有大约20k（60*40*9）个anchor，排除超过边界的，剩下越6k个用于训练</li>
<li>如果不排除那些anchor，很难正确评估损失值，而不收敛</li>
<li>在测试时，RPN仍会产生跨边界的anchor，我们按图片边缘进行裁剪</li>
</ul>
<p>Anchor的NMS</p>
<ul>
<li>RPN的推荐区域有高度重叠，为了减少，基于cls得分进行NWS</li>
<li>IoU采用0.7，这样剩下大概2k个区域</li>
<li>NMS不会损失精度，同时极大减少计算量</li>
<li>我们的区域取前N个，训练时使用2K个，测试时使用另一个数量</li>
</ul>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4 实验"></a>4 实验</h2><ul>
<li>数据集 PASCAL VOC 2007</li>
<li>训练5k，测试5k，20个目标类别</li>
<li>ImageNet 预训练，fast版ZF，5个卷积，3个fc；VGG-16，13个卷积，3个fc</li>
<li>mAP作为评价，平均精度</li>
</ul>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster</a> R-CNN Towards Real-Time Object Detection with Region Proposal Networks_Attachments\t2.png)</p>
<p>首先，实验采用自身提出的Proposal训练分类网络，使用RPN+ZF共享卷积层的方法mAP为59.9%</p>
<p>Ablation Experiments 分解实验：</p>
<ul>
<li>卷积层共享的影响：未进行卷积层共享的精度降低，可能的原因是共享卷积层后，检测器fine-tuned的特征用于RPN fine-tune，区域提取的质量提高</li>
<li>RPN对Fast R-CNN训练的影响，使用SS的Proposal对分类器训练，然后使用RPN对分类器测试。从结果看训练和测试的Proposal的一致性还是很重要的。另外6k个Proposal与100个Proposal的差异不大，说明NMS对精度的影响是很小的。</li>
<li>cls和reg的作用。测试时没有cls，则没有NMS和排序，随机选择N个Proposal，1K时差别不大，但是100时差别很大，说明cls得分对选择Proposal是有用的。测试时没有reg，即Proposal变成了anchor box，精度也下降了，说明高质量的Proposal主要取决于位置回归。</li>
<li>其他更好的网络模型对RPN的影响。我们使用VGG-16训练RPN，进行Proposal，但是分类器仍然使用原来的SS+ZF，精度从56.8%提升到59.2%。说明RPN+VGG提供的Proposal效果更好。</li>
</ul>
<h3 id="VGG-16的检测精度和运行时间"><a href="#VGG-16的检测精度和运行时间" class="headerlink" title="VGG-16的检测精度和运行时间"></a>VGG-16的检测精度和运行时间</h3><p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster</a> R-CNN Towards Real-Time Object Detection with Region Proposal Networks_Attachments\t3.png)</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster</a> R-CNN Towards Real-Time Object Detection with Region Proposal Networks_Attachments\t4.png)</p>
<ul>
<li>表2和表3分别是在PASCAL VOC2007和PASCAL VOC2012上的结果</li>
<li>分类器均为Fast R-CNN，主要比较RPN的模型差异</li>
<li>训练集：07表示采用VOC 2007训练集，12表示采用VOC 2012做训练集，07+12表示用上面两个一起做训练集，07++12表示用07的训练和测试集+12的训练集做为训练集</li>
<li>训练时Proposal都是2k</li>
<li>可以发现RPN+VGG比SS更精确，且速度更快</li>
</ul>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster</a> R-CNN Towards Real-Time Object Detection with Region Proposal Networks_Attachments\t5.png)</p>
<p>表4总结了目标检测器的运行时间</p>
<ul>
<li>运行平台K40，SS运行在CPU上</li>
<li>SS的时间大约为1-2s</li>
<li>Fast R-CNN基于VGG-16模型在2k SS上需要320ms</li>
<li>我们的整个系统在VGG上只需198ms</li>
<li>region-wise包括：NMS, pooling, fc, softmax</li>
<li>由于共享了卷积层Proposal的计算只有10ms</li>
<li>由于更少的Proposal数量，所以region-wise时间更少</li>
<li>ZF模型更简单，因此可实现17fps的效率</li>
</ul>
<h3 id="分析IoU和Recall关系"><a href="#分析IoU和Recall关系" class="headerlink" title="分析IoU和Recall关系"></a>分析IoU和Recall关系</h3><p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster</a> R-CNN Towards Real-Time Object Detection with Region Proposal Networks_Attachments\f2.png)</p>
<p>图2：Recall和IoU重叠率的关系</p>
<ul>
<li>proposal数量分别为300，1000，2000</li>
<li>RPN对于proposal数量减少不敏感，主要归功于cls</li>
<li>SS和EB会随着proposal数量的减少迅速下降</li>
</ul>
<h3 id="one-stage检测-vs-two-stage区域推荐-检测"><a href="#one-stage检测-vs-two-stage区域推荐-检测" class="headerlink" title="one-stage检测 vs two-stage区域推荐+检测"></a>one-stage检测 vs two-stage区域推荐+检测</h3><p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Faster</a> R-CNN Towards Real-Time Object Detection with Region Proposal Networks_Attachments\t6.png)</p>
<p>OverFeat[^18]提出一种在卷积特征图上滑窗通过回归器和分类器进行检测。该方法是one-stage，特定类别的检测流程，我们的方法是two-stage级联不定类别推荐和特定类别检测。OverFeat中，region-wise特征从由一个宽高比的尺度金字塔滑窗获得。RPN中特征从一个方向滑窗和相对不同宽高比、尺度的anchor预测区域。虽然两个方法都用到了滑窗，但是RPN中区域推荐任务只是第一个阶段，Fast R-CNN用来进一步细化他们。在第二阶段里，region-wise特征在推荐的box中进一步池化，而从能够更加专注与该区域的特征。我们详细这将导致更精确的检测。</p>
<p>为了比较，one-stage和two-stage系统，我们进行了大量实验。如表5所示。5 scales是图像金字塔的尺度。结果看two-stage的效果更好，且计算更快。</p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h2><p>我们提出了一种RPN用于高效精确的区域推荐。通过共享卷积层特征，RPN可以做到几乎计算开销0成本。我们的方法可以一个5-17fps的深度学习目标检测系统。学习得到的RPN改进了RP的质量因此提升了整体目标检测的精度。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[^5]: R. Girshick. Fast R-CNN. arXiv:1504.08083, 2015.<br>[^6]: R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.<br>[^7]: K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV. 2014.<br>[^11]: A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.<br>[^18]: P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014. </p>
<p>[^23]: M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional neural networks. In ECCV 2014.</p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo 搭建个人博客</title>
    <url>/Tutorial-%E6%95%99%E7%A8%8B/Hexo-Create-personal-blog/</url>
    <content><![CDATA[<p>此教程基本为当前blog的搭建过程,其中遇到的主要问题记录在下面，未来不定期更新。</p>
<span id="more"></span>

<h2 id="1-按照-Next-主题的步骤搭建"><a href="#1-按照-Next-主题的步骤搭建" class="headerlink" title="1. 按照 Next 主题的步骤搭建"></a>1. 按照 Next 主题的步骤搭建</h2><p>首先安装hexo官网教程安装hexo<br><a href="https://hexo.io/docs/">https://hexo.io/docs/</a><br>然后将Next主题安装在theme文件夹内<br><a href="https://github.com/iissnan/hexo-theme-next">https://github.com/iissnan/hexo-theme-next</a></p>
<h2 id="2-更换主题scheme，在theme-next-config中更换scheme。"><a href="#2-更换主题scheme，在theme-next-config中更换scheme。" class="headerlink" title="2. 更换主题scheme，在theme/next/_config中更换scheme。"></a>2. 更换主题<code>scheme</code>，在<code>theme/next/_config</code>中更换<code>scheme</code>。</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Schemes</span><br><span class="line">#scheme: Muse</span><br><span class="line">#scheme: Mist</span><br><span class="line">#scheme: Pisces</span><br><span class="line">scheme: Gemini</span><br></pre></td></tr></table></figure>
<h2 id="3-新建文章"><a href="#3-新建文章" class="headerlink" title="3. 新建文章"></a>3. 新建文章</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new post &quot;title&quot;</span><br></pre></td></tr></table></figure>
<h2 id="4-安装hexo-wordcount来显示每篇文章的字数，阅读时间等等"><a href="#4-安装hexo-wordcount来显示每篇文章的字数，阅读时间等等" class="headerlink" title="4. 安装hexo-wordcount来显示每篇文章的字数，阅读时间等等"></a>4. 安装hexo-wordcount来显示每篇文章的字数，阅读时间等等</h2><p><a href="https://github.com/willin/hexo-wordcount">https://github.com/willin/hexo-wordcount</a></p>
<h2 id="5-更换markdown渲染工具，适配mathjax"><a href="#5-更换markdown渲染工具，适配mathjax" class="headerlink" title="5. 更换markdown渲染工具，适配mathjax"></a>5. 更换markdown渲染工具，适配mathjax</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
<h2 id="6-上传头像"><a href="#6-上传头像" class="headerlink" title="6. 上传头像"></a>6. 上传头像</h2><p>将头像文件放置在<code>/source/uploads/avatar.gif</code><br>在主题<code>_config</code>文件中找到<code>avatar: </code> 填上上述地址</p>
<h2 id="7-部署git"><a href="#7-部署git" class="headerlink" title="7. 部署git"></a>7. 部署git</h2><p>首先在github上建立一个repo，其名称为yourname.github.io，注意这里yourname需要跟你的github的用户名一致。然后在hexo的_config填写配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">type: git</span><br><span class="line">repo: https://github.com/xzhewei/xzhewei.github.io.git</span><br><span class="line">branch: master</span><br><span class="line">message:</span><br></pre></td></tr></table></figure>
<p>然后用hexo生成静态网页，最后提交</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>
<h2 id="8-建立sitemap"><a href="#8-建立sitemap" class="headerlink" title="8. 建立sitemap"></a>8. 建立sitemap</h2><p>安装sitemap生成工具</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-sitemap --save</span><br></pre></td></tr></table></figure>
<p>然后在_config.yml中填写配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># sitemap</span><br><span class="line">sitemap:</span><br><span class="line">path: sitemap.xml</span><br></pre></td></tr></table></figure>
<p>最后用hexo生成，部署即可。</p>
<h2 id="9-增加文章分享功能"><a href="#9-增加文章分享功能" class="headerlink" title="9. 增加文章分享功能"></a>9. 增加文章分享功能</h2><p>编辑主题配置文件，添加&#x2F;修改字段<code>jiathis</code>，值为<code>true</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># JiaThis 分享服务</span><br><span class="line">jiathis: true</span><br></pre></td></tr></table></figure>
<h2 id="10-阅读次数统计-LeanCloud"><a href="#10-阅读次数统计-LeanCloud" class="headerlink" title="10. 阅读次数统计( LeanCloud )"></a>10. 阅读次数统计( LeanCloud )</h2><p><a href="https://notes.wanghao.work/2015-10-21-%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD.html#%E9%85%8D%E7%BD%AELeanCloud">原文</a></p>
<ul>
<li>LeanCloud 注册帐号</li>
<li>创建应用，名称自拟</li>
<li>创建<code>Class</code>，名称为<code>Counter</code>，选择无限制ACL权限</li>
<li>设置-应用Key，复制<code>App ID</code>和<code>App Key</code>到主题配置文件</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">leancloud_visitors:</span><br><span class="line">    enable: true</span><br><span class="line">    app_id: 0CYW8yvHIgva4K5nBgVMLQaS-gzGzoHsz</span><br><span class="line">    app_key: hQM50ybVtNLAsEM3VPIHYSSk</span><br></pre></td></tr></table></figure>

<h2 id="11-文章内anchor跳转"><a href="#11-文章内anchor跳转" class="headerlink" title="11. 文章内anchor跳转"></a>11. 文章内anchor跳转</h2><p>markdown语法中可以通过<code>[link](#tile name)</code>来跳转到特定的段落标题上，例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[link](#tile name)</span><br><span class="line">点击上面的连接会跳转到下面的标题</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">### title name</span><br><span class="line">跳转到上面的标题</span><br></pre></td></tr></table></figure>
<p>但是在hexo中md转换为html后标题中的空格会转换为<code>-</code>因此上述连接需修改为<code>[link](#tile-name)</code></p>
]]></content>
      <categories>
        <category>Tutorial 教程</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记《CityPersons: A Diverse Dataset for Pedestrian Detection》</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Pedestrian-Detection/Note-CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection/</url>
    <content><![CDATA[<p><strong>Abstract</strong></p>
<p>CNN在行人检测上的发展显著，但是考虑训练数据和模型架构上仍有需要解决的问题。我们回顾CNN的设计和适应性使得Faster R-CNN在Caltech上实现先进的结果。</p>
<p>为了从更多更好的数据中得到改进，我们提出<code>CityPersons</code>行人数据集，该数据集在Cityscapes数据集的基础上标注。<strong>CityPersons的多样性使得我们第一次实现训练一个的CNN模型能够在多个基准数据集上具有较好的泛化性能。</strong>并且使用CityPerson训练的FasterR-CNN模型在Caltech上实现更高的检测率和定位精度。</p>
<span id="more"></span>

<ul>
<li>CVPR 2017</li>
<li>CityPerson DataSet</li>
<li>ShanShan Zhang</li>
<li><a href="https://arxiv.org/abs/1702.05693">pdf</a></li>
</ul>
<p><strong>理解</strong></p>
<p>本文的创新的主要在两个方面：<br>    1.建立了一个行人检测数据集，包含丰富的信息；<br>    2.证明该数据集训练的分类器泛化性更好。</p>
<p>从作者张珊珊以前的工作看，她对如何标记行人检测数据集是有经验的。16年CVPR她就对caltech进行的重新标记，使得检测器性能有较大的提升。然后这一次又是在别人的数据集基础上进行标记，不同的是该数据集为语义分割数据集，本身没有行人检测所需的bounding box，然后她把在caltech上标记的技巧应用上来，并且利用语义分割信息，简化了可见区域的标注问题。但是本文最大的创新点还是该数据集对检测器泛化性能的影响。该影响可能会使得CityPersons数据集成为可见光行人检测领域的ImageNet。</p>
<p>最后，对目前工作有借鉴的是对Faster R-CNN的改动。改进的思路仍然是提高特征图尺寸、对CityPersons数据集调整proposal的候选尺寸、剔除干扰样本、使用更新的求解器。这些改进有的不是最优的，但是可以作为tracke在以后的优化处理上用到。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>行人检测是计算机视觉的流行领域，广泛的应用于监控、辅助驾驶、移动机器人等。近10年发布了多个行人检测基准数据集，使得该领域得到长足的发展。</p>
<p>虽然在这些基准数据集上相关方法的效果得到长足进步，但是这些方法在实际应用中的效果缺不明朗。我们认为是时候不仅关注数据集内的性能，而是跨数据集间的性能。</p>
<p>最近，一系列卷积神经网络的方法在Caltech鞠准数据集上得到较好的排名。大部分模型是基于Faster R-CNN的变种。我们提出对Faster R-CNN的适当调整就能实现这些变种相同的效果。然而，由于卷积网络是高容量模型，不清楚是否更多的数据能够使这类模型变得更好。</p>
<p>为了推动行人检测领域，我们提出“CityPersons”数据集，具有高质量的标注、丰富的多样性、可以用于训练新的模型或作为新的基准数据集。</p>
<p>我们的主要贡献是：</p>
<ol>
<li>提出CityPersons数据集，将公开，并提供在线评估。</li>
<li>使用CityPerson预训练的Faster R-CNN，通过适当的调整在Caltech和KITTI数据集上实现先进的结果，尤其是对于困难实验（小目标、遮挡），并且定位精度更高。</li>
<li>使用CityPersons能够实现更好的跨数据集的泛化性能。</li>
<li>我们的初步试验结果显示。使用语义标签作为增加的监督信息有希望能够改进小目标的检测。</li>
</ol>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-</a> A Diverse Dataset for Pedestrian_Attachments&#x2F;f10c.png)<br>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-</a> A Diverse Dataset for Pedestrian_Attachments&#x2F;f10d.png)</p>
<h3 id="1-1-Related-work"><a href="#1-1-Related-work" class="headerlink" title="1.1 Related work"></a>1.1 Related work</h3><p>本文，我们研究了卷积神经网络、数据集和语义标签用于行人检测，所以对这三个方面的相关工作进行讨论。</p>
<h3 id="卷积网络用于行人检测"><a href="#卷积网络用于行人检测" class="headerlink" title="卷积网络用于行人检测"></a>卷积网络用于行人检测</h3><p>卷积神经网络在ImageNet、Pascal、MS COCO数据集上实现了较好的分类和检测效果。Faster R-CNN已经成为了标准的检测器架构。许多研究尝试在此基础上扩展，但是少数几个效果更好的方法却具有更简单的架构。一个需要注意的例外的SSD[^23],其使用更简单的架构实现的相似的效果。</p>
<p>早期尝试将convnet用于行人检测的方法，使用传统检测器的输出（主要是决策森林和手工特征），然后convnet对他们进行重新打分（并进行bbox回归）。现在更好的方法是用convnet的输出作为proposal，然后用决策森林进行重新打分（利用卷积特征）。</p>
<p>本文我们提出对Faster R-CNN适当的调整就能实现现金的检测效果，而不需要其他的部件。</p>
<h3 id="行人数据集"><a href="#行人数据集" class="headerlink" title="行人数据集"></a>行人数据集</h3><p>过去十年间发布了多个行人检测数据集。INRIA，ETH，TudBrussels，Daimler，这些数据集被更大更流行的Caltech-USA和KITTI超过。这两个数据集都是基于车载视角在大城市中采集，并提供标注的视频序列。</p>
<p>尽管帧数多，但是数据集密度低。一张图像平均1人，遮挡情况数量不足。并且两个数据集都只在一个城市采集，因此行人和背景的多样性受限。</p>
<p>基于Cityscapes数据集的优势，我们在其基础上标注高质量的bbox，保护大量被遮挡的行人，27个不同的城市。数据集的多样性是的在CityPersons上训练的模型更具泛化性。</p>
<h3 id="语义标签用于行人检测"><a href="#语义标签用于行人检测" class="headerlink" title="语义标签用于行人检测"></a>语义标签用于行人检测</h3><p>在4.3节，我们采用CityPersons中的语义标签用于训练行人检测器，实现更好的语义建模。我们使用语义标记网络得到的语义概率图作为增加通道输入行人检测convnet中。</p>
<h2 id="2-一个行人检测convnet"><a href="#2-一个行人检测convnet" class="headerlink" title="2. 一个行人检测convnet"></a>2. 一个行人检测convnet</h2><p>首先建立一个较强的参考检测器，作为我们的试验工具。我们的目标是找到一个直接的架构实现在Caltech-USA数据集上更好的性能。</p>
<h3 id="训练、测试（-MR-O-MR-N-）"><a href="#训练、测试（-MR-O-MR-N-）" class="headerlink" title="训练、测试（$MR^O$,$MR^N$）"></a>训练、测试（$MR^O$,$MR^N$）</h3><p>我们训练Caltech模型使用改进的标注，更少的位置错误、更高的召回率、改进的忽略区域、更对齐的bbox）。遵循标准的Caltech评估方法；log miss-rate在0.01到1 FPPI之间。我们同时评估原始标记$MR^O$和改进标记$MR^N$。其他的设置均为Reasonable。</p>
<h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h3><p>使用Faster R-CNN的默认参数训练，在行人检测上效果不是太好。因为Faster R-CNN在处理小尺度目标（50~70pixel）事的效果不好，而这类尺度目标是Caltech等行人检测数据集上的主要尺寸。为了处理小行人，我们提出了5点改进，使得$MR^O$的漏检率从20.98降到10.27。在本论文写作的时候，Caltech上的最好成绩是9.6，我们的Faster R-CNN排名第三。</p>
<h3 id="改进1-量化-RPN-尺度"><a href="#改进1-量化-RPN-尺度" class="headerlink" title="改进1 量化 RPN 尺度"></a>改进1 量化 RPN 尺度</h3><p>Faster R-CNN中RPN的默认尺度是稀疏的[0.5 1 2]，并且认为目标尺度的分布是均匀的。然而，当我们观察Caltech的训练数据时，我们发现小尺度行人比大尺度多。我们的目标是让网络生成更多的小尺寸proposals，以至于更好的处理。我们将所有尺寸范围分解为10个小bin，每个bin的样本数量相同。最后使用11个点的尺寸作为RPN的proposal的尺寸。</p>
<h3 id="改进2-输入图像升采样"><a href="#改进2-输入图像升采样" class="headerlink" title="改进2 输入图像升采样"></a>改进2 输入图像升采样</h3><p>对输入图像进行升采样，能够提升$MR^O$3.74。我们将其原因归结于升采样后目标的尺寸与ImageNet目标尺寸的分布更接近。同时我们发现，使用更大的升采样因子不能实现更好的提升。</p>
<h3 id="改进3-细化特征步长"><a href="#改进3-细化特征步长" class="headerlink" title="改进3 细化特征步长"></a>改进3 细化特征步长</h3><p>Caltech中大部分行人的尺寸为80x40。VGG16的特征步长是16个像素。这样大的步长减少了bbox覆盖行人的机会，强迫网络处理偏移行人较大的bbox的识别。去除VGG16第四个max-pooling层使得特征的步长变为8pixels，帮助检测器更好的处理小目标。</p>
<h3 id="改进4-处理忽略区域"><a href="#改进4-处理忽略区域" class="headerlink" title="改进4 处理忽略区域"></a>改进4 处理忽略区域</h3><p>传统的Faster R-CNN是不处理忽略区域的。一般做法是将该区域认为是背景、混淆区域，这样会影响检测器的性能。通过避免选择忽略区域作为负样本训练RPN，我们得到了1.33 MR的提升。</p>
<h3 id="改进5-求解器"><a href="#改进5-求解器" class="headerlink" title="改进5 求解器"></a>改进5 求解器</h3><p>我们将Caffe标准的SGD求解器改为Adam，得到了性能提升。</p>
<p>表1显示了我们每个改进加入后的性能提升。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-</a> A Diverse Dataset for Pedestrian_Attachments&#x2F;t1.png)</p>
<h3 id="其他架构"><a href="#其他架构" class="headerlink" title="其他架构"></a>其他架构</h3><p>我们也探索了其他架构如SSD，MS-CNN等，但是调整之后没有得到改进的效果。在上述改进后，我们的Faster R-CNN得到10%的改进。</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>通过适当的调整，Faster R-CNN在Caltech数据集上能够实现较好的性能。我们在后续的实验中将使用该模型。</p>
<h2 id="3-CityPersons数据集"><a href="#3-CityPersons数据集" class="headerlink" title="3. CityPersons数据集"></a>3. CityPersons数据集</h2><p>Cityscapes数据集用于城市道路场景的语义分割任务。其包括一个很大且多样的立体视频序列，从德国和周边其他国家的多个城市采集。具有精细的像素级的标注信息，包含30个语义类别，超过5000个图像，采集自27个城市。精细的标注包括行人个体和车辆。另外20000张图像从其他23个城市采集，包括粗糙的语义标签，不标记个体标签。</p>
<p>本论文提出的CityPersons数据集，在Cityscapes基础上建立，为行人检测领域提供了一个新的有趣的数据集。在5000个精细标注的数据集上，我们为每个行人都建立了高质量bbox标记。第3.2节，我们比较了CityPersons与其他数据集在体积、多样性和遮挡上的差异。第4节，我们现实了新数据如何改进在其他数据集上的结果。</p>
<h3 id="3-1-Bounding-Box-标注"><a href="#3-1-Bounding-Box-标注" class="headerlink" title="3.1 Bounding Box 标注"></a>3.1 Bounding Box 标注</h3><p>Cityscapes已经提供了实例级别的标记。但是这些标记只提供了可见区域的像素。直接使用这些标记进行训练会有以下问题：1）box的比例是不规则的会影响bbox的归一化；2）没有对准每个行人，可能在水平和垂直方向上不是行人的中心。3）现有的数据集都是标记整个行人，而不只是可见区域。因此我们需要对这些行人进行重新标记。</p>
<h3 id="精细化分类"><a href="#精细化分类" class="headerlink" title="精细化分类"></a>精细化分类</h3><p>Cityscapes数据集对行人的分类为：person和rider。本文我们将所有humans分为四类：pedestrian（walking，running，standing up），rider（riding bicycles or motorbikes），sitting person，other person（非正常姿势）</p>
<h3 id="标注规则"><a href="#标注规则" class="headerlink" title="标注规则"></a>标注规则</h3><p>对于pedestrians和riders我们遵循相同的标注规则。我们对其头顶到两腿之间划一条线，然后bbox自动按照固定比例（0.41）生成。这种规则能够实现更加精确的对准效果。行人的可见区域可以通过segment mask自动生成。如图2所示。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-</a> A Diverse Dataset for Pedestrian_Attachments&#x2F;f2.png)</p>
<p>其他类别的person，如sitting和其他persons，没有统一的对准策略，所以我们只提供分割区域的bbox，而不是全身的bbox。</p>
<p>同时，我们要求标注者在图像中搜索包含虚假行人的区域，例如雕塑、海报、模特、镜子反光等，都被标记为ignore区域。</p>
<h3 id="标记工具"><a href="#标记工具" class="headerlink" title="标记工具"></a>标记工具</h3><p>由于我们已经有了每个个体的segment mask，我们标记更加快速。基于此，我们开发了一个新的标注工具，避免人工在图像上寻找分割实例。该工具自动提出一个行人分割区域，询问标注者对其进行细化分类。然后对其进行全身标记。感谢高质量的分割标记，使用该工具避免了遗漏行人，尤其是人气场景。但是忽略区域的标记还是需要人工在全图上搜索。</p>
<h3 id="3-2-统计"><a href="#3-2-统计" class="headerlink" title="3.2 统计"></a>3.2 统计</h3><h4 id="体积"><a href="#体积" class="headerlink" title="体积"></a>体积</h4><p>我们数据集的bbox标注的数量如表2所示。一共5000张图像，35k个行人，13k个忽略区域。训练、验证、测试集的设置按照Cityscapes。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-</a> A Diverse Dataset for Pedestrian_Attachments&#x2F;t2.png)</p>
<h4 id="多样性"><a href="#多样性" class="headerlink" title="多样性"></a>多样性</h4><p>我们比较了Caltech，KITTI，CityPersons的多样性如表3所示。由于KITTI测试集的标注非公开，我们只考虑训练集。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-</a> A Diverse Dataset for Pedestrian_Attachments&#x2F;t3.png)</p>
<p>CityPersons的训练集包括18个不同城市，3种季节，多种天气条件。Caltech和KITTI数据集只在一个城市、一个季节采集。</p>
<p>从密度上看，我们的数据集平均每个图像有7个行人，其他的大约只有1个行人。</p>
<p>独立的行人也是多样性的重要体现。我们的数据集中，独立行人接近20000个，其他两个分别为1300个，和6000个。注意，在KITTI和CityPersons中，采样间隔非常大，因此每个行人都认为是独立行人。</p>
<p>CityPersons提供了更加精细的标记。如图3所示，其中大部分是行人，骑行和坐着的人只有10%和5%。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-</a> A Diverse Dataset for Pedestrian_Attachments&#x2F;f3.png)</p>
<h4 id="遮挡"><a href="#遮挡" class="headerlink" title="遮挡"></a>遮挡</h4><p>Cityscapes数据集是通过车辆进行采集，其中包含一些著名城市的中心，如法兰克福、汉堡。有些图片中包含100多个行人，每个都有很多的遮挡。如此多的遮挡情况在其他数据集上是很少见的。图4显示了不同遮挡等级的行人的分布。我们注意到Caltech种包含60%的行人是完全可见的，而CityPersons中是30%。这表明，我们有更多的遮挡情况，这也使得我们的数据集对处理遮挡更有兴趣。并且在Resonable子集中，Caltech大部分都是非遮挡行人，而CityPersons得遮挡情况更多。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-</a> A Diverse Dataset for Pedestrian_Attachments&#x2F;f4.png)</p>
<p>为了更好的理解那种情况的遮挡更多，我们将所有遮挡模式量化为11种，图5显示了其中的9种。如图5所示。前两种遮挡基本覆盖了resonable，有55.9%。第三、第四个情况是左边或右边遮挡。除了这些之外，还有30%的其他遮挡类型。遮挡类型的分布多样性使得数据集更加具有挑战性。</p>
<h3 id="3-3-基准"><a href="#3-3-基准" class="headerlink" title="3.3 基准"></a>3.3 基准</h3><p>本论文发表时，我们将建立一个CityPersons的网站，包含训练和验证集下载。以及一个在线评估服务器用于计算提交的测试集的结果。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-</a> A Diverse Dataset for Pedestrian_Attachments&#x2F;f5.png)</p>
<p>我们遵循Caltech相同的评价规则，分为不同的自己进行评估。本文中MR代表log平均漏检率，在Resonable设置下。在评估行人检测性能时，cyclists、sitting、其他行人、忽略区域不予考虑，同时，检测到该区域的部分不认为是虚警。</p>
<h3 id="3-4-基准实验"><a href="#3-4-基准实验" class="headerlink" title="3.4 基准实验"></a>3.4 基准实验</h3><p>为了更好的理解CityPersons数据集的困难，我们用三个检测器训练评估。ACF、Checkerboard、Faster R-CNN。我们队Faster R-CNN的设置从Caltech实践经验中得到。由于CityPersons的每张图像有7个行人，因此我们只采用1.3倍升采样来匹配12GB的GPU内存。</p>
<p>我们使用CityPersons进行再训练，然后用验证集验证。图6显示了三个不同的检测器在两个数据集上的对比。FasterRCNN相比于ICF检测器有巨大提升，三个检测器的排名在两个数据集上相同，并且CityPersons得分更低，说明更具挑战性。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-</a> A Diverse Dataset for Pedestrian_Attachments&#x2F;f6.png)</p>
<p>为了理解数据量队训练效果的影响，我们显示了随着数据增加的性能的提升如图7所示。我们可以看见随着数据的增加漏检率降低，说明数据量对CNN模型有较大影响。</p>
<p>考虑到速度和质量的权衡，我们使用交替模型来开关是否升采样，速度快了两倍，但是性能降低2%。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-</a> A Diverse Dataset for Pedestrian_Attachments&#x2F;f7.png)</p>
<h2 id="4-使用CityPersons改进质量"><a href="#4-使用CityPersons改进质量" class="headerlink" title="4. 使用CityPersons改进质量"></a>4. 使用CityPersons改进质量</h2><p>CityPersons在手，我们有两个不同的方法提升行人检测结果。我们将会看到，CityPersons能够提升小尺度行人、遮挡、的检测率，并且定位更精确。</p>
<h3 id="4-1-跨数据集的泛化性"><a href="#4-1-跨数据集的泛化性" class="headerlink" title="4.1 跨数据集的泛化性"></a>4.1 跨数据集的泛化性</h3><p>通常，一个检测器在基准集的训练集上训练。因此在多个基准集评价时都进行重新训练。然而，我们希望在一个数据集上训练然后在多个基准集的效果都很好。由于CityPersons的大量、多样性，我们考虑，这个数据集能否训练出一个具有良好泛化性的检测器呢？</p>
<p>为了体现CityPersons的跨数据集的泛化能力，我们分别在Caltech、KITTI、CityPersons上训练分类器，然后将他们在6个不同的数据集上进行测试。检测器分别为ACF和Faster R-CNN。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-</a> A Diverse Dataset for Pedestrian_Attachments&#x2F;t4.png)</p>
<p>总体上看Faster R-CNN比ACF的泛化性更好。CityPersons训练的分类器的平均MR比其他两个数据集好。</p>
<h3 id="4-2-更好的预训练改进质量"><a href="#4-2-更好的预训练改进质量" class="headerlink" title="4.2 更好的预训练改进质量"></a>4.2 更好的预训练改进质量</h3><p>我们可以发现，CityPersons可以作为很好的训练原数据，应用于不同的数据集。即可以用CityPersons进行预训练，或者是额外的训练数据来改进性能。</p>
<p>例如，我们的目标域是Caltech，我们可以先在CityPersons上进行训练，然后在Caltech上finetune。表5显示了性能的提升。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-</a> A Diverse Dataset for Pedestrian_Attachments&#x2F;t5.png)</p>
<p>同理，在KITTI上也是。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-</a> A Diverse Dataset for Pedestrian_Attachments&#x2F;t6.png)</p>
<h3 id="4-3-开发Cityscapes的语义标签"><a href="#4-3-开发Cityscapes的语义标签" class="headerlink" title="4.3 开发Cityscapes的语义标签"></a>4.3 开发Cityscapes的语义标签</h3><p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-</a> A Diverse Dataset for Pedestrian_Attachments&#x2F;f9.png)</p>
<p>使用FCN-8s在Cityscapes的粗糙语义标记图上进行训练，将FCN-8s的结果作为一个通道输入给convnet作为语义补充信息。实现大约0.6%的提升。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/CityPersons-</a> A Diverse Dataset for Pedestrian_Attachments&#x2F;t7.png)</p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Pedestrian Detection</category>
      </categories>
      <tags>
        <tag>Pedestrian Detection</tag>
        <tag>Datasets</tag>
        <tag>CityPersons</tag>
      </tags>
  </entry>
  <entry>
    <title>VPS搭建VPN（shadowsocks和v2ray）</title>
    <url>/Tutorial-%E6%95%99%E7%A8%8B/VPN-shadowsocks-and-v2ray/</url>
    <content><![CDATA[<p>最近跟什么大开会相关，好像大部分的VPN都失效了。蓝灯（Lantern）稳定性速度都不错但是每个月500MB的流量还是不够用。因此，还是考虑自己搭一个VPN来得稳妥。</p>
<h2 id="1-方案准备"><a href="#1-方案准备" class="headerlink" title="1. 方案准备"></a>1. 方案准备</h2><ul>
<li>VPS服务商：Bandwagon（搬瓦工）</li>
<li>VPN Server：shadowsocks 和 <del>v2ray</del>（速度没有shadowsocks稳定）</li>
<li>VPN 软件：showdsocks（Win，Ubuntu，android） 和 <del>shadowray</del> SuperWingy（iOS）</li>
</ul>
<span id="more"></span>

<h2 id="2-购买Bandwagon服务器"><a href="#2-购买Bandwagon服务器" class="headerlink" title="2. 购买Bandwagon服务器"></a>2. 购买Bandwagon服务器</h2><p>我采用的是搬瓦工，其实VPS有很多，之所以选择它是因为用的人多，并且价格合适，还提供一键安装shadowsocks的傻瓜工具。其他的VPN可以去这里选：<a href="https://www.vpser.net/ten-dollars-vps">VPS列表</a></p>
<p>在搬瓦工上搭建shadowsocks的教程主要参考<a href="http://blog.csdn.net/win_turn/article/details/51559867">这里</a>。</p>
<p>首先登录官网</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://bandwagonhost.com/index.php</span><br><span class="line">https://bwh1.net</span><br></pre></td></tr></table></figure>
<p>或者你愿意点下我的分享连接也很感谢</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://bandwagonhost.com/aff.php?aff=20801</span><br></pre></td></tr></table></figure>
<p>登录网站后，点击导航栏中【VPS Hosting】按钮，如下图：<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/0.5344765409653371.png" alt="0.5344765409653371"></p>
<p>因为只是用于VPN，所以选择最低配置就足够了。点击【ORDER】按钮订购，如下图：<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/0.80205022123858.png" alt="0.80205022123858"></p>
<p>如果没有最低配的那就选择高一个级别，当然价格也更贵一点。注意：Bandwidth是一个月的流量，最低配是500G，不是天天看高清电影应该是够了。</p>
<p>进入配置页面，Billing Cycle项选择【annually】，表示“年付”，Configurable Options项选择【US West Coast - Los Angeles(USCA_2)】，表示我们的服务器地址选择“美国西海岸-洛杉矶”。然后点击【Add to Cart】按钮加入购物车，如下图：<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/0.4624321167139738.png" alt="0.4624321167139738"></p>
<p>进入购物车页面，如下图：<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/0.7658803460564967.png" alt="0.7658803460564967"></p>
<p>优惠码可以去这个网址查看：<code>https://www.banwagong.com/youhuima</code><br>点击【Checkout】按钮结账。进入个人资料页面，如果您已经有账户，点击【Click here to login】登录您的账户；如果没有账户，那么就按照要求输入个人信息。如下图：<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/0.058923141948659996.png" alt="0.058923141948659996"></p>
<p>填写完成后，点击【Update】更新个人信息。</p>
<p>然后，在【Payment Method】付款方式项中，选择【Credit Card and AliPay (Stripe)】，表示选择信用卡或支付宝方式进行付款。然后勾选上【 I have read and agree to the Terms of Service】前面的选择框，然后点击【Complete Order】按钮，进入付款页面。如下图：<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/0.30879515471342.png" alt="0.30879515471342"></p>
<p>点击【Pay now】按钮，开始付款，直接跳转到支付宝页面，这个就不截图了，中国人嘛，还能不会用支付宝？</p>
<h2 id="3-搭建shadowsocks"><a href="#3-搭建shadowsocks" class="headerlink" title="3. 搭建shadowsocks"></a>3. 搭建shadowsocks</h2><p>点击右上角【Client Area】按钮，如下图：<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/0.6043703378018064.png" alt="0.6043703378018064"></p>
<p>跳转到登录界面，输入电子邮箱地址和密码（刚才注册时输入的密码），点击【Login】按钮，如下图：<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/0.7973071788231076.png" alt="0.7973071788231076"></p>
<p>登录成功后，点击【Services】服务器按钮，在弹出的下拉选项中，点击【My Services】我的服务器按钮。如下图：<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/0.036330636450951914.png" alt="0.036330636450951914"></p>
<p>跳转到服务器列表页面。点击【KiwiVM Control Panel】按钮，如下图：<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/0.1154515450985989.png" alt="0.1154515450985989"></p>
<p>跳转到控制面板页面，我们先来熟悉一下这里展示的服务器信息。如下图：<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/0.8761737666956608.png" alt="0.8761737666956608"></p>
<p>左侧有直接建立shadowsocks的工具<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/6af293b1-4295-499d-ae56-103a68d822c1.png" alt="6af293b1-4295-499d-ae56-103a68d822c1"></p>
<p>打开后自动执行，完成后跳转会这个页面即可看到：<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/6489ec0a-470d-48f9-8b8c-67b866b0edc8.png" alt="6489ec0a-470d-48f9-8b8c-67b866b0edc8"></p>
<p>根据上面的介绍可以下载shadowsocks的客户端，填写配置，就完成了。</p>
<h2 id="4-搭建v2ray用shadowray-iOS"><a href="#4-搭建v2ray用shadowray-iOS" class="headerlink" title="4. 搭建v2ray用shadowray-iOS"></a>4. 搭建v2ray用shadowray-iOS</h2><p><strong>我建议使用 SuperWingy 在 iOS 上使用 Shadowsocks，因为根据个人体验 shadowray+v2ray 速度和链接都不太稳定，而且两个app都要买。所以，如果你用 SuperWingy 后面就不用看了。</strong></p>
<p>上面的搭建完成后，一个主要问题是iOS版的应用已经被下架了。但是shadowsocks的作者又发布了一个shadowray（要12块，在app store里搜索），其基于v2ray。<br>下面的教程基于：<a href="http://www.liyonge.com/2017/10/11/v2ray/#.Wd9EJicwH0M.twitter">http://www.liyonge.com/2017/10/11/v2ray/#.Wd9EJicwH0M.twitter</a></p>
<p>首先用一个ssh客户端登录你的vps，用root用户</p>
<p>自动安装脚本：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bash &lt;(curl -L -s https://install.direct/go.sh)</span><br></pre></td></tr></table></figure>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/4f0ec3c5-245f-469c-8971-8844d8fe898a.png" alt="4f0ec3c5-245f-469c-8971-8844d8fe898a">上面的PORT是端口号，UUID是你服务的ID，在后面shadowray的设置上需要<br>可以看到这里出现了问题，主要原因是我使用的centos 6，而v2ray的启动方式需要centos 7，但是如果你升级到centos 7，搬瓦工的一键安装shadowsocks又不支持了。还好找到了解决方法。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://github.com/v2ray/v2ray-core/issues/101#issuecomment-214670792%5D</span><br></pre></td></tr></table></figure>
<p>将下面的脚本替换&#x2F;etc&#x2F;init.d&#x2F;v2ray，如果已经有了这个文件，那先删除。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line">#</span><br><span class="line"># v2ray        Startup script for v2ray</span><br><span class="line">#</span><br><span class="line"># chkconfig: - 24 76</span><br><span class="line"># processname: v2ray</span><br><span class="line"># pidfile: /var/run/v2ray.pid</span><br><span class="line"># description: V2Ray proxy services</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line">### BEGIN INIT INFO</span><br><span class="line"># Provides:          v2ray</span><br><span class="line"># Required-Start:    $network $local_fs $remote_fs</span><br><span class="line"># Required-Stop:     $remote_fs</span><br><span class="line"># Default-Start:     2 3 4 5</span><br><span class="line"># Default-Stop:      0 1 6</span><br><span class="line"># Short-Description: V2Ray proxy services</span><br><span class="line"># Description:       V2Ray proxy services</span><br><span class="line">### END INIT INFO</span><br><span class="line"></span><br><span class="line">DESC=v2ray</span><br><span class="line">NAME=v2ray</span><br><span class="line">DAEMON=/usr/bin/v2ray/v2ray</span><br><span class="line">PIDFILE=/var/run/$NAME.pid</span><br><span class="line">LOCKFILE=/var/lock/subsys/$NAME</span><br><span class="line">SCRIPTNAME=/etc/init.d/$NAME</span><br><span class="line">RETVAL=0</span><br><span class="line"></span><br><span class="line">DAEMON_OPTS=&quot;-config /etc/v2ray/config.json&quot;</span><br><span class="line"></span><br><span class="line"># Exit if the package is not installed</span><br><span class="line">[ -x $DAEMON ] || exit 0</span><br><span class="line"></span><br><span class="line"># Read configuration variable file if it is present</span><br><span class="line">[ -r /etc/default/$NAME ] &amp;&amp; . /etc/default/$NAME</span><br><span class="line"></span><br><span class="line"># Source function library.</span><br><span class="line">. /etc/rc.d/init.d/functions</span><br><span class="line"></span><br><span class="line">start() &#123;</span><br><span class="line">  local pids=$(pgrep -f $DAEMON)</span><br><span class="line">  if [ -n &quot;$pids&quot; ]; then</span><br><span class="line">    echo &quot;$NAME (pid $pids) is already running&quot;</span><br><span class="line">    RETVAL=0</span><br><span class="line">    return 0</span><br><span class="line">  fi</span><br><span class="line"></span><br><span class="line">  echo -n $&quot;Starting $NAME: &quot;</span><br><span class="line"></span><br><span class="line">  mkdir -p /var/log/v2ray</span><br><span class="line">  $DAEMON $DAEMON_OPTS 1&gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line">  echo $! &gt; $PIDFILE</span><br><span class="line"></span><br><span class="line">  sleep 2</span><br><span class="line">  pgrep -f $DAEMON &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">  RETVAL=$?</span><br><span class="line">  if [ $RETVAL -eq 0 ]; then</span><br><span class="line">    success; echo</span><br><span class="line">    touch $LOCKFILE</span><br><span class="line">  else</span><br><span class="line">    failure; echo</span><br><span class="line">  fi</span><br><span class="line">  return $RETVAL</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stop() &#123;</span><br><span class="line">  local pids=$(pgrep -f $DAEMON)</span><br><span class="line">  if [ -z &quot;$pids&quot; ]; then</span><br><span class="line">    echo &quot;$NAME is not running&quot;</span><br><span class="line">    RETVAL=0</span><br><span class="line">    return 0</span><br><span class="line">  fi</span><br><span class="line"></span><br><span class="line">  echo -n $&quot;Stopping $NAME: &quot;</span><br><span class="line">  killproc -p $&#123;PIDFILE&#125; $&#123;NAME&#125;</span><br><span class="line">  RETVAL=$?</span><br><span class="line">  echo</span><br><span class="line">  [ $RETVAL = 0 ] &amp;&amp; rm -f $&#123;LOCKFILE&#125; $&#123;PIDFILE&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">reload() &#123;</span><br><span class="line">  echo -n $&quot;Reloading $NAME: &quot;</span><br><span class="line">  killproc -p $&#123;PIDFILE&#125; $&#123;NAME&#125; -HUP</span><br><span class="line">  RETVAL=$?</span><br><span class="line">  echo</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">rh_status() &#123;</span><br><span class="line">  status -p $&#123;PIDFILE&#125; $&#123;DAEMON&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># See how we were called.</span><br><span class="line">case &quot;$1&quot; in</span><br><span class="line">  start)</span><br><span class="line">    rh_status &gt;/dev/null 2&gt;&amp;1 &amp;&amp; exit 0</span><br><span class="line">    start</span><br><span class="line">    ;;</span><br><span class="line">  stop)</span><br><span class="line">    stop</span><br><span class="line">    ;;</span><br><span class="line">  status)</span><br><span class="line">    rh_status</span><br><span class="line">    RETVAL=$?</span><br><span class="line">    ;;</span><br><span class="line">  restart)</span><br><span class="line">    stop</span><br><span class="line">    start</span><br><span class="line">    ;;</span><br><span class="line">  reload)</span><br><span class="line">    reload</span><br><span class="line">  ;;</span><br><span class="line">  *)</span><br><span class="line">    echo &quot;Usage: $SCRIPTNAME &#123;start|stop|status|reload|restart&#125;&quot; &gt;&amp;2</span><br><span class="line">    RETVAL=2</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br><span class="line">exit $RETVAL</span><br></pre></td></tr></table></figure>
<p>vi的操作这里就不说了，自行百度。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ sudo chmod +x /etc/init.d/v2ray</span><br><span class="line">$ sudo chkconfig v2ray on</span><br><span class="line">$ sudo service v2ray start</span><br></pre></td></tr></table></figure>
<p>看到下面这个就代表v2ray启动了</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/6b261b23-7141-4112-9bbd-6c76a0a0583f.png" alt="a3ad5759-4019-437a-8f55-ddd2e6e2b78a"></p>
<p>去app store下载shadowray<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/a3ad5759-4019-437a-8f55-ddd2e6e2b78a.jpg" alt="a3ad5759-4019-437a-8f55-ddd2e6e2b78a"></p>
<p>在里面add server，把你的服务器的ip，v2ray的PORT，UUID填入进去。Done。<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/VPS%E6%90%AD%E5%BB%BAVPN%EF%BC%88shadowsocks%E5%92%8Cv2ray%EF%BC%89_files/cd33c834-c746-410d-8392-d97b53270302.jpg" alt="cd33c834-c746-410d-8392-d97b53270302"></p>
]]></content>
      <categories>
        <category>Tutorial 教程</category>
      </categories>
      <tags>
        <tag>shadowsocks</tag>
        <tag>v2ray</tag>
        <tag>vpn</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记《A unified multi-scale deep convolutional neural network for fast object detection》</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Pedestrian-Detection/Note-A-unified-multi-scale-deep-convolutional-neural-network-for-fast-object-detection/</url>
    <content><![CDATA[<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/MS-CNN/1508297031192.png" alt="1508297031192"></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文主要提出了一个多分支的检测网络来匹配多尺度目标的感知域和anchor尺寸。</p>
<ul>
<li>multi-scale CNN for fast multi-scale object detection</li>
<li>proposal sub-network detect at multi output layer</li>
<li>scale-specific detectors combined</li>
<li>optimizing a multi-task loss</li>
<li>feature upsampling by deconvolution</li>
</ul>
<span id="more"></span>


<ul>
<li>Multi-scale CNN (MS-CNN)</li>
<li>UC San Diego, IBM</li>
<li><a href="https://github.com/zhaoweicai/mscnn">code</a></li>
<li><a href="https://www.jianguoyun.com/p/DfCoF8IQoY3_BRiisTc">pdf</a></li>
</ul>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p><strong>Motivation</strong></p>
<ul>
<li>目标检测主要基于滑窗法来搜索多尺度多比例的目标。</li>
<li>这些方法虽然在人脸、行人检测能够实现实时的检测，但是在多类目标检测却很困难</li>
<li>R-CNN 利用预选的ROI，将其缩放到224 $\times$ 224 用CNN进行分类，这样在计算效率底下</li>
<li>Faster R-CNN 固定了 receptive field 感知域（这里主要指RPN）</li>
</ul>
<p><strong>Invention</strong></p>
<ul>
<li>MS-CNN</li>
<li>为了目标尺寸和感知域不一致的问题，proposal 网络在多层进行预测，每层针对特定尺寸的目标</li>
<li>各个层的检测器组合成一个strong multi-scale detector</li>
<li>对特征图升采样替代对图像升采样，能够减少计算量和内存</li>
</ul>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>最早实现实时检测是级联 (cascaded) 检测器<code>VJ</code>[^1] ，之后有两条研究路径提高效率：</p>
<ul>
<li><p>快速特征提取：VJ使用积分图计算Haar特征，之后ACF在其基础上实现100fps的HOG特征计算。</p>
</li>
<li><p>级联学习：软级联[^14]，拉格朗日公式学习级联[^17]，学习异质 (heterogeneous) 特征[^15]</p>
</li>
<li><p>级联检测器的问题是，在此框架下很难实现多类检测</p>
</li>
</ul>
<p>此后，相关研究尝试使用深度神经网络提升目标分类</p>
<ul>
<li>R-CNN，组合了目标推荐机制和CNN分类器，但是速度受到proposal数量和重复CNN计算的限制</li>
<li>SPPnet，通过空间金字塔池化，实现一张图片只计算一次特征图，速度提升了一个数量级</li>
<li>Fast R-CNN，提出了通过ROI池化和多任务学习（分类+回归）进行反向传播的思想，但是仍然依赖一个独立的proposal</li>
<li>Faster R-CNN，使用相同的神经网络，从而极大的加快proposal的速度。</li>
<li>YOLO，是另一个有趣的尝试，其能够实现~40 fps，但是会妥协一些精度</li>
</ul>
<p>对于目标检测，一些研究表明在单一网络中组合中间层是有益的。</p>
<ul>
<li>GoogLeNet[^22], 提出在中间的高层网络上使用三加权分类损失，这样正则化方法对深层模型有效</li>
<li>Full CNN[^11], 高层具有更高的语义信息，高层与中间层组合后实现更精确的语义分割。</li>
</ul>
<p>MS-CNN与上面的方法类似，也从中间层计算损失，但是其目的不是正则化学习，而是为了提供更多的细节信息。并且是对每个中间层生成一个独立的目标检测器。</p>
<h2 id="3-多尺度目标推荐网络-Multi-scale-Object-Proposal-Network"><a href="#3-多尺度目标推荐网络-Multi-scale-Object-Proposal-Network" class="headerlink" title="3. 多尺度目标推荐网络 Multi-scale Object Proposal Network"></a>3. 多尺度目标推荐网络 Multi-scale Object Proposal Network</h2><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/MS-CNN/1508332598574.png" alt="1508332598574"></p>
<h3 id="3-1-多尺度检测"><a href="#3-1-多尺度检测" class="headerlink" title="3.1 多尺度检测"></a>3.1 多尺度检测</h3><blockquote>
<p>覆盖各种尺寸的目标是目标检测的重要问题。</p>
</blockquote>
<ul>
<li>单一模型，多尺度图像和特征图，一般具有较好的精度，但是计算开销大</li>
<li>多个模型，单一图像尺寸和特征图，避免重复计算特征，但是对每个尺度生成一个检测器</li>
<li>多个模型，多个尺度图像和特征图，这样是上述两个方法的折中</li>
<li>单一模型，几个尺度图像和多个尺度特征图（估计），通过差值计算得到中间缺失的特征图</li>
</ul>
<p>基于CNN的多尺度策略与上面的一些区别</p>
<ul>
<li>将输入的目标区域全部缩放到同一尺寸计算卷积特征，R-CNN，类似a</li>
<li>RPN，是在同一个特征图上将多尺寸的ROI生成相同尺寸的模型（一个检测器），类似b</li>
<li>收到c的启发，我们提出一个新的多尺度策略，从单一输入图像中，在多个中间特征层上进行多尺度的目标检测（后面SSD，FPN的做法也很类似）</li>
</ul>
<h3 id="3-2-网络结构"><a href="#3-2-网络结构" class="headerlink" title="3.2 网络结构"></a>3.2 网络结构</h3><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/MS-CNN/1508336633269.png" alt="1508336633269"></p>
<p>该网络有多个检测分支，每个检测分支都是最终的proposal检测结果。该网络有个标准的主干CNN，一组单一检测分支。</p>
<p>注意，这里在conv4-3之后开始建立检测分支，因为在之前，回传的梯度会对后面的检测结果具有较大影响，造成训练的不稳定。</p>
<p>多分支的联合损失</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/MS-CNN/1508338563779.png" alt="1508338563779"></p>
<ul>
<li>$W$是网络参数</li>
<li>$Y_i&#x3D;(y_i,b_i)$是Ground Truth</li>
<li>$X_i$是训练图像块</li>
<li>$a_m$是损失权重</li>
<li>$S&#x3D;{(X_i,Y_i)}_{i&#x3D;1}^N&#x3D;{S^1,S^2,\dots,S^M}$ 是训练样本</li>
<li>$M$ 是检测分支的数量</li>
</ul>
<p>注意，属于$S^m$的样本只对其分支贡献损失，这个样本在训练之前会规划好。</p>
<p>损失$l^m$采用的是Faster R-CNN之前采用的方法，联合分类和回归损失</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/MS-CNN/1508339139882.png" alt="1508339139882"></p>
<ul>
<li>一个是多类分类损失</li>
<li>一个是定位损失，背景样本不提供定位损失</li>
</ul>
<h3 id="3-3-采样"><a href="#3-3-采样" class="headerlink" title="3.3 采样"></a>3.3 采样</h3><p>对于每个检测层，$S^m&#x3D;{S^m _ +,S^m _ -}$   </p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/MS-CNN/1508383544192.png" alt="1508383544192"></p>
<p>anchor尺寸是与filter的尺寸相关的，anchor与标记IoU$\ge$0.5认为是正样本，$\le$0.2认为是负样本。$|S_-|&#x3D;\gamma|S_+|$</p>
<p>对于自然图像，目标和非目标的数量具有极大的不平衡。采样是为了补偿这种不平衡。考虑三种采样策略：随机，bootstrapping，混合。</p>
<ul>
<li>随机采样获得的样本大量为随机样本，我们知道困难样本挖掘能提升性能</li>
<li>Bootstrapping策略，用物体性得分对样本排序。然后从高到低收集负样本</li>
<li>混合策略，就是随和和bootstrapping一半一半，在我们的实验中其效果和bootstrapping相似</li>
</ul>
<p>为了保证每个检测层只检测特定尺度范围的目标，训练集按照相应的范围组织。但是，在一张图中，一些尺度可能没有正样本，导致正负样本不平衡$|S_-|&#x2F;|S_+|\gg\gamma$ ，导致学习不稳定。为了解决这个问题，交叉熵损失修改为：</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/MS-CNN/1508399094430.png" alt="1508399094430"></p>
<h3 id="3-4-实现细节"><a href="#3-4-实现细节" class="headerlink" title="3.4 实现细节"></a>3.4 实现细节</h3><p><strong>数据增强</strong> 在[^4][^6] 中说，多尺度的训练不必要，因为深度神经网络适应尺度不变性。然而，在Caltech和KITTI这类数据集上不成立，因为目标的尺度范围可以跨多个倍数（octaves）。但是，不同尺寸的正样本数量也具有很大的差别。为了解决这个不平衡，对原图像进行随机缩放为多个尺度。</p>
<p><strong>Fine-tuning</strong> Fast R-CNN 和 RPN原始训练一个小的mini-batch需要大量的内存，但是一个图像中有很多区域是没有用的背景。为了节省内存，我们在目标周围裁剪一个448 $\times$ 448 的图像块。这样能够极大的减少内存的需求，从而使得mini-batch能够适应4张图片，对一个12G内存的GPU。</p>
<p>网络采用流行的VGG16初始化。由于采用了bootstrapping和多任务损失，可能导致在早期的迭代中不稳定。因此采用two-stage步骤。</p>
<ul>
<li>第一阶段，随机采样，定位损失系数小（$\lambda&#x3D;$0.05），迭代10000次，学习率0.00005。</li>
<li>第二阶段，bootstrapping采样，定位损失系数（$\lambda&#x3D;$1），“det-8”的检测分支损失$a_i&#x3D;0.9$，其他检测分支系数为1，迭代25000次，学习率0.00005，每10000次缩小10倍。</li>
</ul>
<p>two-stage训练流程能够实现稳定的多任务训练。</p>
<h2 id="4-目标检测网络"><a href="#4-目标检测网络" class="headerlink" title="4. 目标检测网络"></a>4. 目标检测网络</h2><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/MS-CNN/1508403256331.png" alt="1508403256331"></p>
<p>RPN本身虽然也可以作为检测器，但滑窗不能很好覆盖目标，所以不是很好。为了提高精度，因此增加了检测网络。ROI pooling层在特征图上提取固定维度的特征（7$\times$7$\times$512），然后送入fc层，如图4所示。这里增加了一个反卷机层来提升特征图分辨率。因此，多任务损失扩展为：</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/MS-CNN/1508403584903.png" alt="1508403584903"></p>
<p>其中$l^{M+1}$和$S^{M+1}$损失和训练样本对于检测子网络。$S^{M+1}$是用Fast R-CNN方式采集。检测子网络与proposal子网络共享参数W，然后增加了一些参数$W_d$。ROI pooling应用于”conv4-3”具有更好的效果。可能的原因是”conv4-3”相对更高的分辨率，更适合位置感知的bounding box回归。</p>
<h3 id="4-1-CNN-特征图估计"><a href="#4-1-CNN-特征图估计" class="headerlink" title="4.1 CNN 特征图估计"></a>4.1 CNN 特征图估计</h3><p>升采样虽然能提升小目标检测效果，但是有三个副作用：</p>
<ul>
<li>内存需求增加</li>
<li>训练、测试慢</li>
<li>不会增加图像信息</li>
</ul>
<p>我们考虑用更有效的方法提升特征图分辨率：</p>
<ul>
<li>这个做法类似上面的图2（d）不是通过对图像缩放，而是对特征图缩放，采用最小二乘法估计</li>
<li>在CNN中，使用反卷积更好</li>
<li>特征图缩放，不会带来太多的计算和内存成本</li>
</ul>
<p>据我们所知，这是第一次应用翻卷机到目标检测中，用于改善速度和精度</p>
<h3 id="4-2-嵌入语义"><a href="#4-2-嵌入语义" class="headerlink" title="4.2 嵌入语义"></a>4.2 嵌入语义</h3><p>在文献[^7][^5][^26]中表明语义信息对目标检测有用。本文专注于从多个区域中提取语义。如图4所示，我们从目标区域（绿色）和语义区域（蓝色）提取特征，然后堆叠在一起。语义区域是目标区域的1.5倍，后面增加一个卷积层减少通道数，从而保证在不增加模型参数的同时不损失精度。</p>
<h3 id="4-3-实现细节"><a href="#4-3-实现细节" class="headerlink" title="4.3 实现细节"></a>4.3 实现细节</h3><p>用3.4节描述的proposal网络训练好的结果初始化。学习率为0.0005，每10000次迭代减小10倍，25000次迭代后终止。通过反向传播优化（6）。使用Bootstrapping采样策略，$\lambda&#x3D;1$，前两层”conv1-1”和”conv2-2”的参数固定，用于加速训练。</p>
<h2 id="5-实验评估"><a href="#5-实验评估" class="headerlink" title="5. 实验评估"></a>5. 实验评估</h2><p>我们在KITTI和Caltech上评估MS-CNN检测器，不同于VOC和ImageNet，这两个数据集中有更多的小目标。由于KITTI没有测试集标注，我们参考[^5]的方法，将训练集分为训练和验证集，并且训练一个用于车辆检测，另一个用于行人&#x2F;骑车人检测。表1描述了模型的对多尺度目标所使用的不同anchor。硬件平台E5-2630单核，64GB内存，TITAN GPU。</p>
<h3 id="5-1-Proposal-评估"><a href="#5-1-Proposal-评估" class="headerlink" title="5.1 Proposal 评估"></a>5.1 Proposal 评估</h3><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/MS-CNN/1508419232345.png" alt="1508419232345"></p>
<p>使用oracle recall用于评估指标[^31]。car的IOU大于等于70%，行人、骑车人IOU大于等于50%。</p>
<p><strong>独立检测分支的作用</strong> 表2显示了不同检测分支与行人高度的检测精度关系。与期望的一致，尺度匹配的精度越高。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/MS-CNN/1508421600322.png" alt="1508421600322"></p>
<p><strong>输入图像尺寸的作用</strong> 图5显示了proposal网络与输入图像尺寸的关系。行人和车辆的proposal网络对图像尺寸较为鲁棒，骑车行人从384-576性能有提升。这说明，proposal网络不需要增加图像输入尺寸也能得到较好的proposal结果。</p>
<p><strong>检测子网络改进推荐子网络</strong> 论文[^4]表明多任务学习能有利于bb回归和分类。另一方面，论文[^9]显示，两个任务共享特征也并不能改进proposal。图5显示，多任务学习能有效帮助proposal的生成，尤其是对于行人。（这个跟我所实验的不一样，检测和推荐共享网络会相互干扰，原因可能是用）</p>
<p><strong>与其他先进方法比较</strong> 图6比较了多种方法：BING[^32], Selective Search[^8], EdgeBoxes[^33], MCG[^34], 3DOP[^5], RPN[^9]。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/MS-CNN/1508423079688.png" alt="1508423079688"></p>
<p>图像第一排是IoU确定时，召回率与候选样本数量的关系。第二排是100个候选样本，召回率与IoU的关系。MS-CNN在只有100个proposal的情况下实现了98%的召回率。有监督学习的方法比无监督的方法好。RPN与MS-CNN的结果最接近，RPN对图像进行了两倍的上采样。MS-CNN能够与GT有高度的重合，说明了回归网络的作用。</p>
<h3 id="5-2-目标检测评估"><a href="#5-2-目标检测评估" class="headerlink" title="5.2 目标检测评估"></a>5.2 目标检测评估</h3><p>由于cyclist数量较少，本实验只考虑car 和 pedestrian。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/MS-CNN/1508423826969.png" alt="1508423826969"></p>
<p><strong>输入图像上采样的影响</strong> 表3显示了输入上采样是检测的很重要的因素。上采样1.5~2倍时能显著的改善性能。（这里有个问题，图像上采样之后，网络每个层的感知域就变小了，所以大目标精度下降，中小目标提升）</p>
<p><strong>采样策略</strong> 表3比较了随机采样，bootstrapping采样，混合采样。对于车辆这三种没有太大区别，对于行人，随机采样效果更差。</p>
<p><strong>CNN 特征估计</strong> 我们尝试了三种学习反卷积的方法：1）双线性差值权重（bilinearly interpolated weights）；2）双线性差值权重初始化，然后通过反向传播学习；3）高斯噪声初始化，然后反向传播学习。我们发现第一种方法最好。表3中显示，反卷积能在输入图像尺寸较小时提升。</p>
<p><strong>嵌入语义</strong> 嵌入语义信息后，精度也有提升，但是参数会增加，因此对通道缩减能有效解决问题。</p>
<p><strong>基于proposal的目标检测</strong> 相比于proposal，增加了detection后结果提升。表4是公开的KITTI上的结果。</p>
<p><strong>在KITTI上比较</strong> MS-CNN使用“h768-ctx-c”模型（所以没用反卷积？还是直接增加输入图像尺寸来的有效？）</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/MS-CNN/1508425810979.png" alt="1508425810979"></p>
<p><strong>Caltech上的效果</strong> MS-CNN使用“h720-ctx” （所以没有反卷积，没有-c）</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/MS-CNN/1508425896306.png" alt="1508425896306"></p>
<h2 id="6-结论"><a href="#6-结论" class="headerlink" title="6 结论"></a>6 结论</h2><ul>
<li>提出了一个统一的深度卷积神经网络MS-CNN，用于快速多尺度目标检测</li>
<li>在多个中间网络层进行检测，使得感知域匹配目标尺寸</li>
<li>探究了CNN特征估计（反卷积），作为输入升采样的另一种选择，能节省计算和内存开销</li>
<li>综上，MS-CNN能实现15fps的检测速度</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[^1]: P. Viola and M. Jones, “Robust real-time face detection,” , IJCV, vol. 57, no. 2, pp. 137–154, 2004.<br>[^4]: R. Girshick, “Fast R-CNN,” In ICCV, 2015.<br>[^5 ]: X. Chen and Y. Zhu, “3D Object Proposals for Accurate Object Class Detection,” In NIPS, pp. 1–9, 2015.<br>[^6]: K. He, X. Zhang, S. Ren, and J. Sun, “Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,” In ECCV, 2014.<br>[^7]: S. Gidaris and N. Komodakis, “Object detection via a multi-region and semantic segmentation-aware U model,” In ICCV, 2015.<br>[^8]: K. E. A. van de Sande, J. R. R. Uijlings, T. Gevers, and A. W. M. Smeulders, “Segmentation as selective search for object recognition,” in ICCV, 2011.<br>[^9 ]: S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,” In NIPS, 2015.<br>[^11]: Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In CVPR, 2015.<br>[^14]: L. Bourdev and J. Brandt, “Robust object detection via soft cascade,” In CVPR, 2005<br>[^15]: Cai, Z., Saberian, M.J., Vasconcelos, N.: Learning complexity-aware cascades for deep pedestrian detection, In ICCV, 2015<br>[^17]: Saberian, M.J., Vasconcelos, N.: Boosting algorithms for detector cascade learning. Journal of Machine Learning Research 15(1) (2014) 2569–2605<br>[^22]: Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In CVPR, 2015<br>[^26]: S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick, “Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks,” In CVPR, 2016.<br>[^31]: Hosang, R. Benenson, P. Dollar, and B. Schiele, “What Makes for Effective Detection Proposals?,” TPAMI, vol. 38, no. 4, pp. 814–830, 2016.<br>[^32]: M. M. Cheng, Z. Zhang, W. Y. Lin, and P. Torr, “BING: Binarized normed gradients for objectness estimation at 300fps,” in CVPR, 2014, pp. 3286–3293.<br>[^33]: C. L. Zitnick and P. Dollár, “Edge Boxes: Locating Object Proposals from Edges,” in ECCV, 2014.<br>[^34]: P. Arbeláez, J. Pont-Tuset, J. Barron, F. Marques, and J. Malik, “Multiscale combinatorial grouping,” in CVPR, 2014.</p>
<p><strong>Citation</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;cai2016unified,</span><br><span class="line">annote = &#123;MS-CNN&#125;,</span><br><span class="line">author = &#123;Cai, Zhaowei and Fan, Quanfu and Feris, Rogerio S and Vasconcelos, Nuno&#125;,</span><br><span class="line">booktitle = &#123;ECCV&#125;,</span><br><span class="line">organization = &#123;Springer&#125;,</span><br><span class="line">pages = &#123;354--370&#125;,</span><br><span class="line">title = &#123;&#123;A unified multi-scale deep convolutional neural network for fast object detection&#125;&#125;,</span><br><span class="line">year = &#123;2016&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Pedestrian Detection</category>
      </categories>
      <tags>
        <tag>Pedestrian Detection</tag>
        <tag>Multi-scale</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记《Rethinking Atrous Convolution for Semantic Image Segmentation》</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Semantic-Segmentation/Note-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/</url>
    <content><![CDATA[<p>![1508501321087](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508501321087.png)</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文我们回顾了atrous 卷积，一个调整filter感知域并控制特征图分辨率的强大工具。为了处理多尺度目标分割，我们的模型使用不同空率（atrous rates）的atrous卷积进行级联或平行，以此来获得不同尺度的语义信息。并且，我们提出Atrous Spatial Pyramid Pooling (ASPP) 孔空间池化模型，探索多尺度上卷机特征具有图像级的全局特征编码并提升性能。本文提出的 <em>DeepLabv3</em> 比之前的版本提升，并且没有DenseCRF后处理，在VOC 2012图像分割benchmark上，实现与其他先进方法类似的性能。</p>
<span id="more"></span>

<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>DCNN用于语义分割任务面临两个挑战：</p>
<ul>
<li>池化和卷积步长会减小特征分辨率，妨碍需要丰富空间信息的任务，例如分割</li>
<li>目标的尺度多样性</li>
</ul>
<p>解决方法：</p>
<ul>
<li>使用 atrous 卷积 （也称为 膨胀（dilated） 卷积）来改变ImageNet预训练模型的功能，在网络的最后几层取消将采样操作，通过在filter的中间增加孔等同于提高卷积核尺寸。这样在不增加网络参数的情况下提升了特征图的分辨率。</li>
<li>图2显示了几种处理多尺度的方法：<ul>
<li>图像金字塔</li>
<li>编码-解码，先提取特征，然后恢复空间分辨率</li>
<li>使用atrous级联获得高分辨率和大感知域特征</li>
<li>空间金字塔池化[^10][^80] 在特征图上使用不同比率的池化和filter操作得到不同的感受域</li>
</ul>
</li>
</ul>
<p>本文，我们使用atrous卷积增加感受域获得不同尺度的语义信息。我们提出的模型包含多种尺寸的atrous，并且发现batch normolization层对于训练很重要。我们发现使用3$\times$3的atrous卷积，如果空隙过大也不行，主要因为图像边界效应。ASPP中简单的退化为1$\times$1卷积来获得图像级的特征。后面我们会分享训练的经验机械，包括简单有效的bootstrapping方法来处理稀少和精细的标注目标。最后，我们的方法在PASCAL VOC 2012上实现了85.7%的精度，并不适用DenseCRF后处理。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>![1508487635972](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508487635972.png)</p>
<p>(a) 图像金字塔</p>
<p>(b) 编码-解码器</p>
<p>(c) 语义模型</p>
<p>(d) 空间金字塔池化</p>
<p><strong>膨胀卷积</strong> 最近基于膨胀卷积的模型在语义分割领域很流行。[^73]研究了孔率（atrous rates）对获得大范围（long-rang）信息的影响。[^72]采用hyper孔率应用域ResNet最后两个Block。[^16]尝试学习可变形卷积。膨胀卷积也应用于目标检测[^56][^15][^31]</p>
<h2 id="3-Methods"><a href="#3-Methods" class="headerlink" title="3. Methods"></a>3. Methods</h2><p>本节回顾了如何应用膨胀卷积提取高分辨率特征用于语义分割。后面讨论了，将膨胀卷积用于级联和平行模型中。</p>
<h3 id="3-1-膨胀卷积用于Dense-Feature-提取"><a href="#3-1-膨胀卷积用于Dense-Feature-提取" class="headerlink" title="3.1 膨胀卷积用于Dense Feature 提取"></a>3.1 膨胀卷积用于Dense Feature 提取</h3><p>DCNN由于池化和卷积步长导致特征分辨率降低。因此有些方法尝试用反卷积来恢复特征分辨率。但是，我们提倡使用膨胀卷积。膨胀卷积的公式如下：</p>
<p>![1508500253840](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508500253840.png)</p>
<ul>
<li>$x$ 输入特征</li>
<li>$i$ 特征位置</li>
<li>$r$ 孔率（atrous rates）</li>
<li>$k$ filter尺寸</li>
<li>$w$ 权重</li>
</ul>
<p>膨胀卷积能够调整filter的感知域，如图1所示。因此，其能够让我们显式的控制全卷机网络中的特征响应尺寸。这里我们用<em>output_stride</em> 来表示输出的空间分辨率比例。假设一个DCNN的会缩小32倍，取消最后一个池化层或卷积层的步长设为1，然后后面的孔率设为2。这样在不增加任何参数的情况下提高特征分辨率。具体细节看[^10]</p>
<h3 id="3-2-Going-Deeper-with-Atrous-Convolution"><a href="#3-2-Going-Deeper-with-Atrous-Convolution" class="headerlink" title="3.2 Going Deeper with Atrous Convolution"></a>3.2 Going Deeper with Atrous Convolution</h3><p>![1508502437271](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508502437271.png)</p>
<p>级联模型在ResNet的block4开始，使用atrous。本来使用stride和pooling是为了得到大范围的信息，最终缩小到低分辨率的特征图上。然而，我们发现，这样的stride丢失的信息会损害语义分割。因此采样atrous来保持分辨率。</p>
<h4 id="3-2-1-多网格"><a href="#3-2-1-多网格" class="headerlink" title="3.2.1 多网格"></a>3.2.1 多网格</h4><p>受到之前一些研究对不同层级采用不同网格尺寸的启发，我们对block4到block7采用不同的孔率。$Multi\_Grid&#x3D;(r_1,r_2,r_3)$ ，我们采用$rates&#x3D;2\cdot(1,2,4)&#x3D;(2,4,8)$。（这里不太懂是级联的用不同孔率，还是在一层上就用不同的空率）</p>
<h3 id="3-3-Atrous-Spatial-Pyramid-Pooling（ASPP）"><a href="#3-3-Atrous-Spatial-Pyramid-Pooling（ASPP）" class="headerlink" title="3.3 Atrous Spatial Pyramid Pooling（ASPP）"></a>3.3 Atrous Spatial Pyramid Pooling（ASPP）</h3><p>![1508504434479](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508504434479.png)</p>
<p>借鉴了SPP Net的平行空间分辨率的方法。这样能形成不同的感受域，更好的获得多尺度信息。但是，当孔率增加是，有效的filter权重数量也在变少（filter的权重用于特征区域，而不是padding）。当对一个65$\times$65的特征图应用不同孔率时，各个有效权重膨胀卷积操作的比例。当孔率增加，我们看到有效权重的数量减少，极端情况下，只有中心的fitler权重是有效权重。之后还有图像级池化。</p>
<p>![1508505311787](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508505311787.png)</p>
<h2 id="4-Experimental-Evaluation"><a href="#4-Experimental-Evaluation" class="headerlink" title="4. Experimental Evaluation"></a>4. Experimental Evaluation</h2><p>网络模型使用ImageNet预训练的ResNet。在TensorFlow框架下实现。如果希望输出output _ stride&#x3D;8，那就从block3开始用rate&#x3D;2，block4的rate&#x3D;4。</p>
<p>我们在PASCAL VOC 2012 语义分割benchmark上评估模型。评估指标为像素交并集。</p>
<h3 id="4-1-训练方法"><a href="#4-1-训练方法" class="headerlink" title="4.1 训练方法"></a>4.1 训练方法</h3><p><strong>学习率</strong> 我们采用“poly”学习率，对初始学习率乘以$(1-\frac{iter}{max\_iter})^{power}$  ，$power&#x3D;0.9$。</p>
<p><strong>Crop Size</strong> 尺寸为512</p>
<p><strong>Batch normolization</strong> 对ResNet增加模块全部包含batch normolization。后面这段是具体的训练细节。</p>
<p><strong>Upsampling logits</strong> 对最后的结果进行上采样，来保证对原图进行反向传播，不损失原始的标注细节。</p>
<p><strong>Data augmentation</strong> 对原图进行随机缩放（0.5~2.0），然后随机左右翻转。</p>
<h3 id="4-2-使用膨胀卷积的深度网络"><a href="#4-2-使用膨胀卷积的深度网络" class="headerlink" title="4.2 使用膨胀卷积的深度网络"></a>4.2 使用膨胀卷积的深度网络</h3><p>我们先实验了级联膨胀卷积的模型</p>
<p><strong>ResNet-50：</strong> 表1，我们看见output _ stride影响，output _ stride的增加伴随着信号的损失，当采用膨胀卷积后，效果从20.29%提升到75.18%，说明膨胀卷积对级联block进行语义分割的重要性。</p>
<p>![1508509468319](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508509468319.png)</p>
<p><strong>Res-50 vs. Res-101</strong> 两个网络模型，增加级联的block，其性能持续改善，但是提升逐渐减少。</p>
<p>![1508510227795](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508510227795.png)</p>
<p><strong>Multigrid</strong> 表3显示了不同孔率搭配的结果。</p>
<p>![1508510702238](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508510702238.png)</p>
<p><strong>Inference Strategy on val set</strong> 我们还尝试使用output _ stride&#x3D;8的结果。然后对输入图像进行多尺度处理，左右翻转，也能进一步提升，最后的结果是对每个结果probability求平均。</p>
<p>![1508549190998](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508549190998.png)</p>
<h3 id="4-3-ASPP"><a href="#4-3-ASPP" class="headerlink" title="4.3 ASPP"></a>4.3 ASPP</h3><p><strong>ASPP</strong> 表5我们实验了ASPP和应用于block4的Multigrid配合。应该是一个block中有多个卷积层，使用不同的rates，还有图像级的Image Pooling</p>
<p>![1508512107326](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508512107326.png)</p>
<p><strong>Inference strategy on val set</strong> 同上。发现ASPP比级联的稍好。</p>
<p><strong>与DeepLabv2比较</strong> 在没有DenseCRF和MS-COCO预训练的情况下已经比v2好。主要是改进因为fine-tuning batch normalization，以及更好的编码多尺度上下文。</p>
<p><strong>Block5 + ASPP</strong> 我们尝试在组合级联和平行模型，在Block5上的效果却不好。</p>
<p><strong>量化结果</strong> 在PASCAL VOC 2012测试集上的结果mIOU&#x3D;85.7<br>![1508564060949](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508564060949.png)</p>
<p><strong>Bootstrapping</strong> 我们不是像其他人挖掘困难负样本，而是复制包含困难类别的图像。这样简单的Bootstrapping策略也能得到较好的结果。</p>
<p>![1508513867319](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Rethink</a> Atrous&#x2F;1508513867319.png)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@article&#123;Chen2017Rethinking,</span><br><span class="line">author = &#123;Chen, Liang Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig&#125;,</span><br><span class="line">title = &#123;&#123;Rethinking Atrous Convolution for Semantic Image Segmentation&#125;&#125;,</span><br><span class="line">journal=&#123;arXiv preprint arXiv:1706.05587&#125;</span><br><span class="line">year = &#123;2017&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[^10]: L.-C. Chen, G. Papandreou, I.Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. arXiv:1606.00915, 2016.<br>[^15]: J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection via region-based fully convolutional networks. arXiv:1605.06409, 2016.<br>[^16]: J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y.Wei. Deformable convolutional networks. arXiv:1703.06211, 2017.<br>[^31]: J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z.Wojna, Y. Song, S. Guadarrama, and K. Murphy. Speed&#x2F;accuracy trade-offs for modern convolutional object detectors. In CVPR, 2017.<br>[^56]: G. Papandreou, I. Kokkinos, and P.-A. Savalle. Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection. In CVPR, 2015.<br>[^72]: P.Wang, P. Chen, Y. Yuan, D. Liu, Z. Huang, X. Hou, and G. Cottrell. Understanding convolution for semantic segmen- tation. arXiv:1702.08502, 2017.<br>[^73]: Z. Wu, C. Shen, and A. van den Hengel. Bridging category-level and instance-level semantic image segmen- tation. arXiv:1605.06885, 2016.<br>[^80]: H. Zhao, J. Shi, X. Qi, X.Wang, and J. Jia. Pyramid scene parsing network. arXiv:1612.01105, 2016.</p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Semantic Segmentation</category>
      </categories>
      <tags>
        <tag>Atrous Convolution</tag>
        <tag>Image Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记《Soft-NMS – Improving Object Detection With One Line of Code》</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-Soft-NMS-%E2%80%93-Improving-Object-DetectionWith-One-Line-of-Code/</url>
    <content><![CDATA[<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Soft-NMS/6f066bd9-979f-4e52-b5bb-b7780a2527f4.jpg"></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Non-maximum suppression（NMS）是目标检测流程中重要的组成部分。首先根据得分对bbox排序，然后选择得分最高的bbox M, 将与M有较大重叠的bbox抑制掉。以前的方法中，如果一个bbox重叠率大于设定的阈值则称为漏检。所以，提出 Soft-NMS, 一个衰减得分与重叠率成函数关系。该方法在coco-style mAP评价尺度上都有提升。计算复杂度，参数都没有特别增加，很容易集成到任何目标检测框架中。</p>
<span id="more"></span>

<ul>
<li><a href="http://bit.ly/2nJLNMu">code</a></li>
</ul>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>目标检测在各个领域都有重要作用，其各个模块都需要谨慎设计，不能称为计算瓶颈。任何需要大量计算、重新训练的模块，如果只能带来一点提升都会被忽略。但是如果一个简单不需要重新训练的模块也能带来性能提升，那么就会被广泛采纳。本文，提出一个S-NMS，作为传统NMS的替代。</p>
<p>一般一个检测框旁边会议多个得分同样很高的检测框（增加虚警率），所以需要采用NMS进行抑制。近年来RPN等方法所产生的ROI已经大大减少，但是仍然需要NMS来减少虚警率。</p>
<p>首先NMS的输入是 $B$ 和 $S$, 选择得分高的 $M$, 抑制掉大部分BBox得到 $D$。这个过程不断迭代剩下的 $B$。这个过程中，将 $M$ 周围的 BBox 的得分设为0，导致漏检了部分目标，如图1所示。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Soft-NMS/921487d8-8ee5-4c41-9d33-827cc4764d92.png"></p>
<p>因此，我们在传统的贪心NMS算法中提出一行改动，使得得分的下降随着重叠率的升高的增加，而不是直接为0。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>NMS作为检测算法的一个部分已经有50年的历史。第一次是用于边缘检测中，之后逐渐用于特征点检测，人脸检测，目标检测。</p>
<ul>
<li>边缘检测中，NMS用于细化边缘（edge thinning）移除虚警</li>
<li>特征点检测中，NMS通过局部阈值得到唯一的特征点</li>
<li>人脸检测中，通过重叠率来分割BBox为不连续的子集</li>
<li>人体检测中，Dala等人证明使用贪心NMS，可以实现性能提升</li>
</ul>
<p>从此之后贪心NMS称为目标检测中默认的算法。近几十年，贪心NMS仍然取得最好的性能。也有一些基于学习的方法被提出。例如：</p>
<ul>
<li>先进行两两重叠率计算，然后使用相似传播聚类（affinity propagation clustering）选择最后的检测结果[^26]</li>
<li>另一个多类别版本也提出了[^21], 但是多类别检测是一个不同的问题，我们需要选择一个阈值可用于多类的目标</li>
<li>由于不同的目标可能适合不同的NMS阈值，因此采用mAP作为评价</li>
</ul>
<p>在显著目标检测中，有人提出了推荐子集优化算法[^30]. 其基于MAP，联合优化检测窗口的位置和数量。<br>在行人检测中，一个二元约束的二元优化方案（a quadratic unconstrained binary optimization, QUBO）[^27]，使用检测得分作为一元潜在因子，重叠率作为成对因子用于优化后的检测BBox子集。<br>另外一些基于学习的框架中，决策处理时结合个体预测得分用于优化选择最后的结果[^15]。<br>据我们所知，对于一般目标检测，贪心NMS仍然是PASCAL VOC MS-COCO等数据集的baseline方法。</p>
<h2 id="3-Background"><a href="#3-Background" class="headerlink" title="3. Background"></a>3. Background</h2><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Soft-NMS/4679005e-94d2-4be0-8914-8b115a254f8d.jpg"></p>
<p>这里描述R-CNN系列方法的流程，proposal分支，classifcation分支。proposal提出K个anchor box，然后按得分排序，进行NMS。<br>分类网络不对ROI做限制，导致可能对一个物体有多个检测框，因此需要用NMS对每个类别筛选BBox。然后在这个基础上在去除一些得分特别低的检测结果。目标检测流程如图3所示。</p>
<h2 id="4-Soft-NMS"><a href="#4-Soft-NMS" class="headerlink" title="4. Soft-NMS"></a>4. Soft-NMS</h2><ul>
<li>当NMS阈值取得低时，例如0.3，那么会导致高IOU评估时的平均精度降低，例如IOU&#x3D;0.7当NMS阈值取得低时，例如0.3，那么会导致高IOU评估时的平均精度降低，例如IOU&#x3D;0.7</li>
<li>因为一个高得分的box会抑制周围的目标因为一个高得分的box会抑制周围的目标</li>
<li>当NMS阈值取的高时 $N _ t$ 取 0.7 会增加虚警</li>
<li>并且虚警增加的数量远多于目标（因为ROI数量远多于目标数量），因此使用高NMS也不是一个好选择</li>
</ul>
<p>NMS算法可以写成如下形式，即一个re-scoring函数</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Soft-NMS/600fecc0-66a4-4aa8-8a76-627a24dc523c.png"></p>
<p>我们的想法是降低高重叠率的box的得分，而不是一起抑制，因此NMS应该考虑以下因素：</p>
<ul>
<li>抑制一个box周围似然概率更低的box</li>
<li>低NMS是次优的，会造成漏检率增加</li>
<li>高NMS会导致平均精度下降</li>
</ul>
<p><strong>重新设计函数Soft-NMS</strong><br>Linear-S-NMS<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Soft-NMS/e4682ee9-d71e-408d-b5cc-5f04721f1344.png"></p>
<p>Gaussian-S-NMS<br><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Soft-NMS/e5390b66-2484-47ee-987f-d0f7ee6af4d8.png"></p>
<p>Soft-NMS 一次的计算复杂度是 $O(N)$, N是检测框的数量。对于所有检测框，那计算复杂度是 $O(N^2)$, 这个与贪心NMS一样。由于目前的NMS是在得分阈值之后进行，因此不是对所有检测框进行，因此计算复杂度不是高。</p>
<p>需要指出，S-NMS也是贪心算法，但是S-NMS是一个更一般的版本，传统的NMS是其特殊情况。除了这两个函数外，其实还可以考虑有更多参数的函数，例如将重叠率，检测得分都考虑进去。例如，一般的逻辑函数Gompertz函数，但是会引入更多超参。</p>
<h2 id="5-数据集和评估"><a href="#5-数据集和评估" class="headerlink" title="5. 数据集和评估"></a>5. 数据集和评估</h2><p>实验在PASCAL VOC 和 MS-COCO上进行。评估的方法有三个：</p>
<ul>
<li>Faster R-CNN[^24]</li>
<li>R-FCN[^16]</li>
<li>Deformable-RFCN</li>
</ul>
<p>采用的评估模型：</p>
<ul>
<li>在PASCAL上使用的model选择已经训练好的，公开的，由作者提供的模型</li>
<li>在COCO上R-FCN没有训练好的，因此我们在Caffe上自己训练一个，从ResNet-101开始，并做了一些改动，5 scale anchor，800 pixel，16 img&#x2F;minibatch，256 ROI&#x2F;img，结果比原作者的提升1.9%</li>
<li>同样 Deformable-R-FCN 也使用相同的设定训练。</li>
</ul>
<p>传统NMS效率</p>
<ul>
<li>上述NMS均使用0.3</li>
<li>一张图最多400个检测结果，每个类别最多100个，将每张图最多检测结果降为100，则AP减少0.1</li>
<li>阈值0.0001，4 CPU threads，0.01 s&#x2F;img，80 classes</li>
<li>阈值0.01， 1 CPU core， 0.005 s&#x2F;img</li>
</ul>
<h2 id="6-实验"><a href="#6-实验" class="headerlink" title="6. 实验"></a>6. 实验</h2><p>我们展现了S-NMS的鲁棒性，并且进行特定实验理解为何S-NMS会比粗汉同NMS好</p>
<h3 id="6-1-结果"><a href="#6-1-结果" class="headerlink" title="6.1 结果"></a>6.1 结果</h3><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Soft-NMS/b4fd5cdb-5067-4fc2-8094-f7b8b457bb8e.png"></p>
<p>表1展示了NMS和S-NMS的有效性，我们设定线性NMS为 $N _ t &#x3D; 0.3$，高斯NMS $\sigma _ t &#x3D; 0.5$。S-NMS都在改进。我们比较了S-NMS在MS-COCO每个类别上的改进，我们发现对于动物（马，长颈鹿，羊，大象等）提升有3-6%，但是对一些物体（烤面包机，球等等）提升较少，因为前者更容易出现交叠的情况。</p>
<h3 id="6-2-敏感度分析"><a href="#6-2-敏感度分析" class="headerlink" title="6.2 敏感度分析"></a>6.2 敏感度分析</h3><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Soft-NMS/3cc88fce-aba9-4a02-a6b1-2b52eaf15eb4.png"></p>
<p>图4分析了当$N _ t$ 和 $ \sigma _ t$ 变化时对AP[0.5:0.95]。我们发现0.3到0.6之间AP相对稳定。在后面的实验，未特别说明$\sigma$取0.5。</p>
<h3 id="6-3-什么时候S-NMS-work-better"><a href="#6-3-什么时候S-NMS-work-better" class="headerlink" title="6.3 什么时候S-NMS work better"></a>6.3 什么时候S-NMS work better</h3><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Soft-NMS/2260ccaa-9534-4582-a151-5f06675a3185.png"></p>
<p><strong>定位性能</strong> 如表3所示，我们发现两个超参基本都是跟评价指标变化，但是当$N _ t$增加时，AP会降低，但是其取低值时，AP下降没那么多，因此低$N _ t$具有更好的泛化性。但是，高斯S-NMS的参数增高时可以得到更好的性能。并且相比比线性S-NMS更好，提升也更大。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Soft-NMS/c479385e-adfc-48eb-bab3-fbf00ba48f02.png"></p>
<p><strong>精度与召回率</strong> 图5显示了在不同$O _ t$ 下的召回率与精度的关系。我们发现当要求定位精度很高时，S-NMS也有很好的改进，因为传统的NMS是直接将得分置为0，而S-NMS是衰减得分，变相保留了一些BBox。</p>
<h3 id="6-4-量化结果"><a href="#6-4-量化结果" class="headerlink" title="6.4 量化结果"></a>6.4 量化结果</h3><p>图7显示了一些检测结果，当IoU阈值为0.45时。</p>
<ul>
<li>No.8 中有一个横跨多个人人群的BBox，它与多个BBox有交集，但是NMS不足以抑制，但是S-NMS可以对其得分进行多次衰减。</li>
<li>No.9 中也有相似的现象。</li>
<li>No.1 中在女士手包周围的BBox被抑制了。</li>
<li>No.4 中在晚旁边的虚警被抑制</li>
<li>NO.2,5,6,13 中的动物有些被NMS抑制掉了，但是S-NMS只是稍微降低了得分，但是仍然保留了下来</li>
</ul>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Soft-NMS/1509521123325.png" alt="1509521123325"></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[^15]: D. Lee, G. Cha, M.-H. Yang, and S. Oh. Individualness and determinantal point processes for pedestrian detection. In European Conference on Computer Vision, pages 330–346. Springer, 2016.<br>[^16]: Y. Li, K. He, J. Sun, et al. R-fcn: Object detection via region- based fully convolutional networks. In Advances in Neural Information Processing Systems, pages 379–387, 2016.<br>[^21]: D. Mrowca, M. Rohrbach, J. Hoffman, R. Hu, K. Saenko, and T. Darrell. Spatial semantic regularisation for large scale object detection. In Proceedings of the IEEE international conference on computer vision, pages 2003–2011, 2015.<br>[^24]: S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91–99, 2015.<br>[^26]: R. Rothe, M. Guillaumin, and L. Van Gool. Non-maximum suppression for object detection by passing messages be- tween windows. In Asian Conference on Computer Vision, pages 290–306. Springer, 2014.<br>[^27]: S. Rujikietgumjorn and R. T. Collins. Optimized pedestrian detection for multiple and occluded people. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3690–3697, 2013.<br>[^30]: J. Zhang, S. Sclaroff, Z. Lin, X. Shen, B. Price, and R. Mech. Unconstrained salient object detection via proposal sub- set optimization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5733– 5742, 2016.</p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>NMS</tag>
        <tag>ICCV2017</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记《Flow-Guided Feature Aggregation for Video Object Detection》</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Video-Object-Detection/Note-Flow-Guided-Feature-Aggregation-for-Video-Object-Detection/</url>
    <content><![CDATA[<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/FGFA/1509936888557.png" alt="1509936888557"></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>将图像目标检测器用于视频是具有挑战性的，其精度受外观变化的影响很大，例如，运动模糊、失焦、奇特姿势等。目前很多方法尝试引入box级别的时间信息，但是都没有实现端到端的训练。我们提出一个 <em>flow-guided</em> 特征集合框架实现视频目标检测端到端的学习。它在特征层面上的时间相关性。通过聚合运动路径上的临近的特征能够改善前帧特征，从而改进视频目标检测精度。本文提出的方法显著的改善ImageNet VID的单帧检测基准方法，特别是对于快速运动的目标。代码会公开。</p>
<span id="more"></span>
<ul>
<li>中国科学技术大学</li>
<li>Microsoft Research Asia</li>
</ul>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>近几年目标检测发展迅速，大部分都具有相似的two-stage结构。一个深度卷积神经网络用于生成特征图，一个浅层检测网络在特征图上检测目标。</p>
<p>这些方法在静止图像效果较好，但是直接用到视频目标检测上却存在问题。检测精度受目标外观变化的影响，例如运动模糊、视频失焦、罕见姿势等。</p>
<p>然而，视频中在一段时间中包含个体的信息是非常丰富。一些视频目标检测方法使用简单的策略利用这些时间信息[^18][^19][^12][^23]。首先，对单帧进行检测，然后通过专门的后处理过程在时间维度上组合BBox。这一过程依赖现成的运动估计方法，例如光流法，并且用跟踪连接BBox。一般来说，这些方法的提升主要来自启发式的后处理过程，而不是学习规则。并且没有端到端的训练，我们称这些技术为box级别的方法。</p>
<p>我们尝试从学习规则中开发时间信息改善检测质量。我们尝试通过集成时间改进pre-frame的特征学习。由于视频运动的原因，同一个物体的特征通常没有进行空间对准。只把特征简单聚合可能甚至恶化性能，如表1(b) 所示。因此，在学习时对运动建模是非常重要的。</p>
<p>本文工作，我们提出flow-guided feature aggregation (FGFA)。如图1所示，特征提取网络对pre-frame提取特征图。为了增加reference帧的特征，使用光流光流估计nearby帧与refernce帧的运动信息。邻近帧的特征图根据flow motion 融合到 reference 帧中。</p>
<p>相比于其他Box级别的方法，我们的方法在特征上处理，并且进行端到端的学习，与之前的其他工作具有互补性。它改进的前帧的特征，生成高质量的BBox。Box级别的方法可以进一步优化这些BBox。我们的方法在ImageNet VID数据集上验证。严格的消融实验证实了其能够有效的改进单帧检测方法。与Box级别的方法结合可以进一步提升。</p>
<p>另外，我们对物体的运动强度做了分析。结果表面快速运动的物体更具挑战性，也是我们的方法提升最大的地方，其能够在一段快速运动物体上提取丰富的外观信息。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p><strong>图像目标检测</strong><br>目前先进的目标检测方法一般基于深度神经网络。R-CNN 使用多阶段流程实现基于区域推荐的目标检测。为了加速，提出了ROI pooling来共享特征图。Faster R-CNN 提出 RPN 网络与 Fast R-CNN 共享特征。最近，R-FCN [^5] 提出在最后的 score map 进行位置敏感的 ROI pooling 操作，将特征共享推向了极限。</p>
<p>我们的方法更关注在视频上检测目标，其配合时间信息提升卷积特征图的质量。</p>
<p><strong>视频目标检测</strong><br>最近，ImageNet 提出了一个新的挑战：视频目标检测（VID）。在这项挑战中，最近的所有方法都尽在最后的BBox后处理阶段考虑时间信息。</p>
<ul>
<li>T-CNN[^18][^19] 通过光流预测邻近帧的BBox，然后对高置信度的BBox应用跟踪算法得到tubelets。BBox根据tubelets的分类结果进行重新打分。</li>
<li>Seq-NMS[^12] 在连续帧邻近的高置信 BBox 构造序列。序列中的 Box 基于平均置信分重打分，其他接近该序列的 Box 被抑制。</li>
<li>MCMOT [^23] 将后处理过程作为一个多目标跟踪问题。其使用了一系列手工规则（置信度，颜色&#x2F;运动线索，改变检测点，前后验证）来验证一个BBox是否是跟踪的目标。不幸的是，这些方法是multi-stage pipeline，其一个结果依赖另一个。因此，很难修正前一个结果带来的错误。</li>
</ul>
<p>不同的是，我们的方法在特征层级考虑时序信息，整个系统实现了端到端训练。除此之外，我们的方法也可以和这些 BBox 后处理技术配合进一步改进识别精度。</p>
<p><strong>基于流的运动估计</strong><br>时序信息需要在连续帧的像素或特征上建立相关性。光流广泛的用于视频分析和处理。传统方法采用变分方法主要面向小偏移。最近一些专注于大偏移[^3]并结合匹配的方法（DeepFlow[^44]，EpicFlow[^31]）被加入变分方法中。上述方法均采用手工特征。最近基于深度学习的方法（FlowNet [^8]，及其后续方法 [^28][^17]）开始采用光流。与我们的方法最相似的工作是 deep feature flow [^49]，其显示出视频中信息的冗余性，并尝试提高视频的检测速度。其显示了联合训练 流-子网络和 识别-子网络的可能性。</p>
<p>本文，我们专注于使用连续帧的外观丰富信息来改进特征表示。我们延续 deep feature flow 的设计融合跨帧的特征。</p>
<p><strong>特征集成</strong><br>特征集成广泛的用于动作识别和视频描述中。一方面，目前大部分工作使用循环神经网络聚合连续帧特征。另一方面，使用 exhaustive 时空卷积来直接提取特征[^38][^21][^41][^42]。然而，这些方法中卷积核的尺寸限制了对快速运动物体建模的能力。因此，应该考虑大尺寸的卷积核，但是这样会极大的增加模型参数，并且来带过拟合的问题（这里是不是可以用膨胀卷积做）。作为对比，我们的方法基于流引导的集成，能够适应各种不同程度的运动。</p>
<p><strong>视觉跟踪</strong><br>最近，DCNN 也开始用于目标跟踪 [^25][^16] 并实现的不错的精度。当新跟踪一个目标，就生成一个新的网络，其共享之前的CNN层，但是具有一个新的二元分类层，并且在线更新。跟踪和视频目标检测任务显然不同，因为跟踪假设已知目标的起始位置，并且不需要预测目标类别。</p>
<h2 id="3-Flow-Guided-Feature-Aggregation"><a href="#3-Flow-Guided-Feature-Aggregation" class="headerlink" title="3. Flow Guided Feature Aggregation"></a>3. Flow Guided Feature Aggregation</h2><h3 id="3-1-A-Baseline-and-Motivation"><a href="#3-1-A-Baseline-and-Motivation" class="headerlink" title="3.1 A Baseline and Motivation"></a>3.1 A Baseline and Motivation</h3><p>对于输入的视频，我们的目的是输出所有帧中检测到的目标的BBox。一个基准方法是采用现成的目标检测其对每一帧进行独立的检测。现在基于CNN的目标检测器都具有相似的结构。一个子网络负责将图片转换为特征图 $f &#x3D; N _ {feat}(I)$，然后在特征图上检测结果 $y &#x3D; N _ {det}(f)$。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/FGFA/1510037047150.png" alt="1510037047150"></p>
<p>图2显示了目标外观剧烈变化的例子。单帧的检测结果可能由于外观的差（模糊）导致检测失败，如图1所示， $t$ 帧运动模糊。但是邻近帧具有高相应图，因此他们的特征可传递给参考帧。融合之后的特征图响应增强后实现成功检测。</p>
<p>为了实现特征传播和增加，需要两个模块：</p>
<ol>
<li>运动引导的空间融合。估计帧间的运动然后响应的融合特征图。</li>
<li>特征聚合模块。如何合适的融合多帧特征。</li>
</ol>
<h3 id="3-2-模型设计"><a href="#3-2-模型设计" class="headerlink" title="3.2 模型设计"></a>3.2 模型设计</h3><p><strong>流引导融合</strong><br>使用一个 flow network[^8] 来估计流场。 对于给定的参考帧 $I _ i$ 和邻近帧 $I _ j$ 则流场为 $M _ {i\rightarrow j} &#x3D; F(I _ i, I _ j)$。流场和特征图的融合基于下面的公式</p>
<p>$$f _ {j \rightarrow i} &#x3D; W(f _ j, M _ {i \rightarrow j})$$<br>$W(\cdot)$ 是一个双线性融合函数应用于每个通道的所有位置的特征图。</p>
<p><strong>特征融合</strong><br>特征经历过流引导融合后，然后再reference的特征上进行累加。我们在不同的空间位置使用不同的权重，所有通道共享权重。参考帧的融合特征为：<br>$$ \overline f _ i &#x3D; \sum _ {j &#x3D; i-K} ^{i+K} w _ {j \rightarrow i} f _ {j \rightarrow i}$$<br>$K$ 表示周围的邻近帧。上式与 attention 模型相似。特征融合后输入到检测网络得到结果。</p>
<p>$$y _ i &#x3D; N _ {det}(\overline f _ i)$$<br>相比于其他baseline方法在box层面上融合，我们的方法大在最终结果得出前融合多帧信息。</p>
<p><strong>自适应权重</strong><br>自适应权重显示了所有帧每个位置相对于参考帧的重要性。如果融合特征与参考帧特征接近，则具有大的权重。我们使用余弦相似性指标来评估warped特征与参考帧特征的相似性。除此之外，我们不直接使用卷积特征，而是使用一个小的全卷积网络对特征进行变换后计算相似性。自适应权重估计为：</p>
<p>$$w _ {j \rightarrow i}(p) &#x3D; exp(\frac{f^e _ {j \rightarrow i}(p) \cdot f^e _ {i}(p)}{|f^e _ {j \rightarrow i}(p)||f^e _ {i}(p)|})$$<br>其中 $f^e &#x3D; \epsilon (f)$ 。权重的估计可以看成是通过SoftMax操作后的特征余弦相似性处理。</p>
<h3 id="3-3-训练和推理"><a href="#3-3-训练和推理" class="headerlink" title="3.3 训练和推理"></a>3.3 训练和推理</h3><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/FGFA/1510062087184.png" alt="1510062087184"></p>
<p><strong>推理</strong><br>算法1总结了推理过程。输入连续的视频帧 ${I _ i}$ 然后指定聚合范围 $K$。初始化，特征网络应用于开始 K+1 帧。然后，循环的对所有视频帧进行视频目标检测，并且计算基于光流引导的特征图。然后计算嵌入特征，接着计算聚合权重。然后计算聚合后的特征，接着输入到检测网络得到检测结果。然后计算 K+1+i 帧的特征图。继续下一帧的计算。（这里感觉有很多的重复计算，例如i和i+1，可能变化不大，但是每次都要计算光流，然后计算融合特征）。</p>
<p>对于运行时的复杂度与单帧检测的比例：<br>$$<br>r &#x3D; 1 + \frac{(2K + 1)\cdot(O(F)+O(\epsilon)+O(W))}{O(N _ {feat}) + O(N _ {det})}<br>$$<br>一般来说，$N _ {det}$, $\epsilon$, $W$ 的复杂度是可以忽略的相比于 $N _ {feat}$。因此上式变为$r &#x3D; 1 + \frac{(2K + 1)\cdot O(F)}{O(N _ {feat}) + O(N _ {det})}$ 。我们发现，其计算复杂度最大的是 $F$, 一般来说其相比 $N _ {feat}$ 是低得多的，因此是负担得起的。</p>
<p><strong>训练</strong><br>FGFA 整体架构是可以进行端到端训练的。并且 feature warping 模块也是采用双线性差值实现，因此也是可微的。</p>
<p><em>时间dropout</em>。 在 SGD 中 K 的取值受到内存的限制。 我们在测试时使用大 K，但是在训练时使用小 K（&#x3D;2）。在训练时，邻近帧是随机在一个大范围内采样，其等价于测试阶段。类似dropout技术，这个类似时间 dropout。从表3的效果看，这个训练策略 work well。</p>
<h3 id="3-4-网络架构"><a href="#3-4-网络架构" class="headerlink" title="3.4 网络架构"></a>3.4 网络架构</h3><p>下面具体描述 FGFA 模型中的每个子网络。<br><strong>Flow network</strong><br>我们使用 FlowNet，在 Fly Chairs dataset 上进行预训练，应用的图像具有一般的分辨率，并且输出的 stride&#x3D;4 。由于 feature network 的输出 stride &#x3D; 16，因此对 flow field 进行降采样以此来匹配。</p>
<p><strong>Feature network</strong><br>我们采用 ResNet 和 Inception-Resnet 作为 feature network。这两个网络本来用于图像识别，为了解决特征 misalignment 的问题，我们利用了修改后的版本 Aligned-Inception-ResNet[^6]，在 ImageNet 分类数据集上进行预训练。</p>
<p>我们对预训练的模型进行了精细的调整使其应用于我们的 FGFA 模型中。我们调整了三个模块用于目标检测。我们移除了最后一个平均池化层和全连接层，并且重新训练了卷积层。为了增加特征分辨率，我们将最后一个模块的有效 stride 从32变为16，即在 conv5 的 stride 从2变为1。为了重新训练感受域的尺寸，最后一个 block 的卷积膨胀为2。最后，在顶层随机初始化 $3 \times 3$ 卷积，将特征维度减为 1024。</p>
<p><strong>Embedding network</strong><br>此网络包含三层，$1\times 1\times 512$，$3\times 3\times 512$，$1\times 1\times 2048$ 三个卷积层，进行随机初始化。</p>
<p><strong>Detection network</strong><br>我们使用 R-FCN[^5] 及其 [^49]中的设计。我们在 1024-d 的特征图上应用 RPN 子网络和 R-FCN 子网络，其分别连接开始的 512-d 特征和最后的 512-d 特征。9个 anchors（3尺度、3比例）用于RPN，每个图像产生300个 proposal。R-FCN 的位置感知得分图为 $7\times 7$ groups。</p>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h2><h3 id="4-1-实验设置"><a href="#4-1-实验设置" class="headerlink" title="4.1 实验设置"></a>4.1 实验设置</h3><p><strong>ImageNet VID dataset[^33]</strong><br>该数据集为流行的大尺度视频目标检测基准数据集。遵循[^18][^23]协议。模型在3862个视频上进行训练和评估，然后在555个视频上进行验证。视频帧率为25或30 fps。一共有30个目标类别，是ImageNet DET 的子集。</p>
<p><strong>慢、中、快运动</strong><br>为了更好的分析，我们将 GT 按其运动速度分类。物体的运动速度根据其平均 IoU （±10帧）得分进行量化，我们称其为 motion IoU，其值越低说明物体运动越快。我们按照其得分，分为 slow（＞0.9），medium（[0.7,0.9]），fast（＜0.7）三组。举例如图4所示。</p>
<p>我们在评估时，分别对三种速度进行 mAP 评估。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/FGFA/1510369413663.png" alt="1510369413663"></p>
<p><strong>实现细节</strong><br>我们利用了 ImageNet DET 和 VID 训练集，使用了 two-phase 训练。在第一个 phase，在 ImageNet DET上训练 feature 和 Detection网络。SGD训练，一个图一个 mini-batch。120K迭代，在4个GPU上，每个GPU一个mini-batch。学习率开始的80K迭代为0.001，后40K为0.0001。在第二个phase，整个FGFA模型在 VID数据集上训练。60K次迭代，学习率开始的40K迭代为0.001，后20K次迭代为0.0001。训练和测试时，都将图像缩放为短边600像素用于feature网络，300像素用于flow网络。实验设备：E5 2670 v2 CPU 2.5GHz Nvidia K40 GPU。</p>
<h3 id="4-2-消融实验"><a href="#4-2-消融实验" class="headerlink" title="4.2 消融实验"></a>4.2 消融实验</h3><p><strong>FGFA 架构设计</strong></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/FGFA/1510066036034.png" alt="1510066036034"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/FGFA/1510321692279.png" alt="1510321692279"></p>
<p>表1比较了我们的FGFA和单帧检测baseline方法。</p>
<p>方法（a）是单帧 baseline，其精度与原文描述的相似，说明我们的实现具有可比性且可作为评估基准。注意，我们没有增加多尺度训练、测试，语义信息，模型组合等。为了能够得到清晰可比的结果。</p>
<p>从结果可以看出，快速运动的物体检测非常具有挑战性。由于不同尺寸的物体可能有不同的运动速度，进一步分析了尺寸额定影响。表2实现了不同尺寸的检测精度，其结果显示，不同尺寸的物体快速运动时检测起来更加困难。</p>
<p>方法（b）是一个特征集合方法。没有采用光流运动。但是采用端到端训练。其性能相比于baseline略微下降。快速运动物体的下降更明显。说明在视频目标检测中考虑运动信息是非常重要的。</p>
<p>方法（c）增加了自适应权重模块。可以发现其对于慢、中物体的帮助较小，但是对快速运动物体的帮助更大。图5（左）看到对于快速运动物体，权重更加偏重向中间的参考帧几种。</p>
<p>方法（d）是本文提出的FGFA，增加了光流融合模块。同样对快速运动物体提升很大。并且图5看得出，自适应权重的分布更加平均，这说明光流引导特征集合能更有效的融合临近帧的特征。图6显示了一些结果。</p>
<p>方法（e）是d的降级版，不适用端到端训练。其使用baseline（a）的特征、检测子网络，已训练好的 FlowNet。在训练期间，这些网络都是固定的，只有embedding子网络进行学习。从结果看其性能降低，说明端到端学习的重要性。</p>
<p>FGFA使用773ms处理一帧，比单帧检测慢（288ms），因为流网络对每帧需要处理2K+1次。为了减少评估的次数，我们采用只对邻近帧进行评估的FGFA。非邻近帧的流是通过合成中间流场获得。这样，流场的计算可以在不同参考帧上复用。计算速度减少为356ms，精度下降~1%，由于流场估计错误的累加导致。</p>
<p><strong># 训练测试时的帧数</strong> </p>
<p>由于内存限制，我们使用ResNet-50进行实验。我们分别采用2和5帧作为mini-batch训练，1，5，9，13，17，21，25帧进行测试。结果如表3所示。2和5帧训练的结果非常接近，这也验证了temporal dropout训练策略的有效性。在测试时，我们期望精度会随着帧数的增加而提升，我们发现改进在21帧时达到饱和。因此，我们默认采用2帧训练，21帧测试。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/FGFA/1510385244941.png" alt="1510385244941"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/FGFA/1510370625780.png" alt="1510370625780"></p>
<h3 id="4-3-结合Box级别的技术"><a href="#4-3-结合Box级别的技术" class="headerlink" title="4.3 结合Box级别的技术"></a>4.3 结合Box级别的技术</h3><p>我们的方法关注改进特征质量并提升视频帧的识别精度。输出bbox可以进一步通过box级别的后处理方法改进。实际上，我们测试了三种不同的技术，运动引导传播（Motion guided propagation, MGP）[^18]，Tubelet  rescoring [^18]，Seq-NMS [^12] 。MGP和Tubelet rescoring被应用于VID比赛中，我们使用其官方的代码，然后实现了Seq-NMS。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/FGFA/1510389593609.png" alt="1510389593609"></p>
<p>表4显示了结果。该三项技术第一次组合在单帧方法上。说明，这些后处理方法是有效的，并且可以看到Seq-NMS带来的提升最大。FGFA与那些方法配合时，只有Seq-NMS看到了提升。在使用Aligned-Inception-ResNet作为特征网络时，精度提升到80.1%，说明Seq-NMS和FGFA高度的互补。</p>
<p><strong>比较其他先进系统</strong></p>
<p>不像目标检测，视频目标检测缺少评估方法和比较准则。ImageNet VID 的现有方法能得到较好的结果，但是他们都是高度复杂且工程化的系统。这使得直接公平的比较不同的工作很困难。</p>
<p>本项工作旨在规则的视频目标检测学习框架，而不是最好的系统。FGFA相比于单帧检测方法的提升说明了其有效性。最为参考，VID 2016的冠军方法是81.2%mAP。其使用了各种技术，例如模型组合，级联检测，语义信息，多尺度推测等等。而我们的方法没有使用这些技术，就达到了80.1%的 mAP。因此我们的方法相比目前最好的工程系统具有高度的竞争性。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[^3]: T. Brox and J. Malik. Large displacement optical flow: de- scriptor matching in variational motion estimation. TPAMI, 2011.<br>[^5]: J. Dai, Y. Li, K. He, and J. Sun. R-fcn: Object detection via region-based fully convolutional networks. In NIPS, 2016.<br>[^6]: J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y.Wei. Deformable convolutional networks. arXiv preprint arXiv:1703.06211, 2017.<br>[^8]: A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, P. v.d. Smagt, D. Cremers, and T. Brox. Flownet: Learning optical flowwith convolutional networks. In ICCV, 2015.<br>[^12]: W. Han, P. Khorrami, T. Le Paine, P. Ramachandran, M. Babaeizadeh, H. Shi, J. Li, S. Yan, and T. S. Huang. Seq-nms for video object detection.<br>[^16]: N. Hyeonseob and H. Bohyung. Learning multi-domain con- volutional neural networks for visual tracking. In CVPR, 2016.<br>[^17]: E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In CVPR, 2017.<br>[^18]: K. Kang, H. Li, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang, Z. Wang, R. Wang, X. Wang, and W. Ouyang. T-cnn: Tubelets with convolutional neural networks for object de- tection from videos. arXiv preprint arxiv:1604.02532, 2016.<br>[^19]: K. Kang,W. Ouyang, H. Li, and X.Wang. Object detection from video tubelets with convolutional neural networks. In CVPR, 2016.<br>[^21]: A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei. Large-scale video classification with convo- lutional neural networks. In CVPR, 2014.<br>[^23]: B. Lee, E. Erdenee, S. Jin, M. Y. Nam, Y. G. Jung, and P. K. Rhee. Multi-class multi-object tracking using changing point detection. In ECCV, 2016.<br>[^25]: W. Lijun, O. Wanli, W. Xiaogang, and L. Huchuan. Visual tracking with fully convolutional networks. In ICCV, 2015.<br>[^28]: A. Ranjan and M. J. Black. Optical flow estimation using a spatial pyramid network. arXiv preprint arXiv:1611.00850, 2016.<br>[^31]: J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid. Epicflow: Edge-preserving interpolation of correspondences for optical flow. In CVPR, 2015.<br>[^33]: O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. Berg, and F.-F. Li. Imagenet large scale visual recognition challenge. In IJCV, 2015.<br>[^38]: L. Sun, K. Jia, D.-Y. Yeung, and B. E. Shi. Human action recognition using factorized spatio-temporal convolutional networks. In ICCV, 2015.<br>[^41]: D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. Learning spatiotemporal features with 3d convolutional net- works. In ICCV, 2015.<br>[^42]: D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. Deep end2end voxel2voxel prediction. In CVPR Workshop, 2016.<br>[^44]: P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid. Deepflow: Large displacement optical flowwith deep match- ing. In ICCV, 2013.<br>[^49]: X. Zhu, Y. Xiong, J. Dai, L. Yuan, and Y.Wei. Deep feature flow for video recognition. In CVPR, 2017.</p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Video Object Detection</category>
      </categories>
      <tags>
        <tag>ICCV2017</tag>
        <tag>Video Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记《DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling》</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-DeNet-Scalable-Real-time-Object-Detection-with-Directed-Sparse-Sampling/</url>
    <content><![CDATA[<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/DeNet/1511406384680.png" alt="1511406384680"></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>​ICCV2017</li>
<li><a href="https://github.com/lachlants/denet">code</a></li>
<li>ROI Sparse Sampling<span id="more"></span></li>
</ul>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><h3 id="1-2-Probabilistic-Object-Detection"><a href="#1-2-Probabilistic-Object-Detection" class="headerlink" title="1.2 Probabilistic Object Detection"></a>1.2 Probabilistic Object Detection</h3><ul>
<li>一个BBox只能对应一个类别的一个实例</li>
<li>因为没有前置过滤，所以存在大量的候选ROI</li>
<li>所以将采样是目前可行的方法</li>
<li>YOLO和Faster R-CNN采样数量在$10^4$ 到$10^5$，然后通过简单的线性回归精细化BBox位置</li>
<li>我们认为目标实例是大规模采样的一个非常小的子集</li>
<li>基于此，我们开发端到端的CNN来估计该稀疏分布</li>
</ul>
<h2 id="2-Dirested-Sparse-Sampling-DSS"><a href="#2-Dirested-Sparse-Sampling-DSS" class="headerlink" title="2. Dirested Sparse Sampling (DSS)"></a>2. Dirested Sparse Sampling (DSS)</h2><ul>
<li>DSS用于联合优化two stage CNN</li>
</ul>
<h3 id="2-1-Corner-based-RoI-Detector"><a href="#2-1-Corner-based-RoI-Detector" class="headerlink" title="2.1 Corner-based RoI Detector"></a>2.1 Corner-based RoI Detector</h3><ul>
<li>提出一个概念：BBox角估计用于ROI估计</li>
<li>估计图像上每个位置是否是BBox的一个角 $Pr(t|k,y,x)$ </li>
<li>$k \in { top<del>left,top</del>right,bottom<del>left,bottom</del>right }$ 一个角类型</li>
<li>一个BBox包含物体的似然估计 $Pr(s\neq null|B)\propto \prod_{k} Pt(t|k,y_k,x_k)$ 即BBox四个角的概率的乘积</li>
</ul>
<h3 id="2-2-训练"><a href="#2-2-训练" class="headerlink" title="2.2 训练"></a>2.2 训练</h3><ul>
<li>先前向传播获得$B_s$</li>
<li>并且随机采样Ground Truth增强$B_s$</li>
<li>然后通过后续的分类网络训练</li>
<li>为了保证端到端训练，$B_s$不变</li>
<li>因此corner检测网络在bbox分类和估计任务中联合训练</li>
<li>DeNet模型在corner概率分布，分类分布，bbox回归上进行联合优化</li>
</ul>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/DeNet/1511406913328.png" alt="1511406913328"></p>
<ul>
<li>$\phi$ 是ground truth corner 和 classification 分布</li>
<li>$\phi_{B,i}&#x3D;{x_i,y_i,w_i,h_i}$ 是ground truth 与 bbox中IoU最大的box</li>
<li>$\lambda_s,\lambda_t,\lambda_b$ 是用户定义的常数</li>
<li>$\Lambda_s,\Lambda_t,\Lambda_b$ 是归一化常数，使得每个部件为1</li>
<li>$\phi(t|k,y,x)$ 是遍历ground truth后得到的corner map，超出边界的corner则丢弃</li>
<li>$\phi(s,B)$ 是预先遍历的采样bbox和ground truth的分类结果</li>
</ul>
<h3 id="2-3-Detection-Model"><a href="#2-3-Detection-Model" class="headerlink" title="2.3 Detection Model"></a>2.3 Detection Model</h3><ul>
<li>Residual neural networks 比 Faster R-CNN 展现出更好的性能</li>
<li>我们选择34层，ResNet-34作为我们的base网络，参数21M</li>
<li>对基本模型，输入图像均归一化为512$\times$512，去除最后的平均池化和全连接层，并连接两个反卷积层和一个corner detector</li>
<li>corner detector负责生成corner分布，并通过每个位置的特征$F_s$得到特征采样图</li>
<li>反卷积层增加特征图的分辨率</li>
<li>corner detector之后是sparse layer，其观察corner标识符，生成采样ROIs</li>
<li>ROIs用于提取一组N$\times$N的特征向量，从特征采样图中</li>
<li>至此，我们从4.2M个bbox中稀疏采样了$N^2$个bbox</li>
<li>特征向量包括最近近邻的采样特征，一个7$\times$7的网格加上bbox的宽高。$7 \times 7\times F_s + 2$</li>
<li>近邻采样是足够的，特征采样图具有相似，高度相关，稀疏分辨率</li>
<li>最后特征向量通过一个相对浅全连接网络生成最后的分类结果和细调边界的ROI</li>
</ul>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/DeNet/1511425311403.png" alt="1511425311403"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/DeNet/1511425339378.png" alt="1511425339378"></p>
<p>表1和表2描述了增加的网络信息，相关定义如下：</p>
<ul>
<li><strong>Conv</strong> 参数使用正态分布，$\sigma^2&#x3D;2&#x2F;(n_fn_xn_y)$，之后是batch normalization和ReLU激活函数</li>
<li><strong>Deconv</strong> ReLU之后是反卷积，其等价于先扩大空间分辨率再进行卷积</li>
<li><strong>Corner</strong> 通过soft-max函数估计corner分布，并生成采样特征图</li>
<li><strong>Sparse</strong> bbox采样标识符，从corner分布，并生成一个固定尺寸的采用特征，从采用特征图</li>
<li><strong>Classifier</strong> 特征图到概率分布，通过softmax函数，并生成bbox</li>
</ul>
<p>对于DeNet-34我们使用ResNet-34作为base model，$F_s &#x3D; 96$  用于产生特征向量4706个值，32M个参数。DeNet-101模型使用ResNet-101 base model 增加的filter数量大约是1.5$\times$。</p>
<h4 id="2-3-1-Skip-Layer-Variant"><a href="#2-3-1-Skip-Layer-Variant" class="headerlink" title="2.3.1 Skip Layer Variant"></a>2.3.1 Skip Layer Variant</h4><p>skip层连接反卷积层和最后的输出层，使其具有相同的空间维度。每一个skip layer将原特征线性投影到目标特征维度，并简单的增加激活层之前的特征图。</p>
<h4 id="2-3-2-Wide-Variant"><a href="#2-3-2-Wide-Variant" class="headerlink" title="2.3.2 Wide Variant"></a>2.3.2 Wide Variant</h4><p>通过增加翻卷机和skip层，使得corner和feature sampling map的空间分辨率为128$\times$128。同时N增加到48，产生2304个ROI。虽然目前的版本由于增加了分类步骤使得计算开销增加，但是通过工程方法，作者相信可以减少计算开销。</p>
<h2 id="3-实现细节"><a href="#3-实现细节" class="headerlink" title="3. 实现细节"></a>3. 实现细节</h2><p>代码基于Theano，下载地址：<a href="https://github.com/lachlants/denet">https://github.com/lachlants/denet</a></p>
<h3 id="3-1-训练方法"><a href="#3-1-训练方法" class="headerlink" title="3.1 训练方法"></a>3.1 训练方法</h3><p>使用Nesterov风格的SGD，初始lr&#x3D;0.1，动量&#x3D;0.9，权重衰减&#x3D;0.0001。一个batch&#x3D;128，32个样本。epoch&#x3D;30，60时，lr除以10，一共90个epoch。其他的超参与原始的residual network一样。没有使用在线困难负样本挖掘或其他梯度优化技术，增加10%的负样本在训练阶段。</p>
<p>为了改进模型对不同尺度的泛化性，对每个样本，生成一个最小的边框得到一个方形图像。在测试时，使用双线性差值得到512$\times$512的图像，在训练时随机crop一个0.08~1.0相对大小的图像，宽高比(3&#x2F;4，4&#x2F;3)。如果该crop图像没有包含50%的GT，则再生成一个。如同测试一样所有crop都缩放到512$\times$512。并且也使用随机光度调整和镜像调整（对比、饱和、明亮）[^20]。</p>
<h3 id="3-2-定义采样BBox"><a href="#3-2-定义采样BBox" class="headerlink" title="3.2 定义采样BBox"></a>3.2 定义采样BBox</h3><p>一个简单的方法快速搜索corner分布</p>
<ol>
<li>搜索$Pr(t&#x3D;1|k,y,x)&gt;\lambda$的点$C_\lambda$</li>
<li>对每个corner类型，选择M个具有最大似然的corners $C_M \subseteq C_\lambda$</li>
<li>生成一组唯一的bbox，通过用$C_M$中每个左上匹配每个右下corner</li>
<li>计算每个bbox的概率</li>
<li>重复步骤2和3用右上和左下</li>
<li>对bbox的概率排序，保留前$N^2$产生采样bbox$B_S$</li>
</ol>
<p>此方法在步骤1过滤了绝大部分corner，因此相比暴力穷举能加速。</p>
<h2 id="4-结果和分析"><a href="#4-结果和分析" class="headerlink" title="4. 结果和分析"></a>4. 结果和分析</h2><p>模型比较是困难的，因为使用了不同的base model，数据增强策略，数据集融合。特别的，我们注意到SSD使用更大的batch size，而R-CNN使用更大的输入分辨率。我们的DeNet使用一个Titan X GPU 进行训练，8x 的 batch size与SSD设置一样。为了简洁，我们只包含三种Faster R-CNN模型，RPN（VGG），ResNet-101 extension RPN+，R-FCN。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/DeNet/1511856085900.png" alt="1511856085900"></p>
<p>表3提供了baseline模型的总体情况。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/DeNet/1511856174220.png" alt="1511856174220"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/DeNet/1511856195399.png" alt="1511856195399"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/DeNet/1511856267266.png" alt="1511856267266"></p>
<h3 id="4-1-超参优化"><a href="#4-1-超参优化" class="headerlink" title="4.1 超参优化"></a>4.1 超参优化</h3><p>我们使用3.1节使用的训练策略，除了batch使用96。Denet-34模型在07的train和12的trainval上训练，测试在07的val上测试。</p>
<p>表4为粗略搜索corner和bbox回归损失参数。当$\lambda_s&#x3D;1,\lambda_t&#x3D;100,\lambda_b&#x3D;1$时得到的最优的结果。之后我们研究了bbox采样数量的影响，我们设置模型训练的采样数量N&#x3D;{8,12,16,24,32}（注意：采样数量是N的平方）。在测试阶段，我们变化N的值从8到32生成图2。表5中，我们提供了采样bbox数量和检测速率、coverage覆盖率（就是召回率）的关系，如同第二节上的描述。很明显，虽则采样bbox的增加，MAP持续改进。同时，增加测试的bbox，也会改进MAP，但是会降低检测率。在之后的实验中，我们设置N&#x3D;24用于训练和测试。</p>
<h3 id="4-2-效率分析"><a href="#4-2-效率分析" class="headerlink" title="4.2 效率分析"></a>4.2 效率分析</h3><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/DeNet/1511873270775.png" alt="1511873270775"></p>
<p>表6展示了DeNet模型不同模块的消耗时间，我们将运行时间分为4个stage</p>
<ol>
<li>corners估计，图像上传到GPU，通过网络后生成corner分布和采样特征图，corner分布之后从GPU迁移到CPU</li>
<li>生成ROI，根据corner分布，生成采样bbox</li>
<li>分类ROI，最后分类CNN执行，分类分布到bbox回归输出，从GPU到CPU</li>
<li>实例估计，在检测结果上进行NMS</li>
</ol>
<p>我们发现大部分时间都消耗在生成corners，cpu生成ROI也需要许多时间。但是DeNet还是比大部分其他baseline方法快。</p>
<ul>
<li>反卷积，通过反卷积层增加空间信息，不同于R-FCN和SSD方法。这种方法在后面进行，能极大的改进评估效率。</li>
<li>快速ROI特征，通过简单的最近邻采样方法提取特征，限制特征的数量为每ROI有49个。有个RPN变体使用49~580</li>
<li>输入图像纬度，DeNet将所有图像缩放到512$\times$512，而一些RPN方法会将输入变为1000$\times$600</li>
<li>Batching，我们的模型8x采样每batch，改进GPU的利用。</li>
</ul>
<h3 id="4-3-ROI召回率比较"><a href="#4-3-ROI召回率比较" class="headerlink" title="4.3 ROI召回率比较"></a>4.3 ROI召回率比较</h3><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/DeNet/1511875816738.png" alt="1511875816738"></p>
<p>表7显示了300个ROI的情况下的召回率。我们发现在IoU较低的时候RPN和R-FCN的召回率更好，但是增加IoU后DeNet极大的改进了召回率。</p>
<p>我们发现RPN&#x2F;R-FCN利用了bbox回归和nms方法，在推荐网络中。DeNet的ROI召回率不意味着完整模型的MAP。</p>
<h3 id="4-4-MSCOCO"><a href="#4-4-MSCOCO" class="headerlink" title="4.4 MSCOCO"></a>4.4 MSCOCO</h3><p>微软的COCO数据集包含82K的训练和40K的验证图像，包含80个类别。对于测试他们数据集包含80K的测试集，用户已知20K的测试test-dev2015，和未知的20K图像test2015只允许5次评估。由于数据集体积、类别的数量、目标相对大小，MSCOCO相比于Pascal VOC更具挑战性。COCO的评估方法是MAP，在IOU&#x3D;0.5到0.95。该评估策略更加注重定位性能。设置$\lambda_t&#x3D;50$是DeNet-101收敛必要的。DeNet-34训练<del>4天，2个Tesla P100 GPU；DeNet-101需要</del>6.5天，4个Tesla P100GPU。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/DeNet/1511880881841.png" alt="1511880881841"></p>
<p>表8显示了在COCO test-dev2015上的效果，DeNet展现出高献策速度上的优势，相比SSD300，MAP提升6.2%，比SSD512提升2.6%。DeNet-101的优势也很明显， 只是被RPN+击败。文章撰写时，DeNet在COCO上排名前10，其不考虑计算效率。skip 模型变体继续改进中小目标的性能。wide 模型变体改进小目标和定位性能。DeNet在更多的候选ROI上进行选择，相比SSD。RPN和YOLO等方法是不可能使用如此大的候选ROI。</p>
<h3 id="4-5-Pascal-VOC-2007"><a href="#4-5-Pascal-VOC-2007" class="headerlink" title="4.5 Pascal VOC 2007"></a>4.5 Pascal VOC 2007</h3><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/DeNet/1511881135045.png" alt="1511881135045"></p>
<p>表9现在了在Pascal上的测试，训练集用07+12的trainval，测试集用07。DeNet-34训练13小时，DeNet-101训练20小时。DeNet-34 skip版本比SSD300的MAP提升1.6%，速度快20Hz。DeNet-101性能匹配SSD512，速度更快。</p>
<h3 id="4-6-Pascal-VOC-2012"><a href="#4-6-Pascal-VOC-2012" class="headerlink" title="4.6 Pascal VOC 2012"></a>4.6 Pascal VOC 2012</h3><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/DeNet/1511881170641.png" alt="1511881170641"></p>
<p>训练集为07的trainvaltest+12的trainval，测试集为12的testing server。我们可以发现DeNet-34性能匹配SSD300，但是不知道什么原因，DeNet-101性能低于SSD512，并且性能接近RPN+。训练时间分别为18小时，28小时。</p>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p>本文提供了一个基于CNN的稀疏框架，包含ROI检测器和分类模型，通过减少手工优化得到先进的性能和实时效率。通过反卷积和skip层，我们得到更高的计算效率模型通过更耦合ROI、分类、bbox回归。我们的实验表面，skip连接能改进中小目标的检测性能。wide模型表面corner图的分辨对于中小目标的重要性，其提供了未来的研究方向。</p>
<p>通过分析发现，我们的模型定位性能更好。我们认为是由于我们的采样候选ROI更多（4.2$\times 10^6$）而SSD512和RPN是(2.5$\times 10^4$) 。这使得模型可能选择一个更加接近gt的bbox。另外，由于我们不再设定一组reference bbox，因此减少了手工调整的需要（就是anchor设置）。这样就不存在匹配各种各样目标尺寸和分辨率的问题。</p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>本文的corner map是该文章的亮点，相当于网络进行了一次bbox的位置粗略估计，其本质上跟rpn没有区别，但是却能得到更快的效率，其原因是bbox的粗略估计是非常快的，然后就马上进行了一次预筛选。而RPN等方法需要将anchor完整计算一遍因此效率更慢，corner map是一次计算得到，之后的bbox生成不再需要神经网络。而RPN需要对每个位置进行全连接网络的计算，因此速度较慢。</p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>ROI</tag>
      </tags>
  </entry>
  <entry>
    <title>解决Markdown图片存放</title>
    <url>/Tutorial-%E6%95%99%E7%A8%8B/Markdown-storage-image/</url>
    <content><![CDATA[<p>使用对象存储保存markdown图片</p>
<span id="more"></span>

<p>markdown的优势是简洁，纯文本。但是当需要插入图片时就很烦人。需要截图，放到一个文件夹里，然后写相对路径。当我把笔记给另外一个人时，要连着图片文件夹一起给，很麻烦。</p>
<p>那有没有什么办法可以解决呢？那是肯定有的。markdown的图片路径可以是一个url，因此如果你将图片放置在一个服务器上，那么只要是联网的电脑，md文件上的图片就可以直接查看了。</p>
<p>那要怎么才能有个存图片的服务器呢？</p>
<ol>
<li>自己搭一个服务器</li>
<li>租一个虚拟服务器，安装一个apache或nginx</li>
<li>租一个对象存储 OSS</li>
</ol>
<p>第三种是我目前所想到的最简单的方法。</p>
<p>那需要怎么做？</p>
<ol>
<li>选择一个云存储服务商，七牛或阿里云，我选择的是阿里云，学生优惠，40G，1年9.5块，用来存图片绰绰有余了。</li>
<li>进入阿里云的管理控制台就可看到对象存储OSS，买一个资源包，包月包年那种</li>
<li>新建一个bucket，自己定一个名称例如：<code>zavix-image</code></li>
<li>上传文件</li>
<li>设置续写权限，公共读（否则你的图片连接在浏览器和md中是打不开的）。</li>
<li>然后你可以看到一个外网访问域名：<code>zavix-image.oss-cn-shenzhen.aliyuncs.com</code></li>
<li>然后你就可以在md里显示你的图片了，例如你上传了一个图片<code>snow_ball.png</code>，在md里就<code>![](http://zavix-image.oss-cn-shenzhen.aliyuncs.com/snow_ball.jpg)</code> 这样就可以了！</li>
</ol>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/snow_ball.jpg" width=50%/>

<p>另外OSS还要写其他功能，例如：</p>
<ol>
<li>绑定域名：就是把前面那段长的域名<code>http://zavix-image.oss-cn-shenzhen.aliyuncs.com</code>换成你自己的域名，例如<code>zavix-image.oss-cn-shenzhen.aliyuncs.com</code>，但是前提是域名进行了备案，目前我的域名正在备案中。这样能防止出现你之后修改了OSS的服务商或名称后，所有图片链接都要修改的问题。但是这个难度不亚于第2种方法，因此动手能力强的，直接搜linux，ubuntu，nginx，自己建服务器就好了。</li>
<li>图片处理，建立样式，这部分我不太会</li>
<li>计算处理，貌似能出发一些函数计算。</li>
</ol>
]]></content>
      <categories>
        <category>Tutorial 教程</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记《Individualness and Determinantal Point Processes for Pedestrian Detection》</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Pedestrian-Detection/Note-Individualness-and-Determinantal-Point-Processes-for-Pedestrian-Detection/</url>
    <content><![CDATA[<p>&lt;img src&#x3D;”<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1511942386442.png%22/">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1511942386442.png&quot;/</a> width&#x3D;”80%”&gt;</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文，我们介绍了检测候选的个体性作为一个补充用于评价行人检测。个体性从目标推荐和滑窗得到的原始检测候选中分配一个检测结果给每个物体。我们表明，传统的方法，如NMS，是次优的，因为其仅基于周围检测结果的得分来抑制。我们使用行列式点过程(determinantal point process)结合个体性优化选择最后的结果。该方法使用质量和相似性对每个检测结果建模。然后，检测结果中具有高检测得分和低相关性（通过矩阵行列式计算概率）的作为最后的结果。此矩阵由quality terms作为对角元素，其他位置是相似性元素。具体地，我们专注于行人检测问题，该问题由于其频繁的遮挡和不可预知的运动使其称为最具挑战的问题之一。实验结果显示本文提出的方法比NMS和不受约束的二次优化问题更好。</p>
<span id="more"></span>

<ul>
<li>机构：首尔大学，加州大学</li>
<li>行人检测后处理，个体性估计</li>
<li>代码:<a href="http://cpslab.snu.ac.kr/software">http://cpslab.snu.ac.kr/software</a></li>
</ul>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>目标检测的目的是在图像中定位一个已知类别的目标。这对于许多视觉任务的基础，例如跟踪、场景识别、动作识别等。在视觉跟踪中，基于检测的跟踪是在连续图像中基于连续的检测结果定位目标，是非常有效的方法[^1]。通过定位图像中目标的位置，我们能更好理解场景中发生的事情[^2]。目标检测也应用于动作识别，通过发现一系列特定相关的动作序列[^3]。</p>
<p>一般目标检测的框架在测试图像上进行滑窗或者通过训练的分类器推荐目标。目前已经有许多目标检测器能够将独立的目标检测好。然而，一个检测器对每个目标的检测结果周围都会产生大量的原始检测结果（raw detection responses），如图1a所示。冗余的检测结果一般通过贪心算法进行抑制得到图1b，如NMS。</p>
<p>采用上述框架，很难检测高度被遮挡的目标。因为分类器在训练时设计为用于分辨不同的类别，而不是分别类内差异。例如图1中A、B行人外接矩形高度重叠，我们检测时需要判断是FP还是FN。基于NMS，由于A被遮挡，其检测得分会低于B，因此会被抑制。因此，NMS方法不可避免的产生一些FN，当目标相互遮挡时。另一方面，如果一些先验知识能够利用，例如不同的identities，这样的问题就能避免。事实上，false rejection能够极大影响检测精度，例如原始的检测结果召回率为90%，经过NMS之后检测结果仅为50%。因此，从原始检测结果挑选是一件重要的任务。</p>
<p>本文，我们提出一个算法基于个体性（individualness）和行列式点过程（determinantal point process, DPP）用于精确检测，并且可以应用于任何目标检测器。该方法可以作为目标性检测的一个独立补充。物体性检测得到一组候选结果，个体性检测发现候选结果的关系，得到最终检测结果。我们定义个体性使用特征向量的相关性，其包含bbox中的外观和空间信息。具体的，我们关注人群场景中的多行人检测问题。</p>
<p>DPP是一个随机过程，使用量子物理理论中的互斥粒子模型，其禁止高度相关的量子态同时出现。这个特性很适合排除冗余的检测结果。为了应用DPP，需要定义质量和多样性因子，通常为unary score和pair-wise correlation。基于此，我们可以选择一个更优化的子集如图1c所示。</p>
<p>本文的贡献如下所示：</p>
<ol>
<li>现有检测框架中，从候选检测结果中选择最后结果的方法存在问题</li>
<li>引入DPP增强检测精度，通过设计质量和多样性特征设计</li>
<li>用DPP选择优化的检测结果，并在多个行人数据集上验证。</li>
<li>在PETS 2009上，DPM基于本文方法达到41.9%的准确度和99%的精度，而NMS只达到23.2%的准确度和98.2%的精度。</li>
<li>在30个行人的图像上处理超过300个候选结果的时间少于30ms</li>
</ol>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>行人检测方法：HOG、SVM、DPM、Boosting-Based，Faster R-CNN</p>
<p>融合检测结果：NMS。</p>
<ol>
<li>文献[^13]指出定位精确性与检测得分没有很强的相关性。因此其提出一个回归模型学习检测结果与GT的相对位置。</li>
<li>文献[^14]将NMS框架整合到深度学习模型中，但是NMS的参数在训练时仍然是固定的。</li>
</ol>
<p>NMS可以看为优化问题：</p>
<ol>
<li>文献[^15]提出二次无约束二元优化方法（quadratic unconstrained binary optimization，QUBO）替代NMS。QUBO的目的是找到一个二元向量，其中每个元素表示对应的检测BBox是否应该被抑制。目标函数包含一元和二元项。一元项测量BBox是行人的置信度，二元项基于BBox的重叠区域进行惩罚。目标函数使用贪心算法求解。QUBO的问题在于其估计行人之间的分布使用二元目标函数。</li>
<li>文献[^16]提出基于吸引力传播聚类（affinity propagation clustering，APC）的方法。统计BBox两辆之间的相似性，然后聚类相似性最大的。 但是APC没有显式的惩罚相互接近的物体。尽管可以增加互斥函数进行改进，但是检测精度也没用显著的增加。</li>
</ol>
<h2 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h2><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512616433920.png" alt="1512616433920"></p>
<p>提出的方法包含连个阶段：物体性检测和个体性检测。本文使用DDP建模BBox之间的关系。</p>
<h3 id="3-1-行列式点过程方程（Determinantal-Point-Process-Formulation）"><a href="#3-1-行列式点过程方程（Determinantal-Point-Process-Formulation）" class="headerlink" title="3.1 行列式点过程方程（Determinantal Point Process Formulation）"></a>3.1 行列式点过程方程（Determinantal Point Process Formulation）</h3><p>定义：</p>
<ul>
<li>DPP基于BBox的质量和相似性计算最终选择的BBox</li>
<li>有个N个BBox，$y$表示所有BBox的集合，$Y$标示最终选择的子集</li>
<li>BBox的质量为$q_i$</li>
<li>BBox的相似性为$S_{ij}$，两个BBox向量的内积</li>
<li>计算正-半正定核矩阵（positive-semidefinite kernel matrix）$L_Y&#x3D;[L_{ij}]_{i,j\in Y}$</li>
<li><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512620048441.png" alt="1512620048441"></li>
</ul>
<p>则选择BBox的过程即为最优化问题</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512619743792.png" alt="1512619743792"></p>
<p>通常来说这是一个NP-hard（non-deterministic polynomial）问题，需要对所以的可能进行评估，但是幸运的是该问题是log-submodular问题，可以通过简单的贪心算法估计。</p>
<blockquote>
<p><a href="https://baike.baidu.com/item/NP-hard">百度</a><br>NP-hard，其中，NP是指非确定性多项式（non-deterministic polynomial，缩写NP）。所谓的非确定性是指，可用一定数量的运算去解决多项式时间内可解决的问题。NP-hard问题通俗来说是其解的正确性能够被“很容易检查”的问题，这里“很容易检查”指的是存在一个多项式检查算法。相应的，若NP中所有问题到某一个问题是图灵可归约的，则该问题为NP困难问题。</p>
</blockquote>
<p>例如图，当选择$Y&#x3D;{i,j}$时，DPP选择其的得分$P_L(Y)$为</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512621004358.png" alt="1512621004358"></p>
<p>可以发现，当$q$增大时行列式的值增加，当$|S_{i,j}|$增大时，行列式的值减小。因此DPP过程自动的选择高质量和低相似性的组合。</p>
<h3 id="3-2-质量项"><a href="#3-2-质量项" class="headerlink" title="3.2 质量项"></a>3.2 质量项</h3><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512629712724.png" alt="1512629712724"></p>
<p>一般来说质量项就是检测得分，但是原始得分是相互独立的。我们提出一个检测策略考虑周围结果的关系。一个图像中的虚警可能会抑制真正行人BBox。这种问题在BBox很大时更严重，统计一个GT中具有的检测BBox数量如图3b所示。可以发现大部分GT中包含的BBox非常少（这里有个问题，是不是本来滑窗ROI的尺寸本来就比较大，然后0也被统计进去了）。基于这个发现，提出重得分函数</p>
<p>$$s_i^c&#x3D;s_i^oexp(-\lambda n_i)$$</p>
<p>$\lambda$是一个常数。并且这样还可以得到更加贴紧行人的BBox，如第四节所示。（这个想法很特别，而且似乎特别适合二元检测任务，因为根据透视原理，不可能在一个大目标前面还存在多个小目标）。</p>
<p>特别地，在相机固定的场景中，行人的高度变化不大，而且高度$h_i$与位置$(x_i,y_i)$存在函数关系，可以容易的拟合出函数的系数：</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512630353869.png" alt="1512630353869"></p>
<p>假设行人的身高成高斯分布，我们根据BBox的偏差来对其进行重打分：</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512630798943.png" alt="1512630798943"></p>
<p>最后质量项$q$表示为：</p>
<p>$$q&#x3D;\alpha s + \beta$$</p>
<p>权重的设置需要根据检测器来确定，比如DPM的平均得分为0.7，ACF的得分是33.2。</p>
<h3 id="3-3-个体性和多样性特征"><a href="#3-3-个体性和多样性特征" class="headerlink" title="3.3 个体性和多样性特征"></a>3.3 个体性和多样性特征</h3><p>个体性旨在确定两个图像是否是指同一个人。类似多相机环境中的行人身份再识别问题。但是这两个问题不同且难度不一样。</p>
<ol>
<li>重叠区域有完全相同的信息</li>
<li>两个图像快交叠靠的很近</li>
<li>需要考虑被遮挡的行人，再识别问题通常不考虑</li>
</ol>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512631315204.png" alt="1512631315204"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512633314567.png" alt="1512633314567"></p>
<p>为了克服这个问题，我们考虑测量BBox特征描述的相关性。特征应该对背景、尺度不敏感。为了实现这个目的，我们采用卷积特征。总体上，相关矩阵是块对角并且个体相关性较低，因此我们决定采用CNN特征。</p>
<p>仅只用CNN不够有效，如图4所示，对单个行人有多个聚类结果，例如图5中间男人BBox右边有个BBox，尽管两个BBox中有轮廓不一样，但是仍然是一个行人。为了解决这个问题，我们增加考虑BBox的控制位置。空间个体性设计为单个行人周围的BBox具有高相关性。则第i个检测结果的个体性表示为：</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512635866556.png" alt="1512635866556"></p>
<p>k表示像素序号，$\pi_i$表示属于检测结果i的bbox的像素集合。尽管$\varphi_i$的纬度等同于图像尺寸，但是两个BBox的空间相关性可以检测计算得到:</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512636179348.png" alt="1512636179348"></p>
<p>同时不需要存储完整的相关性向量，只需保存检测的BBox尺寸和重叠面积：</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512636263251.png" alt="1512636263251"></p>
<p>（晕，这不就是IOU么）</p>
<p>得到了BBox的物体性特征$$\phi_i$$和$$\varphi_i$$，如何融合两个特征为一个多样性特征？可以采用平均值，但是其要求特征具有相同纬度。我们提出一个更通用有效的方法设计多样性特征，并直接构造正-半正定相似性矩阵S。$S^c$，$S^s$分别为有两个特征构建，换句话说$$S_{ij}^c&#x3D;\phi_i^T\phi_j$$。然后使用一个平和权重（0.8）融合：</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512636884079.png" alt="1512636884079"></p>
<h3 id="3-4-Mode-Finding"><a href="#3-4-Mode-Finding" class="headerlink" title="3.4 Mode Finding"></a>3.4 Mode Finding</h3><p>这一节描述如何求解上面的最优化函数。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512637471602.png" alt="1512637471602"></p>
<ol>
<li>每次选择最大的j</li>
<li>加入到Y集合中</li>
<li>计算P，若增加倍数超过阈值，则继续，并将j从y中删除</li>
<li>直到P不再增加，或y为空</li>
</ol>
<h3 id="3-5-与QUBO的关系"><a href="#3-5-与QUBO的关系" class="headerlink" title="3.5 与QUBO的关系"></a>3.5 与QUBO的关系</h3><p>QUBO使用DPP目标函数转换为相似的形式。DPP是寻找最大化$L_Y$的行列式。QUBO有两个缺点</p>
<ol>
<li>QUBO cannot deal with positively correlated items. 这句话不理解</li>
<li>QUBO 更加惩罚高度相关的BBox，可能不适合遮挡的行人。</li>
</ol>
<p>假设$L_y&#x3D;[2,-0.8;-0.8,1.4]$，QUBO将不选择第二个检测结果因为$-0.8-0.8+1.4&#x3D;-0.2&lt;0$，在DPP中$det(Y_L)&#x3D;2.16&gt;2$ 因此会选择第二个结果。</p>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h2><p>数据集</p>
<ul>
<li>INRIA：288个图像</li>
<li>PETS2009：S1.L1的190帧，平均每帧33个行人，行人较密</li>
<li>EPFL Terrace：5010帧，每25帧取1帧，平均每帧5人</li>
</ul>
<p>模型：DPM，ACF，Faster RCNN</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512654724175.png" alt="1512654724175"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512654762753.png" alt="1512654762753"></p>
<p>表1和图7显示结果，可以看见DPP都比NMS好或一致，并且在PET2009这类数据集上有显著提升。并且RCNN这类方法在INRIA上有较好的效果，但是在PET2009上效果不好，可能是因为区域推荐的方法会生成很多重叠的框，而不是独立个体的框。上述实验不会使用先验知识。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512655923604.png" alt="1512655923604"></p>
<p>图8a里面有贪心NMS，另一种为特殊NMS$\frac{area(d_g\bigcap d_e)}{min(area(d_g),area(d_e))}&gt;0.65$ 能够实现更好的精度。非贪心NMS不排除被抑制的BBox。QUBO的精度与非贪心NMS相似。并且DPP比其他的都显著改善。</p>
<p>图8b表示了CNN网络层的有效性。显示，检测结果对不同的网络层不敏感。我们使用4096维向量用于计算多样性特征。</p>
<p>图8c显示了计算速度。最多300个proposal，只统计算法1的计算时间。卷积计算时间248ms。计算平台Intel Xeon 2.3 GHz，128 RAM，TITAN X D5 12GB CPU。处理过程平均少于30ms。</p>
<p>BBox定位精度使用$\frac{|d_e\bigcap d_g|}{|d_e|}$测量。PET2009中DPP是0.81，NMS是0.76。图9显示一些结果示例。DPP产生更贴紧的BBox，如EPFL-84帧，NMS和QUBO都漏检了中间穿白衣服的人，DPP成功检测。</p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h2><p>我们提出一种基于个体性改进检测性能的算法。个体性评价两个检测候选的相似性，物体性基于得分生成候选框。每个BBox外观和空间信息用于考虑个体性。然后行列式点处理过程结合得分和相似性得到最后结果。实验结果表明DPP比NMS和QUBO更好。并且在平均包含30个行人的图像上处理300个候选框的速度小于30ms。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[^13]: Liu, S., Lu, C., Jia, J.: Box aggregation for proposal decimation: last mile of object detection. In ICCV, pp. 2569–2577 (2015)<br>[^14]: Wan, L., Eigen, D., Fergus, R.: End-to-end integration of a convolution network, deformable partsmodel and non-maximum suppression. In CVPR, pp. 851–859 (2015)<br>[^15]: Rujikietgumjorn, S., Collins, R.T.: Optimized pedestrian detection for multiple and occluded people. In CVPR (2013)<br>[^16]: Rothe, R.,Guillaumin,M., Gool, L.V.: Non-maximum suppression for object detec- tion by passing messages between windows. In ACCV (2014)</p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Pedestrian Detection</category>
      </categories>
      <tags>
        <tag>Pedestrian Detection</tag>
        <tag>NMS</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记《Seq-nms for video object detection》</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Video-Object-Detection/Note-Seq-nms-for-video-object-detection/</url>
    <content><![CDATA[<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515727263191.png" alt="1515727263191"></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>用于视频目标检测的NMS方法</p>
<span id="more"></span>

<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>视频目标检测困难的原因：</p>
<ul>
<li>较大的尺度变化</li>
<li>遮挡</li>
<li>运动模糊</li>
</ul>
<p>本文，我们提出单帧检测的一个简单拓展来帮助解决上述问题。</p>
<p>单帧检测完全忽略了时间维度，本文，我们在后处理阶段融合时序信息，以此优化每帧的检测结果。对于给定的时间序列上的ROI和类别得分，我们使用简单的重叠标准来连接邻近帧的BBox，使得序列的得分最大化。之后抑制附近的BBox，然后对BBox重打分。</p>
<p>贡献：</p>
<ul>
<li>提出Seq-NMS改进用于视频视频数据的物体检测流程。特别地，我们改进了后处理阶段，使用前后帧的高分物体结果增强弱检测结果。</li>
<li>Seq-NMS在ImageNet VID上的表现超过先进的单帧检测结果。</li>
<li>方法在ILSVRC2015上排名第3</li>
</ul>
<h2 id="2-我们的方法"><a href="#2-我们的方法" class="headerlink" title="2. 我们的方法"></a>2. 我们的方法</h2><h3 id="2-1-Seq-NMS"><a href="#2-1-Seq-NMS" class="headerlink" title="2.1 Seq-NMS"></a>2.1 Seq-NMS</h3><p>NMS经常会选错BBox，选择的BBox通常较大，且与GT与较小的IOU。大BBox经常有较高的物体得分，可能是在ROI pooling时大BBox能提取更多的信息。为了解决这个问题，我们尝试使用时序信息对bbox重排序。我们假设邻近帧有相同的物体具有相似的位置和尺寸。</p>
<p>我们提出一些经验性的方法：1）序列选择，2）序列重打分，3）抑制。重复此步骤直到没有剩余序列为止。图1显示了此过程。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515729313541.png" alt="1515729313541"></p>
<h4 id="序列选择"><a href="#序列选择" class="headerlink" title="序列选择"></a>序列选择</h4><p>当IoU超过一定阈值后，第一帧的一个BBox与第二帧的BBox连接，我们首先选择潜在的可能的所有连接。然后尝试找到得分最大的序列。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515729999290.png" alt="1515729999290"></p>
<p>可以通过简单的动态规划算法求解。</p>
<h4 id="序列重打分"><a href="#序列重打分" class="headerlink" title="序列重打分"></a>序列重打分</h4><p>尝试使用average或max函数对序列打分</p>
<h4 id="抑制"><a href="#抑制" class="headerlink" title="抑制"></a>抑制</h4><p>选择后的BBox重候选BBox中移除，并且对IoU超过阈值的BBox抑制。</p>
<h2 id="3-数据集"><a href="#3-数据集" class="headerlink" title="3. 数据集"></a>3. 数据集</h2><p>ImageNet VID数据集，30个类别</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515730390416.png" alt="1515730390416"></p>
<h2 id="4-结果"><a href="#4-结果" class="headerlink" title="4. 结果"></a>4. 结果</h2><h3 id="4-1-RPN和分类器训练细节"><a href="#4-1-RPN和分类器训练细节" class="headerlink" title="4.1 RPN和分类器训练细节"></a>4.1 RPN和分类器训练细节</h3><p>首先，在VID训练集上迭代400K次，Fast R-CNN训练迭代200K次。最后固定卷积层，训练400K次，发现RPN在VID验证集上实现90%的召回率。</p>
<p>对于分类器，我们考虑ZF和VGG16两个网络，ZF在VID的训练集上训练，VGG16在2015DET的训练和验证集上预训练。然后VGG在VID上训练，去掉多余的类别单元，然后固定其他层。</p>
<h3 id="4-2-定量结果"><a href="#4-2-定量结果" class="headerlink" title="4.2 定量结果"></a>4.2 定量结果</h3><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515731237073.png" alt="1515731237073"></p>
<p>NMS表示单帧检测，best是表示每个类别选择其最优的策略，然后求其平均。表2显示检测结果。</p>
<p>图3显示了每个类别的mAP的提升，图4显示了每个类别的提升。可以发现，摩托车、海龟、小熊猫、斑马、羊的提升较大。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515731407526.png" alt="1515731407526"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515731433180.png" alt="1515731433180"></p>
<p>表3显示了VID比赛的结果，我们提交的最好结果是48.7%。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515731904233.png" alt="1515731904233"></p>
<p>T-CNN的方法包含以下技术：1）很强的单帧检测器；2）bbox抑制和传播；3）轨迹&#x2F;tubelet 重打分；4）模型组合。其单帧检测器的mAP可达到67.7%如果仅考虑后面两个技术的提升，我们的方法提升更大（7.3% vs. 6.7%）。</p>
<h3 id="4-3-定性分析"><a href="#4-3-定性分析" class="headerlink" title="4.3 定性分析"></a>4.3 定性分析</h3><p>Seq-NMS可以把一些低得分的物体重新找回，但是也可能带入一些虚警。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515732534734.png" alt="1515732534734"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515732575950.png" alt="1515732575950"></p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Video Object Detection</category>
      </categories>
      <tags>
        <tag>Video Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Detectron: caffemodel转pkl</title>
    <url>/Deep-Learning-Dev/Detectron-caffemodel2pkl/</url>
    <content><![CDATA[<p>Detectron 利用caffe2的模型转换功能，将caffemodel转换为pkl，以VGG16为例。</p>
<span id="more"></span>

<p>首先去VGG官网下载pretrained的caffemodel和prototxt，<a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/">http://www.robots.ox.ac.uk/~vgg/research/very_deep/</a><br>然后运行如下代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python2 tools/pickle_caffe_blobs.py \</span><br><span class="line">    --prototxt /home/all/models/VGG_ILSVRC_16_layers_deploy.prototxt \</span><br><span class="line">    --caffemodel /home/all/models/VGG_ILSVRC_16_layers.caffemodel \</span><br><span class="line">    --output output/vgg16.pkl</span><br></pre></td></tr></table></figure>
<p>这时会出现问题</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;tools/pickle_caffe_blobs.py&quot;, line 223, in &lt;module&gt;</span><br><span class="line">    args.prototxt_file_name, args.caffemodel_file_name</span><br><span class="line">  File &quot;tools/pickle_caffe_blobs.py&quot;, line 210, in load_and_convert_caffe_model</span><br><span class="line">    caffenet, caffenet_weights</span><br><span class="line">  File &quot;/home/all/lib/anaconda2/envs/caffe2/lib/python2.7/site-packages/caffe2/python/caffe_translator.py&quot;, line 299, in TranslateModel</span><br><span class="line">    return TranslatorRegistry.TranslateModel(*args, **kwargs)</span><br><span class="line">  File &quot;/home/all/lib/anaconda2/envs/caffe2/lib/python2.7/site-packages/caffe2/python/caffe_translator.py&quot;, line 254, in TranslateModel</span><br><span class="line">    &#x27;I think something is wrong. This translation script &#x27;</span><br><span class="line">ValueError: I think something is wrong. This translation script only accepts new style layers that are stored in the layer field.</span><br></pre></td></tr></table></figure>
<p>查询<a href="https://github.com/caffe2/caffe2/issues/823">issue&#x2F;823</a>可知，这是因为caffe2转换工具使用的是caffe V1版的proto和caffemodel，而早期的caffemodel如vgg16是老版本，因此需要进行升级。<br>升级工具在caffe&#x2F;tools里，因此先下载caffe然后编译，编译前需要修改Makefile.config来设置环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/BVLC/caffe.git</span><br><span class="line">cd caffe</span><br><span class="line">make</span><br></pre></td></tr></table></figure>
<p>然后使用工具升级prototxt和caffemodel</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd build/tools</span><br><span class="line">xuzhewei@LabServer000:~/lib/caffe-1.0/build/tools$ ./upgrade_net_proto_text /home/all/models/VGG_ILSVRC_16_layers_deploy.prototxt /home/all/models/VGG_ILSVRC_16_layers_deploy_V1.prototxt</span><br><span class="line">I0419 14:14:13.562763 23401 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: /home/all/models/VGG_ILSVRC_16_layers_deploy.prototxt</span><br><span class="line">I0419 14:14:13.563534 23401 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter</span><br><span class="line">I0419 14:14:13.563611 23401 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/all/models/VGG_ILSVRC_16_layers_deploy.prototxt</span><br><span class="line">I0419 14:14:13.563650 23401 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.</span><br><span class="line">W0419 14:14:13.563665 23401 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.</span><br><span class="line">I0419 14:14:13.564394 23401 upgrade_net_proto_text.cpp:49] Wrote upgraded NetParameter text proto to /home/all/models/VGG_ILSVRC_16_layers_deploy_V1.prototxt</span><br><span class="line">xuzhewei@LabServer000:~/lib/caffe-1.0/build/tools$ ./upgrade_net_proto_binary /home/all/models/VGG_ILSVRC_16_layers.caffemodel /home/all/models/VGG_ILSVRC_16_layers_V1.caffemodel</span><br><span class="line">I0419 14:15:38.643944 24177 upgrade_proto.cpp:53] Attempting to upgrade input file specified using deprecated V1LayerParameter: /home/all/models/VGG_ILSVRC_16_layers.caffemodel</span><br><span class="line">I0419 14:15:40.964524 24177 upgrade_proto.cpp:61] Successfully upgraded file specified using deprecated V1LayerParameter</span><br><span class="line">I0419 14:15:40.993201 24177 upgrade_proto.cpp:67] Attempting to upgrade input file specified using deprecated input fields: /home/all/models/VGG_ILSVRC_16_layers.caffemodel</span><br><span class="line">I0419 14:15:40.993227 24177 upgrade_proto.cpp:70] Successfully upgraded file specified using deprecated input fields.</span><br><span class="line">W0419 14:15:40.993252 24177 upgrade_proto.cpp:72] Note that future Caffe releases will only support input layers and not input fields.</span><br><span class="line">I0419 14:15:41.512004 24177 upgrade_net_proto_binary.cpp:48] Wrote upgraded NetParameter binary proto to /home/all/models/VGG_ILSVRC_16_layers_V1.caffemodel</span><br></pre></td></tr></table></figure>
<p>最后再运行detectron的转换代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python2 tools/pickle_caffe_blobs.py \</span><br><span class="line">    --prototxt /home/all/models/VGG_ILSVRC_16_layers_deploy_V1.prototxt \</span><br><span class="line">    --caffemodel /home/all/models/VGG_ILSVRC_16_layers_V1.caffemodel \</span><br><span class="line">    --output output/vgg16.pkl</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Deep Learning Dev</category>
      </categories>
      <tags>
        <tag>Detectron</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记《Training Region-Based Object Detectors with Online Hard Example Mining》</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-Training-Region-Based-Object-Detectors-with-Online-Hard-Example-Mining/</url>
    <content><![CDATA[<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Training">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Training</a> Region-based Object Detectors with Online Hard Example Mining&#x2F;f1.png)</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>region-based ConvNets（R-CNN）极大的推动了目标检测领域的发展，但是其训练过程仍然包括很多启发知识和超参来进行优化。我们提出一种简单但是非常有效的<em>在线困难样本挖掘算法online hard example mining（OHEM）</em>用于训练R-CNN检测器。<strong>我们的动机是：在被检测的数据集中总是包含大量简单的样本和少量困难样本。自动的选择这些困难样本进行训练能够更加快速和有效</strong>。OHEM是一种简单直观的算法，能够避免一些常用的启发只是和超参。更重要的是，其对检测性能有稳定和显著的提升。在数据集变得越来越大的今天，如MS COCO，这项工作是十分有意义的。最后，OHEM与该领域先进的方法结合后，在PASCAL VOC 2007和2012数据集上实现78.9%和76.3%的mAP。</p>
<ul>
<li>OHEM</li>
<li>FAIR</li>
<li>Ross Girshick</li>
<li>Hard Example Mining</li>
</ul>
<span id="more"></span>

<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>图像分类和目标检测是计算机时间两项基本任务。通常将目标检测任务简化为分类任务进行训练。这个简化操作带来了图像分类任务中不存在的新挑战：训练集中的目标和背景样本的数量非常不平衡。在滑窗目标检测器中，例如DPM，这种不平衡的比例可以达到100000:1。最近研究提出的基于目标推荐的检测器能够缓解上述问题，但是比例仍然有70:1。这项挑战属于学习方法领域，如何处理不平衡，并且更快速的训练，实现更高精度。</p>
<p>很明显，这不是一个新问题，一种标准的解决方法叫做<code>bootstrapping</code>，现在称为<code>hard negative mining</code>，从提出已有20年。Bootstrapping是Sung和Poggio[^33]在90年代中期提出，用于训练人流检测模型。其核心思想是逐渐的增长或bootstrap，选择检测器检测的虚警样本，进行训练。通过一种迭代的算法，先以当前训练集训练检测器，然后发现新的虚警样本增加到bootstrapped训练集。这个操作一般包含所有的目标样本和少量随机的背景样本（所以说，这个方法可以辅助选择负样本）。</p>
<p>Bootstrapping广泛应用于目标检测研究：</p>
<ul>
<li>Dala和Triggs的SVM行人检测器</li>
<li>Felzenszwalb之后证明bootstrapping能够使得SVM在整个数据集上达到全局优化的效果。</li>
</ul>
<p>这些算法一般被认为是<code>hard negative mining</code>，经常用于训练目标检测的SVM。Bootstrapping还成功的用于许多其他的学习模型，包括浅层神经网络、boosted决策树。甚至目前先进的基于深度卷积神经网络[^19,20]，如R-CNN[^15]，SPPnet[^16]，仍然采用hard negative mining训练的SVM。</p>
<p>然而目前先进的目标检测器如Fast R-CNN以及变体没有使用Bootstrapping。<strong>可能的原因是将目前的技术转为纯在线学习算法是困难的，尤其是在几百万个样本中进行SGD训练深度卷积网络的情况下。</strong> Bootstrapping需要依赖上述交换模版：（a）一会固定模型寻找新样本；（b）一会固定样本训练模型。SGD训练深度卷积网络需要几十万次SGD，因此在几个迭代中固定模型也会极大的减慢处理过程。因此，需要的是一种在线的困难样本选择方法。</p>
<p>本文，我们提出一个新型的Bootstrapping技术，称为OHEM——在线困难样本挖掘，用于训练基于深度卷积网络的检测器。该算法是对SGD的简单修改，其中训练样本根据正在计算的每个样本当前的损失进行不均匀、非平稳分布的采样。该方法利用了检测问题的优势，该问题中每个SGD mini-batch虽然仅包含1或2个图像，但是有上千个候选样本。<strong>对候选样本进行降采样，考虑样本多样性和高损失值。</strong>由于仅使用候选区的一个小子集，因此梯度计算仍然是高效的。我们将OHEM应用与Fast R-CNN方法，显示出3个优点：</p>
<ul>
<li>去除了一些常用的启发式线索和超参</li>
<li>在mAP上显示出稳定、显著的提升</li>
<li>在更大更困难的数据集上效率更高，如MS COCO数据集</li>
</ul>
<p>除此之外，OHEM与目前的一些方法互补，如多尺度测试[^16]，迭代bounding-box回归[^13]。与上面这些技巧结合，OHEM在VOC 2007和2012的结果分别为78.9%，76.3%。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>目标检测是计算机视觉领域最古老也是最基础的问题之一。数据集的Bootstrapping，目前称为hard negative mining，应用于很多目标检测器的训练中。这些方法用深度卷积神经网络提取特这后，大部分使用SVM作为检测器对样本打分排序。其中的例外是Fast R-CNN和Faster R-CNN。由于这些方法步使用SVM，使用纯粹的在线SGD训练，因此目前的困难样本挖掘方法不能直接使用。本文使用在线困难样本挖掘算法解决这个问题，提高效率和进度。我们简单回顾下困难样本挖掘、基于卷积网络的目标检测器，以及他们之间的合作——使用困难样本挖掘训练深度网络。</p>
<h3 id="困难样本挖掘HEM"><a href="#困难样本挖掘HEM" class="headerlink" title="困难样本挖掘HEM"></a>困难样本挖掘HEM</h3><p>目前主要有两种HEM。</p>
<p>第一种主要用于优化SVM。方法就是先在一个样本集上训练SVM至收敛，然后根据一定的规则去除一些样本，新增一些样本。去除的是在当前模型下离分类边界很远的那些“容易”分类的样本。反之，将违反当前边界的样本，即错分的样本加入训练集中。应用上述规则实现全局优化的SVM。重要的是通常工作集是整个训练样本集的一个小子集。</p>
<p>第二种一般用于非SVM方法，例如浅神经网络和boosted决策树。该算法首先包含正样本和随机负样本。然后训练模型至收敛。之后在一个更大的数据集上获取虚警。将虚警加入训练集中再次进行训练。这个流程一般只迭代一次，并且没有任何收敛性证明。</p>
<h3 id="基于卷积网络的目标检测"><a href="#基于卷积网络的目标检测" class="headerlink" title="基于卷积网络的目标检测"></a>基于卷积网络的目标检测</h3><p>近三年，目标检测领域发展迅速。这种进步主要是由于ImageNet出现后对深度卷积网络的训练。R-CNN和Over Feat[^26]检测器在PASCAL VOC和ImageNet detection上的结果改善显著。OverFeat基于滑窗法，R-CNN基于区域推荐法——Selective search算法。由于R-CNN基于卷积网络的处理流程更快，SPPnet[^16]，MR-CNN[^13]，Fast R-CNN[^14]，因此我们的工作主要在此模型上展开。</p>
<h3 id="深度学习中的困难样本选择"><a href="#深度学习中的困难样本选择" class="headerlink" title="深度学习中的困难样本选择"></a>深度学习中的困难样本选择</h3><p>目前有也有一些工作研究在训练深度网络中挑选困难样本[^22][^27][^33]。与我们的方法类似，这些方法也都是具有当前数据点的损失值来挑选样本。</p>
<ul>
<li>论文[^27]从一个大型数据集中随机样本的loss独立的选择困难正样本和负样本，用于学习图像描述符。</li>
<li>论文[^33]对于给定的一对正样本块，使用triplet loss在数据集中寻找困难负样本块</li>
<li>与我们的方法类似，论文[^22]研究对mini-batch SGD的在线困难样本选择方法。他们的方法也基于loss，但是主要面向分类问题。<em>而我们的关注于基于区域推荐的目标检测器的在线困难样本选择策略。</em>（这个理由有点牵强）</li>
</ul>
<h2 id="3-Fast-R-CNN简介"><a href="#3-Fast-R-CNN简介" class="headerlink" title="3. Fast R-CNN简介"></a>3. Fast R-CNN简介</h2><p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Training">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Training</a> Region-based Object Detectors with Online Hard Example Mining&#x2F;f1.png)</p>
<p>我们首先总结下Fast R-CNN（FRCN）的框架。FRCN是输入一幅图像和一系列ROI。FRCN网络本身分为两个部分：一个是卷积网络层，包含卷积和池化层；另一个是ROI网络，包含ROI池化层，以及全连接层和两个损失层。如图1。</p>
<p>卷积网络将图像转换为卷积特征图，然后对于每个目标推荐，ROI池化层，将ROI映射到卷积特征图上，然后提取固定长度的特征向量（<em>通过插值或超采样</em>）。每个特征向量输入给fc层，最后输出两个结果：（1）softmax，所有类别和背景的概率分布；（2）bounding-box的重新定位的位置回归。</p>
<p>选择FRCN作为我们的基础目标检测器有以下原因：</p>
<ul>
<li>一个快速的端到端的系统</li>
<li>conv和ROI两个网络层是设计也用于其他的检测器SPPnet和MR-CNN，因此我们提出的方法更加有广泛性</li>
<li>基本设置相似，FRCN也允许训练整个卷积网络，相反SPPnet和MR-CNN会固定卷积层。</li>
<li>SPPnet和MR-CNN都需要缓存ROI网络的特征用于训练一个分离的SVM分类器（用于困难负样本挖掘）。FRCN使用ROI网络本身来训练期望的分类器。后面会证明在统一系统中使用SVM分类器是没必要的。</li>
</ul>
<h3 id="3-1-Training"><a href="#3-1-Training" class="headerlink" title="3.1 Training"></a>3.1 Training</h3><p>类似大部分深度网络，FRCN也是使用随机梯度下降进行训练。每个ROI的loss是所有分类的log loss的综合，目的是鼓励准确预测目标类别；另一个位置loss孤立预测精确的bounding box位置。</p>
<p>为了分享ROI之间的卷积计算，SGD的mini-batch创建为层级结构。对于每个mini-batch，先从数据集中选择N个图像，每个图像选择B&#x2F;N个ROI。设置N&#x3D;2，B&#x3D;128。ROI的采样设计到一些启发式方法，后面我们进行简要介绍。<strong>本文的一个贡献是去除一些启发式方法和他们的超参数。</strong></p>
<h4 id="前景ROI"><a href="#前景ROI" class="headerlink" title="前景ROI"></a>前景ROI</h4><p>例如一个ROI被标记为前景fg，其与gt的IoU应该≥0.5。这是一种标准的设计选择，是PASCAL VOC目标检测基准数据集的评估标准。相同的标准也用在R-CNN，SPPnet，MR-CNN等模型中SVM的困难样本挖掘。我们也使用相同的标准。</p>
<h4 id="背景ROI"><a href="#背景ROI" class="headerlink" title="背景ROI"></a>背景ROI</h4><p>一个区域被标记为bg，其最大IoU的区间在[bg_lo, 0.5)之间。在FRCN和SPPnet中使用bg_lo&#x3D;0.1，文献[^14]使用更粗略的困难负样本。<strong>这里假设区域与gt有一些重叠的区域更有可能混淆，称为困难样本。</strong>5.4节显示启发式方法能够帮助收敛、提高精度，这是一种次优方法，<strong>因为忽略一些不经常出现但是重要困难的背景区域。我们的方法移除了bg_lo的阈值。</strong></p>
<h4 id="平衡fg-bg的ROI"><a href="#平衡fg-bg的ROI" class="headerlink" title="平衡fg-bg的ROI"></a>平衡fg-bg的ROI</h4><p>为了处理section1中的数据不平衡性，论文[^14]设计启发式方法平衡每个mini-batch的fg-bg比例，目标比例为1：3，通过随机降采样背景块，因此确保mini-batch中25%的样本是fg。我们发现这是一个训练FRCN的重要设计。去除这个比例（随机采样ROI）或者增加比例，会降低检测精度mAP，3个点。基于我们的方法，可以去除这个比例参数，并且不造成影响。</p>
<h2 id="4-我们的方法"><a href="#4-我们的方法" class="headerlink" title="4. 我们的方法"></a>4. 我们的方法</h2><p>我们提出简单、有效的在线困难样本挖掘算法用于训练Fast R-CNN（或Fast R-CNN类似的目标检测器）。我们认为目前的方法构造mini-batch用于SGD是低效、次优的。我们方法能够更好的训练（训练误差更低）更高的检测性能（mAP）。</p>
<h3 id="4-1-在线困难样本挖掘"><a href="#4-1-在线困难样本挖掘" class="headerlink" title="4.1 在线困难样本挖掘"></a>4.1 在线困难样本挖掘</h3><p>基于SVM的目标检测器中，例如R-CNN或SPPnet中训练的SVM，其步骤是：</p>
<ul>
<li>激活一定数量的图片（j经常为10多，100多）使得训练集达到一定阈值</li>
<li>在该激活的训练集上训练SVM至收敛。</li>
<li>重复上述过程，直到激活训练集包含所有支持向量</li>
</ul>
<p>将类似的策略应用于FRCN后会减慢训练速度，因为在挑选样本时不能更新模型。</p>
<p>我们发现，两个步骤可以结合FRCN的在线SGD进行训练。主要原因是虽然每个SGD迭代只有少量图像，但是每个图像包含上千个ROI样本，从中我们可以挑选困难样本，而不是启发式筛选子集。这个策略适用于SGD中固定模型用于mini-batch的交换模版，且只固定模型一次，在一个mini-batch中。因此模型更新速度跟SGD方法一样快，所以学习没有延迟。</p>
<ul>
<li>输入图像，在SGD的第t次迭代，计算卷积特征图</li>
<li>ROI网络计算所有输入的ROI，ROI池化，几个全连接层，计算loss</li>
<li>loss表示对当前ROI的性能</li>
<li>对ROI以loss排序选择前B&#x2F;N个ROI表示当前检测最差的ROI</li>
<li>由于只选择一小部分ROI更新模型，反向传播更加快速</li>
</ul>
<p>但是，这里有个小问题：区域重叠的位置相关ROI区域具有相关的损失。并且重叠的ROI可能投影到相同的卷积特征区域，因此导致对同一区域的loss加倍考虑。为了处理这些冗余和相关的区域，我们使用标准的非极大值抑制NMS删除重复数据。对于给定的ROI列表和他们的损失，NMS的工作是选择高loss的ROI，删除低loss的ROI和具有高度重叠的区域。我们选择的IoU阈值为0.7。</p>
<p>网盘， 发现上面描述的过程不需要fg-bg比例参数用于数据平衡。如果任何类被忽略，那他们的loss会增加，然后就会有更高的可能性对其采样。</p>
<h3 id="4-2-实现细节"><a href="#4-2-实现细节" class="headerlink" title="4.2 实现细节"></a>4.2 实现细节</h3><p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Training">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Training</a> Region-based Object Detectors with Online Hard Example Mining&#x2F;f2.png)</p>
<p>有很多方法可以实现OHEM，每种都有不同的权衡。一个明显的方法是修改loss层来做困难样本的选择。loss层可以计算所有ROI的loss，然后排序，然后选择困难的ROI，然后将所有非困难ROI的loss置0。这种方法虽然直接，但是并不高效，因为ROI网络仍然会分配内存给所有的ROI，进行反向传播，即使大部分ROI的loss是0，因此没有梯度更新。</p>
<p>为了解决这个，我们提出的架构如图2所示。我们实现维护两个相同的ROI网络，其中一个是只读的。这意味着，只读的ROI网络只在前向传播的时候分配内存，而一般的网络是前向&#x2F;反向都分配内存。对于一个SGD迭代，给定一个卷积特征图，只读ROI网络前向计算所有ROI的损失值，然后困难ROI采样模块使用如4.1描述的方法选择困难样本($R_{hard-set}$)，其输入给一般的ROI网络这个网络分别计算前向和反向，但是仅在$R_{hard-set}$上累计梯度并传递给卷积网络。实际中，我们使用N个图片的所有ROI作为R，因此只读网络的的batch尺寸是$|R|$，而常规网络是$|B|$。</p>
<p>我们使用Caffe实现上述网络，N次前向反向梯度累积，在一个image mini-batch。对于FRCN，我们使用N&#x3D;2，B&#x3D;128。使用上述方法后速度提升两倍。除非特殊说明，否则本文使用上述方法。</p>
<h2 id="5-分析在线困难样本挖掘"><a href="#5-分析在线困难样本挖掘" class="headerlink" title="5. 分析在线困难样本挖掘"></a>5. 分析在线困难样本挖掘</h2><p>这一届比较采用OHEM的FRCN的模型和一般的启发式样本算则方法。我们还比较了一些更抵消的方法，即使用所有ROI，而不是仅仅B个最困难的样本。</p>
<h3 id="5-1-实验设置"><a href="#5-1-实验设置" class="headerlink" title="5.1 实验设置"></a>5.1 实验设置</h3><p>我们主要对两个标准卷积网络架构实验：一个是VGG_CNN_M_1024(VGGM)，是AlexNet的一种扩展版，一个是VGG16。所有实验都在VOC 07数据集上进行。训练所有的SGD对80k个mini-batch迭代，初始化学习率为0.001，每30k次迭代衰减0.1。表1中的1，2行是我们的标准做法，比原文的结果稍微提高。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Training">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Training</a> Region-based Object Detectors with Online Hard Example Mining&#x2F;t1.png)</p>
<h3 id="5-2-OHEM与启发式方法"><a href="#5-2-OHEM与启发式方法" class="headerlink" title="5.2 OHEM与启发式方法"></a>5.2 OHEM与启发式方法</h3><p>标准的FRCN，如表1所述，1-2行。为了测试启发式方法的重要性，去除bg_lo，之后VGGM的mAP掉了2.4，但是VGG16基本一致。与OHEM训练FRCN相比，有2.4mAP的提升。这证明启发式方法是次优的，我们的OHEM是更有效的方法。</p>
<h3 id="5-3-鲁棒梯度估计"><a href="#5-3-鲁棒梯度估计" class="headerlink" title="5.3 鲁棒梯度估计"></a>5.3 鲁棒梯度估计</h3><p>采用两个图片的ROI进行优化，这是因为单一图像由于ROI具有较高的相似性，可能会导致不稳定的梯度，从而减慢收敛的过程。这个对于FRCN可能不是一个问题，但是对我们的方法可能是需要考虑的，因为我们对同一幅图像中的高loss进行采样，这些ROI可能具有较高的相似性。为了解决这个问题，我们实验了当N&#x3D;1时，增加ROI相关性，FRCN会产生1个点的下降，但是我们的方法没有下降，说明更加具有鲁棒性。</p>
<h3 id="5-4-为什么只是困难样本，当你使用所有样本的时候呢？"><a href="#5-4-为什么只是困难样本，当你使用所有样本的时候呢？" class="headerlink" title="5.4 为什么只是困难样本，当你使用所有样本的时候呢？"></a>5.4 为什么只是困难样本，当你使用所有样本的时候呢？</h3><p>在线困难样本挖掘有一个基本假设，考虑所有ROI是必要的，然后选择困难的样本进行训练。但是如果我用所有ROI进行训练呢？容易样本具有低loss，不会对gradient具有太多贡献，本身训练的过程就会关注困难的样本。为了比较这个，我们使用了大尺寸的mini-size，B&#x3D;2048，由于使用了大数据量，调整学习率是必要。虽然结果不基准要高1个点，但是我们的方法更加好。</p>
<h3 id="5-5-更好的优化"><a href="#5-5-更好的优化" class="headerlink" title="5.5 更好的优化"></a>5.5 更好的优化</h3><p>最后，我们分析上述不同FRCN训练方法的loss的变化。我们每20k次优化计算一次所有ROI的平均loss。图3显示了结果：</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Training">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Training</a> Region-based Object Detectors with Online Hard Example Mining&#x2F;f3.png)</p>
<h3 id="5-6-计算成本"><a href="#5-6-计算成本" class="headerlink" title="5.6 计算成本"></a>5.6 计算成本</h3><p>OHEM整机了合理的计算量和内存消耗。</p>
<p>![](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Training">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Training</a> Region-based Object Detectors with Online Hard Example Mining&#x2F;t2.png)</p>
<h2 id="7-增加的方法"><a href="#7-增加的方法" class="headerlink" title="7 增加的方法"></a>7 增加的方法</h2><h4 id="Multi-scale（M）多尺度"><a href="#Multi-scale（M）多尺度" class="headerlink" title="Multi-scale（M）多尺度"></a>Multi-scale（M）多尺度</h4><p>我们采用多尺度策略。尺度定义为图像的短边长s，训练期间随机选择尺度，测试时测试所有尺度。对于VGG16，训练时s&#x3D;{480,576,688,864,900}，测试时s&#x3D;{480,576,688,864,1000}</p>
<h4 id="迭代BB回归（B）"><a href="#迭代BB回归（B）" class="headerlink" title="迭代BB回归（B）"></a>迭代BB回归（B）</h4><p>论文[^13]中采用迭代定位和bbox投票方案。网络评估每个ROI得到一个得分和重新定位的box R1。高得分的R1进行再次评价和定位得到R2。R1和R1组成最终的RF用于后处理，RF_NMS得到NMS后的RF，IoU为0.3。</p>
<p><strong>BibTex</strong></p>
<figure class="highlight latex"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;</span><br><span class="line">Shrivastava2016,</span><br><span class="line">author = &#123;Shrivastava, Abhinav and Gupta, Abhinav and Girshick, Ross&#125;,</span><br><span class="line">booktitle = &#123;2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&#125;,</span><br><span class="line">doi = &#123;10.1109/CVPR.2016.89&#125;,</span><br><span class="line">month = &#123;jun&#125;,</span><br><span class="line">pages = &#123;761--769&#125;,</span><br><span class="line">publisher = &#123;IEEE&#125;,</span><br><span class="line">title = &#123;&#123;Training Region-Based Object Detectors with Online Hard Example Mining&#125;&#125;,</span><br><span class="line">year = &#123;2016&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Video Object Detection</tag>
        <tag>OHEM</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测 Object Detection</title>
    <url>/Paper-Archives-%E8%AE%BA%E6%96%87%E9%9B%86/Object-Detection/</url>
    <content><![CDATA[<p>本文收集目标检测相关论文，会在之后持续更新。</p>
<h2 id="Leaderboard"><a href="#Leaderboard" class="headerlink" title="Leaderboard"></a>Leaderboard</h2><h3 id="Object-Detector"><a href="#Object-Detector" class="headerlink" title="Object Detector"></a>Object Detector</h3><table>
<thead>
<tr>
<th>Detector</th>
<th>Backbone</th>
<th>VOC</th>
<th>COCO</th>
<th>Paper</th>
</tr>
</thead>
</table>
<h3 id="Pedestrain-Detector"><a href="#Pedestrain-Detector" class="headerlink" title="Pedestrain Detector"></a>Pedestrain Detector</h3><table>
<thead>
<tr>
<th>Detector</th>
<th>Backbone</th>
<th>Caltech(MR-2)</th>
<th>Caltech(MR-4)</th>
<th>Paper</th>
</tr>
</thead>
</table>
<span id="more"></span>

<hr>
<h2 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h2><hr>
<h3 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h3><h4 id="RoI"><a href="#RoI" class="headerlink" title="RoI"></a>RoI</h4><h5 id="RepPoints-Point-Set-Representation-for-Object-Detection"><a href="#RepPoints-Point-Set-Representation-for-Object-Detection" class="headerlink" title="RepPoints: Point Set Representation for Object Detection"></a>RepPoints: Point Set Representation for Object Detection</h5><ul>
<li>arXiv: <a href="http://arxiv.org/abs/1904.11490">http://arxiv.org/abs/1904.11490</a></li>
</ul>
<h5 id="CornerNet-Lite-Efficient-Keypoint-Based-Object-Detection"><a href="#CornerNet-Lite-Efficient-Keypoint-Based-Object-Detection" class="headerlink" title="CornerNet-Lite: Efficient Keypoint Based Object Detection"></a>CornerNet-Lite: Efficient Keypoint Based Object Detection</h5><ul>
<li>arXiv: <a href="https://arxiv.org/abs/1904.08900">https://arxiv.org/abs/1904.08900</a></li>
<li>github: <a href="https://github.com/princeton-vl/CornerNet-Lite">https://github.com/princeton-vl/CornerNet-Lite</a></li>
</ul>
<h5 id="Objects-as-Points"><a href="#Objects-as-Points" class="headerlink" title="Objects as Points"></a>Objects as Points</h5><ul>
<li>arXiv: <a href="https://arxiv.org/pdf/1904.07850.pdf">https://arxiv.org/pdf/1904.07850.pdf</a></li>
<li>github: <a href="https://github.com/xingyizhou/CenterNet">https://github.com/xingyizhou/CenterNet</a></li>
</ul>
<h5 id="DuBox-No-Prior-Box-Objection-Detection-via-Residual-Dual-Scale-Detectors"><a href="#DuBox-No-Prior-Box-Objection-Detection-via-Residual-Dual-Scale-Detectors" class="headerlink" title="DuBox: No-Prior Box Objection Detection via Residual Dual Scale Detectors"></a>DuBox: No-Prior Box Objection Detection via Residual Dual Scale Detectors</h5><ul>
<li>arXiv: <a href="http://arxiv.org/abs/1904.06883">http://arxiv.org/abs/1904.06883</a></li>
<li>note: <a href="/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-DuBox-No-Prior-Box-Objection-Detection-via-Residual-Dual-Scale-Detectors/" title="Note-DuBox: No-Prior Box Objection Detection via Residual Dual Scale Detectors">Note-DuBox: No-Prior Box Objection Detection via Residual Dual Scale Detectors</a></li>
</ul>
<h5 id="FoveaBox-Beyond-Anchor-based-Object-Detector"><a href="#FoveaBox-Beyond-Anchor-based-Object-Detector" class="headerlink" title="FoveaBox: Beyond Anchor-based Object Detector"></a>FoveaBox: Beyond Anchor-based Object Detector</h5><ul>
<li>arXiv: <a href="https://arxiv.org/abs/1904.03797">https://arxiv.org/abs/1904.03797</a></li>
</ul>
<h5 id="High-level-Semantic-Feature-Detection-A-New-Perspective-for-Pedestrian-Detection"><a href="#High-level-Semantic-Feature-Detection-A-New-Perspective-for-Pedestrian-Detection" class="headerlink" title="High-level Semantic Feature Detection:A New Perspective for Pedestrian Detection"></a>High-level Semantic Feature Detection:A New Perspective for Pedestrian Detection</h5><ul>
<li>arXiv: <a href="https://arxiv.org/abs/1904.02948">https://arxiv.org/abs/1904.02948</a></li>
<li>github: <a href="https://github.com/liuwei16/CSP">https://github.com/liuwei16/CSP</a></li>
<li>pub: CVPR 2019</li>
</ul>
<h5 id="FCOS-Fully-Convolutional-One-Stage-Object-Detection"><a href="#FCOS-Fully-Convolutional-One-Stage-Object-Detection" class="headerlink" title="FCOS: Fully Convolutional One-Stage Object Detection"></a>FCOS: Fully Convolutional One-Stage Object Detection</h5><ul>
<li>arXiv: <a href="https://arxiv.org/abs/1904.01355">https://arxiv.org/abs/1904.01355</a></li>
<li>github: <a href="https://github.com/tianzhi0549/FCOS">https://github.com/tianzhi0549/FCOS</a></li>
</ul>
<h5 id="Feature-Selective-Anchor-Free-Module-for-Single-Shot-Object-Detection"><a href="#Feature-Selective-Anchor-Free-Module-for-Single-Shot-Object-Detection" class="headerlink" title="Feature Selective Anchor-Free Module for Single-Shot Object Detection"></a>Feature Selective Anchor-Free Module for Single-Shot Object Detection</h5><ul>
<li>arXiv: <a href="http://arxiv.org/abs/1903.00621">http://arxiv.org/abs/1903.00621</a></li>
</ul>
<h5 id="Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points"><a href="#Bottom-up-Object-Detection-by-Grouping-Extreme-and-Center-Points" class="headerlink" title="Bottom-up Object Detection by Grouping Extreme and Center Points"></a>Bottom-up Object Detection by Grouping Extreme and Center Points</h5><ul>
<li>arXiv: <a href="https://arxiv.org/abs/1901.08043">https://arxiv.org/abs/1901.08043</a></li>
<li>github: <a href="https://github.com/xingyizhou/ExtremeNet">https://github.com/xingyizhou/ExtremeNet</a></li>
</ul>
<h5 id="Region-Proposal-by-Guided-Anchoring"><a href="#Region-Proposal-by-Guided-Anchoring" class="headerlink" title="Region Proposal by Guided Anchoring"></a>Region Proposal by Guided Anchoring</h5><ul>
<li>arXiv: <a href="http://arxiv.org/abs/1901.03278">http://arxiv.org/abs/1901.03278</a></li>
<li>pub: CVPR 2019</li>
</ul>
<h5 id="CornerNet-Detecting-Objects-as-Paired-Keypoints"><a href="#CornerNet-Detecting-Objects-as-Paired-Keypoints" class="headerlink" title="CornerNet: Detecting Objects as Paired Keypoints"></a>CornerNet: Detecting Objects as Paired Keypoints</h5><ul>
<li>arXiv: <a href="https://arxiv.org/abs/1808.01244">https://arxiv.org/abs/1808.01244</a></li>
<li>github: <a href="https://github.com/princeton-vl/CornerNet">https://github.com/princeton-vl/CornerNet</a></li>
</ul>
<h5 id="DeNet-Scalable-Real-Time-Object-Detection-With-Directed-Sparse-Sampling"><a href="#DeNet-Scalable-Real-Time-Object-Detection-With-Directed-Sparse-Sampling" class="headerlink" title="DeNet Scalable Real-Time Object Detection With Directed Sparse Sampling"></a>DeNet Scalable Real-Time Object Detection With Directed Sparse Sampling</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1703.10295">https://arxiv.org/abs/1703.10295</a></li>
<li>pub: ICCV 2017</li>
<li>[note](2017 - Tychsen-Smith, Petersson - DeNet Scalable Real-Time Object Detection With Directed Sparse Sampling.md)<a href="/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-DeNet-Scalable-Real-time-Object-Detection-with-Directed-Sparse-Sampling/" title="论文笔记《DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling》">论文笔记《DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling》</a></li>
</ul>
<h4 id="Feature"><a href="#Feature" class="headerlink" title="Feature"></a>Feature</h4><h5 id="Libra-R-CNN-Towards-Balanced-Learning-for-Object-Detection"><a href="#Libra-R-CNN-Towards-Balanced-Learning-for-Object-Detection" class="headerlink" title="Libra R-CNN: Towards Balanced Learning for Object Detection"></a>Libra R-CNN: Towards Balanced Learning for Object Detection</h5><ul>
<li>paper: <a href="https://arxiv.org/abs/1904.02701">https://arxiv.org/abs/1904.02701</a></li>
<li>code: <a href="https://github.com/open-mmlab/mmdetection">https://github.com/open-mmlab/mmdetection</a></li>
<li>pub: CVPR 2018</li>
</ul>
<h5 id="star-Multi-scale-Location-aware-Kernel-Representation-for-Object-Detection"><a href="#star-Multi-scale-Location-aware-Kernel-Representation-for-Object-Detection" class="headerlink" title="$\star$ Multi-scale Location-aware Kernel Representation for Object Detection"></a>$\star$ Multi-scale Location-aware Kernel Representation for Object Detection</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1804.00428">https://arxiv.org/abs/1804.00428</a></li>
<li>pub: CVPR 2018</li>
<li>github: <a href="https://github.com/Hwang64/MLKP">https://github.com/Hwang64/MLKP</a></li>
</ul>
<h4 id="Post-process"><a href="#Post-process" class="headerlink" title="Post process"></a>Post process</h4><h5 id="Softer-NMS-Rethinking-Bounding-Box-Regression-for-Accurate-Object-Detection"><a href="#Softer-NMS-Rethinking-Bounding-Box-Regression-for-Accurate-Object-Detection" class="headerlink" title="Softer-NMS: Rethinking Bounding Box Regression for Accurate Object Detection"></a>Softer-NMS: Rethinking Bounding Box Regression for Accurate Object Detection</h5><ul>
<li>arXiv: <a href="https://arxiv.org/abs/1809.08545">https://arxiv.org/abs/1809.08545</a></li>
<li>github: <a href="https://github.com/yihui-he/softer-NMS">https://github.com/yihui-he/softer-NMS</a></li>
</ul>
<h5 id="Soft-NMS-–-Improving-Object-Detection-With-One-Line-of-Code"><a href="#Soft-NMS-–-Improving-Object-Detection-With-One-Line-of-Code" class="headerlink" title="Soft-NMS – Improving Object Detection With One Line of Code"></a>Soft-NMS – Improving Object Detection With One Line of Code</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1704.04503">https://arxiv.org/abs/1704.04503</a></li>
<li>pub: ICCV 2017</li>
<li>github: <a href="https://github.com/bharatsingh430/soft-nms">https://github.com/bharatsingh430/soft-nms</a></li>
<li>[note](2017 - Bodla et al. - Soft-NMS – Improving Object Detection With One Line of Code.md) <a href="/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-Soft-NMS-%E2%80%93-Improving-Object-DetectionWith-One-Line-of-Code/" title="论文笔记《Soft-NMS – Improving Object Detection With One Line of Code》">论文笔记《Soft-NMS – Improving Object Detection With One Line of Code》</a></li>
</ul>
<hr>
<h3 id="Pedestrian-Detection"><a href="#Pedestrian-Detection" class="headerlink" title="Pedestrian Detection"></a>Pedestrian Detection</h3><h5 id="PCN-Part-and-Context-Information-for-Pedestrian-Detection-with-CNNs"><a href="#PCN-Part-and-Context-Information-for-Pedestrian-Detection-with-CNNs" class="headerlink" title="PCN: Part and Context Information for Pedestrian Detection with CNNs"></a>PCN: Part and Context Information for Pedestrian Detection with CNNs</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1804.04483">https://arxiv.org/abs/1804.04483</a></li>
<li>pub: BMVC 2017</li>
</ul>
<h5 id="Pedestrian-Synthesis-GAN-Generating-Pedestrian-Data-in-Real-Scene-and-Beyond"><a href="#Pedestrian-Synthesis-GAN-Generating-Pedestrian-Data-in-Real-Scene-and-Beyond" class="headerlink" title="Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and Beyond"></a>Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and Beyond</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1804.02047">https://arxiv.org/abs/1804.02047</a></li>
<li>github: <a href="https://github.com/yueruchen/Pedestrian-Synthesis-GAN">https://github.com/yueruchen/Pedestrian-Synthesis-GAN</a></li>
</ul>
<h5 id="Illumination-aware-Faster-R-CNN-for-Robust-Multispectral-Pedestrian-Detection"><a href="#Illumination-aware-Faster-R-CNN-for-Robust-Multispectral-Pedestrian-Detection" class="headerlink" title="Illumination-aware Faster R-CNN for Robust Multispectral Pedestrian Detection"></a>Illumination-aware Faster R-CNN for Robust Multispectral Pedestrian Detection</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1803.05347">https://arxiv.org/abs/1803.05347</a></li>
</ul>
<h5 id="Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd"><a href="#Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd" class="headerlink" title="Repulsion Loss: Detecting Pedestrians in a Crowd"></a>Repulsion Loss: Detecting Pedestrians in a Crowd</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1711.07752">https://arxiv.org/abs/1711.07752</a></li>
<li>pub: CVPR 2018</li>
</ul>
<h5 id="Illuminating-Pedestrians-via-Simultaneous-Detection-amp-Segmentation"><a href="#Illuminating-Pedestrians-via-Simultaneous-Detection-amp-Segmentation" class="headerlink" title="Illuminating Pedestrians via Simultaneous Detection &amp; Segmentation"></a>Illuminating Pedestrians via Simultaneous Detection &amp; Segmentation</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1706.08564">https://arxiv.org/abs/1706.08564</a></li>
<li>pub: ICCV 2017</li>
<li><a href="http://openaccess.thecvf.com/content_iccv_2017/html/Brazil_Illuminating_Pedestrians_via_ICCV_2017_paper.html">paper</a></li>
</ul>
<h5 id="What-Can-Help-Pedestrian-Detection"><a href="#What-Can-Help-Pedestrian-Detection" class="headerlink" title="What Can Help Pedestrian Detection?"></a>What Can Help Pedestrian Detection?</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1705.02757">https://arxiv.org/abs/1705.02757</a></li>
<li>pub: CVPR 2017</li>
</ul>
<h5 id="Expecting-the-Unexpected-Training-Detectors-for-Unusual-Pedestrians-with-Adversarial-Imposters"><a href="#Expecting-the-Unexpected-Training-Detectors-for-Unusual-Pedestrians-with-Adversarial-Imposters" class="headerlink" title="Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters"></a>Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1703.06283">https://arxiv.org/abs/1703.06283</a></li>
<li>pub: CVPR 2017</li>
</ul>
<h5 id="CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection"><a href="#CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection" class="headerlink" title="CityPersons: A Diverse Dataset for Pedestrian Detection"></a>CityPersons: A Diverse Dataset for Pedestrian Detection</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1702.05693">https://arxiv.org/abs/1702.05693</a></li>
<li>[note](2017 - Zhang, Benenson, Schiele - CityPersons A Diverse Dataset for Pedestrian Detection.md)<a href="/Note-%E7%AC%94%E8%AE%B0/Pedestrian-Detection/Note-CityPersons-A-Diverse-Dataset-for-Pedestrian-Detection/" title="论文笔记《CityPersons: A Diverse Dataset for Pedestrian Detection》">论文笔记《CityPersons: A Diverse Dataset for Pedestrian Detection》</a></li>
</ul>
<h5 id="Deep-Multi-Camera-People-Detection"><a href="#Deep-Multi-Camera-People-Detection" class="headerlink" title="Deep Multi-Camera People Detection"></a>Deep Multi-Camera People Detection</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1702.04593">https://arxiv.org/abs/1702.04593</a></li>
<li>pub: International Conference on Machine Learning and Applications</li>
</ul>
<h5 id="Multispectral-Deep-Neural-Networks-for-Pedestrian-Detection"><a href="#Multispectral-Deep-Neural-Networks-for-Pedestrian-Detection" class="headerlink" title="Multispectral Deep Neural Networks for Pedestrian Detection"></a>Multispectral Deep Neural Networks for Pedestrian Detection</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1611.02644">https://arxiv.org/abs/1611.02644</a></li>
<li>pub: BMVC 2016 oral</li>
</ul>
<h5 id="Fused-DNN-A-Deep-Neural-Network-Fusion-Approach-to-Fast-and-Robust-Pedestrian-Detection"><a href="#Fused-DNN-A-Deep-Neural-Network-Fusion-Approach-to-Fast-and-Robust-Pedestrian-Detection" class="headerlink" title="Fused DNN A Deep Neural Network Fusion Approach to Fast and Robust Pedestrian Detection"></a>Fused DNN A Deep Neural Network Fusion Approach to Fast and Robust Pedestrian Detection</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1610.03466">https://arxiv.org/abs/1610.03466</a></li>
<li>pub: WACV 2017</li>
<li>[note](2017 - Du et al. - Fused DNN A Deep Neural Network Fusion Approach to Fast and Robust Pedestrian Detection.md)</li>
</ul>
<h5 id="Is-Faster-R-CNN-Doing-Well-for-Pedestrian-Detection"><a href="#Is-Faster-R-CNN-Doing-Well-for-Pedestrian-Detection" class="headerlink" title="Is Faster R-CNN Doing Well for Pedestrian Detection"></a>Is Faster R-CNN Doing Well for Pedestrian Detection</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1607.07032">https://arxiv.org/abs/1607.07032</a></li>
<li>pub: ECCV 2016</li>
<li>[note](2016 - Zhang et al. - Is Faster R-CNN Doing Well for Pedestrian Detection.md)<a href="/Note-%E7%AC%94%E8%AE%B0/Pedestrian-Detection/Note-Is-Faster-R-CNN-Doing-Well-for-Pedestrian-Detection/" title="论文笔记《Is Faster R-CNN Doing Well for Pedestrian Detection?》">论文笔记《Is Faster R-CNN Doing Well for Pedestrian Detection?》</a></li>
</ul>
<hr>
<h3 id="Video-Object-Detection"><a href="#Video-Object-Detection" class="headerlink" title="Video Object Detection"></a>Video Object Detection</h3><h5 id="Learning-Correspondence-from-the-Cycle-Consistency-of-Time"><a href="#Learning-Correspondence-from-the-Cycle-Consistency-of-Time" class="headerlink" title="Learning Correspondence from the Cycle-Consistency of Time"></a>Learning Correspondence from the Cycle-Consistency of Time</h5><ul>
<li>pub: CVPR 2019 (oral)</li>
<li>github: <a href="https://github.com/xiaolonw/TimeCycle">https://github.com/xiaolonw/TimeCycle</a></li>
</ul>
<h5 id="Detect-and-Track-Efficient-Pose-Estimation-in-Videos"><a href="#Detect-and-Track-Efficient-Pose-Estimation-in-Videos" class="headerlink" title="Detect-and-Track: Efficient Pose Estimation in Videos"></a>Detect-and-Track: Efficient Pose Estimation in Videos</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1712.09184">https://arxiv.org/abs/1712.09184</a></li>
<li>pub: CVPR 2018</li>
<li>github: <a href="https://github.com/facebookresearch/DetectAndTrack">https://github.com/facebookresearch/DetectAndTrack</a></li>
<li>intro: 1st ICCV 2017 PoseTrack</li>
</ul>
<h5 id="Flow-Guided-Feature-Aggregation-for-Video-Object-Detection"><a href="#Flow-Guided-Feature-Aggregation-for-Video-Object-Detection" class="headerlink" title="Flow-Guided Feature Aggregation for Video Object Detection"></a>Flow-Guided Feature Aggregation for Video Object Detection</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1703.10025">https://arxiv.org/abs/1703.10025</a></li>
<li>pub: ICCV 2017</li>
<li>github: <a href="https://github.com/msracver/Flow-Guided-Feature-Aggregation">https://github.com/msracver/Flow-Guided-Feature-Aggregation</a></li>
<li>[note](2017 - Zhu et al. - Flow-Guided Feature Aggregation for Video Object Detection.md)<a href="/Note-%E7%AC%94%E8%AE%B0/Video-Object-Detection/Note-Deep-Feature-Flow-for-Video-Recognition/" title="论文笔记《Deep Feature Flow for Video Recognition》">论文笔记《Deep Feature Flow for Video Recognition》</a></li>
</ul>
<h5 id="Deep-Feature-Flow-for-Video-Recognition"><a href="#Deep-Feature-Flow-for-Video-Recognition" class="headerlink" title="Deep Feature Flow for Video Recognition"></a>Deep Feature Flow for Video Recognition</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1611.07715">https://arxiv.org/abs/1611.07715</a></li>
<li>pub: CVPR 2017</li>
<li>[note](2017 - Zhu et al. - Deep Feature Flow for Video Recognition.md)<a href="/Note-%E7%AC%94%E8%AE%B0/Video-Object-Detection/Note-Deep-Feature-Flow-for-Video-Recognition/" title="论文笔记《Deep Feature Flow for Video Recognition》">论文笔记《Deep Feature Flow for Video Recognition》</a></li>
</ul>
<hr>
<h3 id="Segmentation"><a href="#Segmentation" class="headerlink" title="Segmentation"></a>Segmentation</h3><h5 id="YOLACT-Real-time-Instance-Segmentation"><a href="#YOLACT-Real-time-Instance-Segmentation" class="headerlink" title="YOLACT: Real-time Instance Segmentation"></a>YOLACT: Real-time Instance Segmentation</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1904.02689">https://arxiv.org/abs/1904.02689</a></li>
<li>github: <a href="https://github.com/dbolya/yolact">https://github.com/dbolya/yolact</a></li>
</ul>
<h5 id="Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation"><a href="#Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation" class="headerlink" title="Rethinking Atrous Convolution for Semantic Image Segmentation"></a>Rethinking Atrous Convolution for Semantic Image Segmentation</h5><ul>
<li>arxiv: <a href="https://arxiv.org/abs/1706.05587">https://arxiv.org/abs/1706.05587</a></li>
<li>[note](2017 - Chen et al. - Rethinking Atrous Convolution for Semantic Image Segmentation.md)<a href="/Note-%E7%AC%94%E8%AE%B0/Semantic-Segmentation/Note-Rethinking-Atrous-Convolution-for-Semantic-Image-Segmentation/" title="论文笔记《Rethinking Atrous Convolution for Semantic Image Segmentation》">论文笔记《Rethinking Atrous Convolution for Semantic Image Segmentation》</a></li>
</ul>
]]></content>
      <categories>
        <category>Paper Archives 论文集</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow1.8+Ubuntu16.05+CUDA8.0+cuDNN+Anaconda2安装</title>
    <url>/Tutorial-%E6%95%99%E7%A8%8B/TensorFlow-on-Ubuntu16-04/</url>
    <content><![CDATA[<p>环境：</p>
<ul>
<li>Ubuntu 16.04 64bit</li>
<li>Anaconda2 4.4.11</li>
<li>CUDA 8.0</li>
<li>cuDNN 7.0.5</li>
<li>gcc 5.4.0</li>
</ul>
<p>本文记录我安装TensorFlow时遇到的问题。由于本人使用CUDA8.0，最新版的tensorflow需要CUDA9.0，因此本文介绍基于源代码安装tensorflow的步骤，如果你使用CUDA9.0请直接按照<a href="https://www.tensorflow.org/install/">官网安装教程</a>.</p>
<span id="more"></span>

<h3 id="1-前提配置"><a href="#1-前提配置" class="headerlink" title="1. 前提配置"></a>1. 前提配置</h3><h4 id="1-1-安装CUDA"><a href="#1-1-安装CUDA" class="headerlink" title="1.1 安装CUDA"></a>1.1 安装CUDA</h4><p>首先要仔细看官方文档<br><a href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#ubuntu-installation">http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#ubuntu-installation</a><br>里面关于安装前的准备，如硬件性能查询、依赖包安装、禁用服务等等这里不赘述</p>
<p>安装：显卡驱动、CUDA、Anaconda2</p>
<p>驱动和CUDA都建议用<code>.run</code>文件安装，下载地址：<br><code> http://www.nvidia.com/Download/index.aspx?lang=en-us</code><br><code> https://developer.nvidia.com/cuda-toolkit-archive</code></p>
<p>下载好之后先安装驱动</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ./NVIDIA-Linux-x86_64-390.25.run</span><br></pre></td></tr></table></figure>
<p>之后会有提示步骤，基本安装提示一致安装就行，不要选择abort等取消安装的按钮</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ./cuda_8.0.61_375.26_linux.run</span><br></pre></td></tr></table></figure>
<p>同样也是安装步骤执行，只是这里会提示你安装驱动，记得要取消</p>
<p>下载cudnn7.0 <code> https://developer.nvidia.com/cudnn</code><br>执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo tar -xzf cudnn-8.0-linux-x64-v7.tar -C /usr/local</span><br></pre></td></tr></table></figure>


<p>配置CUDA的环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export PATH=/usr/local/cuda-8.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64 $&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure>

<h4 id="1-2-安装Anaconda"><a href="#1-2-安装Anaconda" class="headerlink" title="1.2 安装Anaconda"></a>1.2 安装Anaconda</h4><p>从Anaconda官网下载<code>run</code>文件安装，python 2.7 64bit<br><code> https://www.anaconda.com/download/</code></p>
<h4 id="1-3-anaconda虚拟环境配置（可选）"><a href="#1-3-anaconda虚拟环境配置（可选）" class="headerlink" title="1.3 anaconda虚拟环境配置（可选）"></a>1.3 anaconda虚拟环境配置（可选）</h4><p>此时你可能需要一个anaconda的虚拟python环境，其优点是不与当前的python环境冲突，缺点是一些必须的包需要重新安装（也很快）。</p>
<p>虚拟环境配置如下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n tensorflow pip python=2.7</span><br></pre></td></tr></table></figure>

<p>上述我们创建了一个名字叫tensorflow的虚拟python环境，其包含了pip和python2.7，开启虚拟环境的方式为</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">xuzhewei@LabServer000:~$ conda activate tensorflow</span><br><span class="line">(tensorflow) xuzhewei@LabServer000:~$</span><br></pre></td></tr></table></figure>

<h3 id="2-从源码安装TensorFlow"><a href="#2-从源码安装TensorFlow" class="headerlink" title="2. 从源码安装TensorFlow"></a>2. 从源码安装TensorFlow</h3><p>为什么要从源码安装？因为tensorflow官方给的二进制安装包要求使用CUDA 9.0，服务器上其他框架有些并未适配该版本，因此需要将tensorflow在本地编译成whl后进行安装。</p>
<h4 id="2-1-安装Bazel"><a href="#2-1-安装Bazel" class="headerlink" title="2.1 安装Bazel"></a>2.1 安装Bazel</h4><p>bazel是用来构建pip的whl安装文件的，安装官网给出的步骤安装即可，没有碰到坑。</p>
<ol>
<li><p>安装依赖包</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install pkg-config zip g++ zlib1g-dev unzip python</span><br></pre></td></tr></table></figure>
</li>
<li><p>下载Bazel</p>
<p>地址：<a href="https://github.com/bazelbuild/bazel/releases/download/0.13.1/bazel-0.13.1-installer-linux-x86_64.sh"><strong>bazel-0.13.1-installer-linux-x86_64.sh</strong></a></p>
<p>注意上述版本只是我当前使用的，请根据具体情况修改</p>
</li>
<li><p>安装</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chmod +x bazel-0.13.1-installer-linux-x86_64.sh</span><br><span class="line">./bazel-0.13.1-installer-linux-x86_64.sh --user</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置环境变量</p>
<p>在<code>~/.bashrc</code>中加入以下代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export PATH=&quot;$PATH:$HOME/bin&quot;</span><br></pre></td></tr></table></figure>

<p>这个文件将会在你下次打开命令行时调用，你也可以直接在命令行运行，在当前环境增加该路径</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export PATH=&quot;$PATH:$HOME/bin&quot;</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="2-2-安装依赖"><a href="#2-2-安装依赖" class="headerlink" title="2.2 安装依赖"></a>2.2 安装依赖</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install python-numpy python-dev python-pip python-wheel</span><br></pre></td></tr></table></figure>

<p>安装libcupti</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install cuda-command-line-tools</span><br><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64</span><br></pre></td></tr></table></figure>

<p>上述<code>LD_LIBRARY_PATH</code>环境变量只在当前环境下有效，若要下次启动直接调用，请加入到<code>~/.bashrc</code>中</p>
<h4 id="2-3-下载tensorflow"><a href="#2-3-下载tensorflow" class="headerlink" title="2.3 下载tensorflow"></a>2.3 下载tensorflow</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/tensorflow/tensorflow</span><br></pre></td></tr></table></figure>

<h4 id="2-4-安装配置"><a href="#2-4-安装配置" class="headerlink" title="2.4 安装配置"></a>2.4 安装配置</h4><p>源代码里有个<code>configure</code>文件帮助进行安装配置，随着版本不同可以出现的配置条目不一样，但是关键需要如下几项：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Do you wish to use jemalloc as the malloc implementation? [Y/n]Y</span><br><span class="line">jemalloc enabled</span><br><span class="line">Do you wish to build TensorFlow with CUDA support? [y/N] Y</span><br><span class="line">CUDA support will be enabled for TensorFlow</span><br><span class="line">Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 8.0</span><br><span class="line">Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7</span><br></pre></td></tr></table></figure>

<p>其他问题要么都默认，要么都选N即可</p>
<h4 id="2-5-构建pip软件包"><a href="#2-5-构建pip软件包" class="headerlink" title="2.5 构建pip软件包"></a>2.5 构建pip软件包</h4><p><strong>关于 gcc 5 或更高版本的说明</strong>：TensorFlow 网站上提供的二进制 pip 软件包是使用 gcc 4 构建的，该编译器使用的是旧版的 ABI。为了使您的构建与旧版 ABI 兼容，您需要在 <code>bazel build</code> 命令中添加 <code>--cxxopt=&quot;-D_GLIBCXX_USE_CXX11_ABI=0&quot;</code>。具备 ABI 兼容性后，针对 TensorFlow pip 软件包构建的自定义操作就能继续在您已构建好的软件包中正常运行了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=&quot;-D_GLIBCXX_USE_CXX11_ABI=0&quot;</span><br></pre></td></tr></table></figure>

<p>完成后会创建一个<code>build_pip_package</code>的脚本，运行后会在指定文件夹里生成<code>.whl</code>文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg</span><br></pre></td></tr></table></figure>

<h4 id="2-6-安装tensorflow"><a href="#2-6-安装tensorflow" class="headerlink" title="2.6 安装tensorflow"></a>2.6 安装tensorflow</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install tensorflow-1.8.0-cp27-cp27mu-linux_x86_64.whl</span><br></pre></td></tr></table></figure>

<h3 id="3-验证"><a href="#3-验证" class="headerlink" title="3. 验证"></a>3. 验证</h3><p>打开python</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ python</span><br></pre></td></tr></table></figure>

<p>在python交互式shell</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">hello = tf.constant(&#x27;Hello, TensorFlow!&#x27;)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(hello))</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Hello, TensorFlow!</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Tutorial 教程</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记《Deep Feature Flow for Video Recognition》</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Video-Object-Detection/Note-Deep-Feature-Flow-for-Video-Recognition/</url>
    <content><![CDATA[<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Deep_Feature_Flow/1515674206560.png" alt="1515674206560"></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>深度卷积网络在许多特性识别任务上取得成果，然而在视频任务上单帧检测太慢，因此将先进的图像识别网络迁移到视频任务上意义重大。我们提出深度特征流（Deep Feature Flow）框架用于快速精确的视频识别。该框架仅在稀疏关键帧上运行卷积网络的子网络，并通过流场将特征传递到其他帧，由于流场计算相对较快因此速度得到显著提升。端到端训练整体框架，识别精度显著提升。深度特征流框架灵活且通用，该方法在两个视频数据集上验证。</p>
<ul>
<li>MSA</li>
<li>视频识别</li>
<li>Deep Feature Flow</li>
</ul>
<span id="more"></span>

<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>随着深度神经网络在图像识别任务上的成功，识别任务也从图像拓展到了视频，例如语义分割Cityspaces数据集和物体检测ImageNet VID数据集。对于重要场景，快速精确的视频识别非常重要，例如自动驾驶、视频监控。然后采用当前图像识别网络对视频单帧进行识别，对于大部分应用计算量都无法承担。</p>
<p>一般认为图像内容在视频上的变化是缓慢的，尤其是高级语义[^45][^51][^21]。因此在特征学习时采用means of regularization，认为视频是一种无监督的数据源。并且，数据冗余和连续性也可以用于减少计算开销。但是相关的研究在视频识别的CNN上却较少。</p>
<p>现代CNN具有相同的架构，大部分层为卷积计算，卷积特征与原图具有空间对应性，因此有机会通过空间扭曲将特征传播到邻近帧。</p>
<p>本文的工作中，我们提出深度特征流（Deep Feature Flow）框架用于快速精确的视频识别，通过流场将关键帧的深度特征传播到其他帧。如图1，特征图对汽车和行人有响应，且邻近两帧相似，通过传播后特征也与原始的特征相似。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Deep_Feature_Flow/1515485794145.png" alt="1515485794145"></p>
<p>一般，流场估计和特征传播比卷积特征计算快得多。因此能够避免计算瓶颈，速度得到显著提升。整个系统通过端到端训练，显著提升识别精度。</p>
<p>总结，深度特征流是一个用于视频识别的快速、精确、同样的端到端框架。其采目前先进的图像识别网络。据我们所知，这是第一次在深度学习框架中联合训练流和视频识别任务。后续的实验证实了该框架在视频物体检测和语义分割任务上的有效性。与单帧方法比，实现了巨大的提升（10X），精度稍有损失。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>据我们所知，我们的工作很特别没有相似的工作进行直接比较。但是，在部分内容有一些相关工作如下所示。</p>
<ul>
<li>图像识别：目标检测领域代表方法是基于区域方法（R-CNN, Fast R-CNN, SPP-Net, Faster R-CNN, R-FCN），语义分割领域代表方法是全卷积神经网络（FCN，[^4], [^50] ）。</li>
<li>网络加速：矩阵分解，量化权重。</li>
<li>光流：相关方法研究主要进行了10多年，且光流主要针对小偏移。目前一些针对大偏移并组合匹配的方法（DeepFlow和EpicFlow），但是都是手工设计。最近，深度学校和语义信息用于光流。FlowNet第一次使用CNN估计运动。在文献[^32]网络结构简化为空间金字塔网络。另一些工作尝试开发语义分割信息帮助光流估计[^37][^1][^19] ，即根据区域的类别对运动流程进行特定的约束。光流信息可用于如姿势估计等视觉任务。本工作开发光流用于加快一般视频识别任务。</li>
<li>开发时序信息：T-CNN在视频的tubelets中融合时序和上下文信息。密集3D CRF提出大范围的时空归一化进行语义视频分割。STFCN[^10]考虑时空FCN，用于视频语义分割。这些工作能够提升精度但是也极大的增加计算开销。然而，我们的方法旨视频上通过时序信息减少计算量。</li>
<li>缓慢特征分析：高级语义特征一般比低级语义特征的变化更慢，因此在连续视频帧上，深度特征变化非常缓慢且连续。这个发现已经被用于视频特征学习的调整中[^45][^21][^51][^49][^40] 。我们推测，我们的方法也能从这个特性上受益。</li>
<li>Clockwork卷积网络[^38] 该工作与我们的工作最接近，其对于特征视频帧抑制某些网络层，并实验之前的特征。但是我们的方法更简单且有效。从速度上该方法仅减少了一些帧的一些层的计算（1&#x2F;3 or 2&#x2F;3）。而我们工作是节省大部分帧的大部分层的计算。<br>对于准确性，Clockwork没有考虑多帧和简单复制特征的关系，并且仅对一个现成的网络进行计算编排，没有fine-tuning或再训练。其一点速度的提升都带来较大的精度下降。我们的工作重新端到端训练了一个考虑运动的两帧网络，并且在速度提升3倍的情况下仅带来很小的精度下降。<br>Clockwork仅应用于FCN的语义分割，我们的工作将一般的图像识别网络迁移到视频领域。</li>
</ul>
<h2 id="3-Deep-Feature-Flow"><a href="#3-Deep-Feature-Flow" class="headerlink" title="3. Deep Feature Flow"></a>3. Deep Feature Flow</h2><p>表1总结了本文的符号，图2为方法示意图。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Deep_Feature_Flow/1515659482706.png" alt="1515659482706"></p>
<h3 id="深度特征流DFF推理（Deep-Feature-Flow-Inference）"><a href="#深度特征流DFF推理（Deep-Feature-Flow-Inference）" class="headerlink" title="深度特征流DFF推理（Deep Feature Flow Inference）"></a>深度特征流DFF推理（Deep Feature Flow Inference）</h3><p>对于给定的前馈神经网络$N$ ，$I$为图像输入，结果$y&#x3D;N(I)$，我们将$N$分解为两个连续的子网络。第一个子网络为$N_{feat}$特征子网络，该网络是全卷积且输出一些中间特征图$f&#x3D;N_{feat}(I)$。第二个子网络为任务网络，对于特定任务有特征的结构，并基于上述特征图$y&#x3D;N_{task}(f)$。</p>
<p>连续的视频帧具有高度的相似性，且越深的特征层相似性越高，我们通过该相似性减少计算开销。具体的，$N_{feat}$仅在特定稀疏关键帧上计算，非关键帧$I_i$的特征通过其之前的关键帧传播得到。</p>
<p>$M _ {i \rightarrow k}$ 是一个二维的流场，其通过一个流估计算法 $F$ 得到。然后通过双线性缩放到与特征图相同的大小，用于传播。当前帧i的位置p通过流场得到其与关键帧k的位置偏差 $\delta p&#x3D;M _ {i \rightarrow k}(p)$ 。</p>
<p>因此特征扭曲可以通过双线性差值得到：</p>
<p>$$ f_i^c(p) &#x3D; \sum_q G(q,p+\delta p)f_k^c(q) $$</p>
<p>c表示特征通道，q是特征图的所有空间位置，$G(·,·)$为双线性差值核，其为二维，因此将其分解为两个一维核：</p>
<p>$$ G(q,p+\delta p)&#x3D;g(q_x,p_x+\delta p_x)\cdot g(q_y, p_y+\delta p_y) $$</p>
<p>$g(a,b)&#x3D;max(0,1-|a-b|)$。</p>
<p>空间扭曲可能由于流估计错误导致不准确。为了更好估计特征，其强度通过尺度场（scale field）进行调节。尺度场通过尺度函数$S$得到$S_{i\rightarrow k}&#x3D;S(I_k,I_i)$。最后，特征传播方程定义为：</p>
<p>$f_i&#x3D;W(f_k,M_{i\rightarrow k},S_{i \rightarrow k}) （3）$</p>
<p>该函数对所有位置、所有通道的特征进行处理，并对特征乘以尺度因子。</p>
<p>该视频识别算法被称为深度特征流，如算法1所示。$F$流场函数是手工设计的低级别流，如SIFT-Flow，不需要训练，马上可用。尺度函数S的每个位置为1。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Deep_Feature_Flow/1515663944929.png" alt="1515663944929"></p>
<h3 id="DFF训练"><a href="#DFF训练" class="headerlink" title="DFF训练"></a>DFF训练</h3><p>流函数一般用于低级图像像素的相关性，计算快，但是对于识别任务不够准确，并且高级特征的改变比像素小。因此为了对此建模，我们也使用一个CNN来估计流场和尺度场，所有的组件都能够端到端训练。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Deep_Feature_Flow/1515674206560.png" alt="1515674206560"></p>
<p>架构如图2（b）所示。使用SGD训练，在每个mini-batch，随机选择一对邻近帧，${I _ k,I _ i},0\le i-k \le 9$。前向传播时，特征网络$N _ {feat}$对关键帧图像$I _ k$计算特征图$f _ k$，之后流网络$F$计算流场和尺度场。当$i&gt;k$时，根据式（3）计算特征$f _ i$，否则不传播。最后，任务网络$N _ {task}$基于特征图生成结果，并产生损失值。反向传播损失到所以组件。可以发现，当$i&#x3D;k$时即退化为每帧训练。</p>
<p>流网络比特征网络快，在Fly Chair dataset[^9]上进行预训练。通过最后一层卷积适当的增加通道数，我们在网络最后增加了一个尺度函数输出。尺度函数都使用（权重0，偏置1进行初始化）。流网络然后按照图2（b）fine-tuned。</p>
<p>特征传播函数是无参且可微的，我们计算特征$f _ i$的导数，对应于$f_k$，尺度场$S _ {i\rightarrow j}$，流场$M _ {i\rightarrow j}$。前两个使用链式法则容易计算。对于后一个，导数为：</p>
<p>$$\frac{\partial f _ i^c(p)}{\partial M _ {i\rightarrow j}}&#x3D;S _ {i\rightarrow j}\sum _ q \frac{\partial G(q,p+\delta p)}{\partial \delta p}f _ k^c(q) ~ (4)$$</p>
<p>然后通过公式2即可计算导数，由于M是二维的，因此$\partial \delta p$可分为$\partial \delta p_x$ $\partial \delta p_y$ 。</p>
<p>本方法可以在稀疏标记的帧上进行训练，并利用所有的数据（这里不理解）。</p>
<h3 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><p>非关键帧计算开销与每帧计算开销的比例：</p>
<p>$$r&#x3D;\frac{O(F)+O(S)+O(W)+O(N _ {task})}{O(N _ {feat})+O(N _ {task})}~(5)$$</p>
<p>由于复杂度相差较大，可简化为：</p>
<p>$$r\approx \frac{O(F)}{O(N_{feat})}~(6)$$</p>
<p>因此复杂度比例由流网络和特征网络决定，可以通过每秒浮点操作FLOPs评估，表2显示了我们实现的典型值。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Deep_Feature_Flow/1515676597477.png" alt="1515676597477"></p>
<p>另外，速度还跟关键帧的稀疏程度相关，我们定义每$l$连续帧选一个关键帧，则加速因子为：</p>
<p>$$s&#x3D;\frac{l}{1+(l-1)*r}~(7)$$</p>
<h3 id="关键帧安排"><a href="#关键帧安排" class="headerlink" title="关键帧安排"></a>关键帧安排</h3><p>在本文工作中，我们简单的固定关键帧的安排，关键帧的应用区间$l$固定。然而根据图像内容的变化可能变化的$l$能够实现更好的精度和速度的权衡。一般，关键帧应该是图像内容具有显著变化时。</p>
<p>如何设计高效和调整关键真的安排是我们之后改进工作。不同的视频任务可能有不同的行为和需求。从数据调整关键帧的安排是一个有趣的选择。</p>
<h2 id="4-网络架构"><a href="#4-网络架构" class="headerlink" title="4. 网络架构"></a>4. 网络架构</h2><p>本方法拟用于不同的网络和识别任务，因此我们采用先进的架构和重要的视觉任务。</p>
<h3 id="Flow-Network"><a href="#Flow-Network" class="headerlink" title="Flow Network"></a>Flow Network</h3><p>我们采用基于CNN的FlowNet[^9] 作为默认方法，并设计了两个低复杂度变体 FlowNet Half，每层减少一半的卷积核，复杂度为1&#x2F;4，另一个采用Inception 结构[^42] 的FlowNet Inception，复杂度减少为1&#x2F;8。</p>
<p>三个网络都在相同的数据集上训练，输出的stride为4。输入图像为1&#x2F;2，因此流网络的输出是原图的1&#x2F;8。特征图的stride是16，使用双线性差值将流场降采样，双线性差值为网络的中的非参数层，且可微。</p>
<h3 id="特征网络"><a href="#特征网络" class="headerlink" title="特征网络"></a>特征网络</h3><p>我们使用ResNet作为特征网络，最后一层1000路分类层去掉，特征stride从32减少为16得到分辨率更高的特征图，后面接DeepLab用于语义分割和R-FCN用于物体检测。跳着第一个block的conv5，其stride从2变为1，使用holing算法，保证感知域的情况下。另外我们增加了（dilation&#x3D;6）的3x3卷积核，减少特征通道维度减少为1024。该1024维的特征图作为后续任务的中间特征图。</p>
<p>表2显示了复杂度的比例。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Deep_Feature_Flow/1515678762441.png" alt="1515678762441"></p>
<h3 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a>语义分割</h3><p>随机初始化1x1的卷积核得到（C+1）得分图，C是类别数。之后跟一个softmax层输出每个像素的概率。因此任务网络仅有一个可学习的网络层，整体的网络结果与DeepLab类似。</p>
<h3 id="物体检测"><a href="#物体检测" class="headerlink" title="物体检测"></a>物体检测</h3><p>我们采用先进的R-FCN，对于中间特征图，两个全卷积网络分别应用于前512维特征和后512维特征，分别用于区域推荐任务和检测任务。</p>
<p>区域推荐分支，使用RPN，我们设定9个anchors，两个并排的1x1的卷积层得到2n维的物体得分和4n的回归值。通过NMS（0.7）后得到300个ROI。</p>
<p>检测分支，两个并排的1x1卷积层输出位置敏感的得分图和bbox回归图。他们的维度是$(C+1)k^2$和$4k^2$。k是检测器&#x2F;回归器数量，细节见[^8] 。最后使用NMS（0.3）得到最后的结果。</p>
<h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5. 实验"></a>5. 实验</h2><p>Cityscapes用于语义分割，ImageNet VID用于物体检测。</p>
<h3 id="5-1-实验设置"><a href="#5-1-实验设置" class="headerlink" title="5.1 实验设置"></a>5.1 实验设置</h3><h4 id="Cityscapes"><a href="#Cityscapes" class="headerlink" title="Cityscapes"></a>Cityscapes</h4><p>50个不同城市，帧率17fps，训练集2975，验证500，测试1525个snippets。每个snippet有30帧，第20帧一像素级的标注，可用于语义分割，30个语义类别。遵守[^5]的规则，训练集上训练，验证集上测试，语义分割的精确度使用像素级的平均交并比得分评估。</p>
<p>对于训练，图像都缩放为短边1024pixels用于特征网络，512pixels用于流网络。使用SGD训练，20K迭代，在8GPU上，每个GPU一个mini-batch，学习率前15K迭代0.001，后5K迭代0.0001。</p>
<h4 id="ImageNet-VID"><a href="#ImageNet-VID" class="headerlink" title="ImageNet VID"></a>ImageNet VID</h4><p>训练集3862，验证集555，测试集937，全标注的视频段，帧率25或30，30个类别，使用mAP作为评价。</p>
<p>训练图像短边分别为600和300pixels，60K迭代，40K学习率0.001，20K学习率0.0001。</p>
<p>训练时，不仅使用VID训练集，还使用DET训练集，每个mini-batch提取图像的比例VID:DET &#x3D; 2:1。</p>
<h3 id="5-2-评估方法和结果"><a href="#5-2-评估方法和结果" class="headerlink" title="5.2 评估方法和结果"></a>5.2 评估方法和结果</h3><p>由于DFF灵活且有多种设计选择，我们为了评估，设定一些默认设置，特征网络ResNet-101，流网络使用FlowNet，关键帧$l$为5用于C的语义分割，10用于I的物体检测。对于每个snippet我们评估l个图像对，k&#x3D;i-l+1,…,i，每个帧i有gt标注。</p>
<h4 id="验证DFF架构"><a href="#验证DFF架构" class="headerlink" title="验证DFF架构"></a>验证DFF架构</h4><p>如表3所示。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Deep_Feature_Flow/1515682224218.png" alt="1515682224218"></p>
<ul>
<li>Frame：在单帧上训练N</li>
<li>SFF：使用预先计算大偏移flow，SFF-fast和SFF-slow采用不同的参数。</li>
<li>DFF：N和F使用端到端训练，固定N训练，固定F训练，分别训练</li>
</ul>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Deep_Feature_Flow/1515682632317.png" alt="1515682632317"></p>
<p>图4整理各种方法的精度和速度。我们发现单帧方法具有很强性能可以作为参考方法。我们使用DeepLab和R-FCN作为参考。SFF方法的精度不够好，虽然采用了最优的参数，提升有限，速度下降很大。</p>
<p>DFF得到了最后的结果，速度相同。但是发现固定N或F，或者分别训练精度下降很大，说明联合序列的重要性。</p>
<h4 id="精度与速度的平衡"><a href="#精度与速度的平衡" class="headerlink" title="精度与速度的平衡"></a>精度与速度的平衡</h4><p>我们设定了不同的关键帧区间l，看对与精度和速度的影响。结果如图3所示，DFF得到了极大的速度提升，速度和精度的变化是平滑的，因此可针对特定应用调整，例如对于检测任务ResNet-101从4.05fps增加到41.26fps，10倍的速度提升，精度仅从73.9%下降到69.5%。对于分割任务，ResNet-50的速度从2.24fps提升到17.48 fps，精度从69.7%下降到62.4%。</p>
<p>图3也显示我们应用使用那个流场网络，发现最小的FlowNet Inception优势最大，在相同精度下比另外两个网络更快。</p>
<p>应该用什么特征网络呢，在高精度区域，明显ResNet-101更好，在高速度区域，对于检测ResNet-101更好，对分割ResNet-50更好。这个差别可能与视频帧率有关，对于数据集C，其帧率更低，变化更大，因此很难采用长时间的传播，而为了获得较快的速度，ResNet-101要采用比ResNet-50更大的l才行。</p>
<p>上面的发现对于实际应用具有一定推荐意义，然而收到数据和任务限制，为了我们开发更多的设计得到更多的经验。</p>
<h4 id="分类网络的分割点"><a href="#分类网络的分割点" class="headerlink" title="分类网络的分割点"></a>分类网络的分割点</h4><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Deep_Feature_Flow/1515683850796.png" alt="1515683850796"></p>
<p>我们应该在网络的什么位置对特征和任务网络进行分割，默认只保留一层网络。分割点分别为5，12，21层。5层是增加了一个维度约减层（1层）和一个瓶颈单元（conv5c，3层），21层是增加了两个单元（conv5a和conv5b），21层是增加了三个单元（conv4）。0层是把任务网络加入到特征网络中，等同于直接输出最后的结果进行传播。</p>
<p>表5展示了不同的结果，可以发现精度的变化可以忽略，而随着任务网络层的增加，速度降低。任务层使用1层和0基本抑制，我们设定为1是为了给特征融合后提供以小额可调的参数，使其更具一般性。</p>
<p>由于版面限制，更多的结果和细节请看本的在线版。</p>
<h2 id="6-未来工作"><a href="#6-未来工作" class="headerlink" title="6. 未来工作"></a>6. 未来工作</h2><p>有几个研究重点用于之后的研究。首先就是联合学习对于流场质量的影响，我们无法评估由于缺少gt数据。目前的光流工作受限于合成数据或小数据，对与深度学习不足。</p>
<p>我们的工作在流场估计和关键帧安排上有改进空间，本文我们采用FlowNet主要是能选择的方法很少。设计更快更精确的流场网络在未来值得研究。对于关键帧的安排，好的方法能够同时显著提升精度和速度。</p>
<p>我们详细该项研究打开了许多新的研究问题，并希望激发更多的研究工作。</p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Video Object Detection</category>
      </categories>
      <tags>
        <tag>Video Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>PyCharm远程运行调试代码</title>
    <url>/Tutorial-%E6%95%99%E7%A8%8B/remote-debug/</url>
    <content><![CDATA[<p>本介绍了使用PyCharm进行远程debug的方法，实现本地写代码，远程服务器训练模型和调试代码的功能。</p>
<span id="more"></span>

<h2 id="为什么要用远程运行调试？"><a href="#为什么要用远程运行调试？" class="headerlink" title="为什么要用远程运行调试？"></a>为什么要用远程运行调试？</h2><p>有这么一个应用场景，你的代码需要在服务器端运行，因为运行环境安装的依赖库都在远端服务器上，而写代码的工作在本地的平台上更顺手。在此之前都是用Visual Studio Code编辑代码，然后用同步到远端服务器，再通过SSH登录服务器运行程序。这样的工作流程不仅效率低，容易出错（如果代码没同步就悲剧），而且不易debug。</p>
<p>难道没有一个IDE能够实现远程运行调试代码么？本着世界上比我聪明的人多了去了的原则，本人觉得一定有，但是一直没找到。直到今天偶然发现PyCharm支持远程debug，看了下网上的教程发现跟我的需求很像，于是折腾了一番，没想到真的成功了。</p>
<p>我的本地环境：</p>
<ul>
<li>Window 10 64bit</li>
<li>PyCharm Professional</li>
</ul>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>首先你需要安装PyCharm Professional版，Community版是没有远程调试功能的，如果你是学生可以通过学校的edu邮箱申请免费试用，如果不是请购买正版。</p>
<p>PyCharm下载地址: <a href="https://www.jetbrains.com/pycharm">https://www.jetbrains.com/pycharm</a></p>
<p>安装过程很简单这里就不赘述了，安装流程下一步下一步即可。</p>
<h2 id="远程服务器同步配置"><a href="#远程服务器同步配置" class="headerlink" title="远程服务器同步配置"></a>远程服务器同步配置</h2><ul>
<li><p>首先本地代码放在：<code>F:\Code\Sync\Detectron</code></p>
</li>
<li><p>服务器代码路径在：<code>/home/xuzhewei/code/Detectron</code></p>
</li>
</ul>
<p>打开PyCharm，打开Project文件夹<code>F:\Code\Sync\Detectron</code></p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/remote-debug/open-pycharm.png" />

<p>首先在<code>Tools&gt;Deployment&gt;Configuration</code>配置远程服务器地址：</p>
<ul>
<li>SFTP host: 是你服务器的地址</li>
<li>Port: 是端口号</li>
<li>Root Path: 是远程你期望存放代码的位置</li>
<li>User name: 是远程服务的登录用户名</li>
<li>Password: 远程服务器的登录密码</li>
</ul>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/remote-debug/sftp.png" />

<p>切换到<code>Mappings</code>标签页</p>
<ul>
<li>Local path: 本地代码存放位置</li>
<li>Deployment path on server Detectron: 服务器端存放Detectron的位置</li>
<li>Web …: 这个我们用不上，是web项目需要配置</li>
</ul>
<p>如果你还有一些文件或文件夹不想同步，那么在配置对话框的第三个tab页<code>Excluded path</code>里面添加即可，可同时指定本地和远程。</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/remote-debug/sftp-mappings.png" />

<h2 id="上传下载文件"><a href="#上传下载文件" class="headerlink" title="上传下载文件"></a>上传下载文件</h2><p>手动上传方式很简单，选择需要同步的文件或文件夹，然后选择 <code>Tools&gt;Deployment&gt;Upload to Detectron</code>(这个是刚刚配置的部署名称)</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/remote-debug/upload.png" />

 

<h2 id="比较远程和本地文件"><a href="#比较远程和本地文件" class="headerlink" title="比较远程和本地文件"></a>比较远程和本地文件</h2><p>有时候你并不确定远程和本地版本的完全一致，需要去比较看看。PyCharm提供了对比视图来为你解决这个问题。</p>
<p>选择<code>Tools&gt;Deployment&gt;Browse Remote Host</code>，打开远程文件视图，在右侧窗口就能看到远程主机中的文件</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/remote-debug/remote-browser.png" />

<p>选择一个你想要对比的文件夹，点击<code>右键-&gt;Sync with Deployed to Detectron</code>，打开同步对比窗口，使用左右箭头来同步内容。</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/remote-debug/diff.png" />

<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/remote-debug/sync.png" />

 

<h2 id="PyCharm-远程调试"><a href="#PyCharm-远程调试" class="headerlink" title="PyCharm 远程调试"></a>PyCharm 远程调试</h2><p>选择<code>File&gt;Settings&gt;Project&gt;Project Interpreter</code>，然后在右边，点击那个小齿轮设置，如下</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/remote-debug/remote-setting.png" />

<p>然后点击<code>Add Remote</code>，填写主机的<code>ssh</code>配置，点击下一步后会让你填写密码</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/remote-debug/remote-server.png" />

<p>然后设置你的Python Interpreter，就是你在服务器端运行的python的位置，这里默认是系统python，但是我采用的anaconda的虚拟python环境因此这里的</p>
<ul>
<li><code>Interpreter</code>地址为 <code>/home/all/lib/anaconda2/envs/caffe2/bin/python</code></li>
<li><code>Sync folders</code>是服务器上代码放置的地址</li>
</ul>
<h2 id="远程调试"><a href="#远程调试" class="headerlink" title="远程调试"></a>远程调试</h2><p>首先验证在服务器上运行测试代码可以成功运行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd ~/code/Detectron</span><br><span class="line">python tests/test_spatial_narrow_as_op.py</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">No handlers could be found for logger &quot;caffe2.python.net_drawer&quot;</span><br><span class="line">net_drawer will not run correctly. Please install the correct dependencies.</span><br><span class="line">E0531 15:09:45.234580  5116 init_intrinsics_check.cc:59] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.</span><br><span class="line">E0531 15:09:45.234616  5116 init_intrinsics_check.cc:59] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.</span><br><span class="line">E0531 15:09:45.234625  5116 init_intrinsics_check.cc:59] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.</span><br><span class="line">Found Detectron ops lib: /usr/local/lib/libcaffe2_detectron_ops_gpu.so</span><br><span class="line">...</span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">Ran 3 tests in 3.146s</span><br><span class="line"> </span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>然后尝试在PyCharm远程调试</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/remote-debug/remote-debug.png" />

<p>然后注意不要用py.test，应直接用<code>python</code>运行</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/remote-debug/remote-debug-run.png" />

<p>输出结果和在SSH里显示的一样</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/remote-debug/remote-debug-result.png" />


<p>参考教程：<a href="https://www.xncoding.com/2016/05/26/python/pycharm-remote.html">https://www.xncoding.com/2016/05/26/python/pycharm-remote.html</a></p>
]]></content>
      <categories>
        <category>Tutorial 教程</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记 An Analysis of Scale Invariance in Object Detection - SNIP</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-An-Analysis-of-Scale-Invariance-in-Object-Detection-SNIP/</url>
    <content><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>本文分析了各种用于识别和检测多样尺度物体的技术</li>
<li>对输入数据不同处理，比较特定尺度设计和尺度不变设计检测器</li>
<li>为了检验上采样对检测小物体是否必须，我们在ImageNet上评估了不同网络架构检测小目标的性能</li>
<li>基于此，我们提出一个深度端到端的可训练图像金字塔网络用于物体检测，其在训练和测试时使用相同图像尺寸</li>
<li>小物体和大物体分别在小尺寸和大尺寸图像想较难检测，我们提出一个新的训练架构：Scale Normalization for Image Pyramids，其<strong>选择不同尺寸物体实例的梯度反向传播</strong>，并视作图片尺寸的函数</li>
<li>单模型性能为45.7%，3网络组合mAP为48.3%，Imagenet-1000预训练，COCO数据仅使用bbox监督</li>
<li>COCO 2017挑战 Best Student Entry</li>
<li><a href="http://bit.ly/2yXVg4c">Code</a></li>
</ul>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-SNIP/f1.png" width="70%"/>

<span id="more"></span>

<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>图片分类任务：ImageNet-1000 top-5 error 2%</p>
<p>物体检测任务：COCO-80 mAP(0.5) 62%</p>
<p>为什么物体检测比图像分类困难这么多？因为尺寸多样性，特别是检测非常小的物体是非常困难的。</p>
<p>ImageNet的物体尺寸中位数的尺寸是图像的0.554，而COCO是0.106，即几乎一半的物体尺寸小于图像尺寸的10%，如图1所示。</p>
<p>这使得CNN必须能够处理尺寸多样性。并且由于分类和检测数据集的物体尺寸差异，导致在fine-tuning时会出现domain-shift问题。</p>
<p>本文，我们第一次展现了该问题的证据，并提出一种训练框架称为SNIP。</p>
<p>也有一些方法提出：</p>
<ul>
<li>1）结合浅层和深层特征</li>
<li>2）使用膨胀或可变性卷积增加感知域用于检测大尺寸物体</li>
<li>3）在不同分辨率特征层分别预测不同尺寸的物体</li>
<li>4）结合语义进行检测</li>
<li>5）在各种尺寸上训练</li>
<li>6）在各种尺寸上测试，然后用NMS综合所有结果</li>
</ul>
<p>上述结构创新显著改善了物体检测，但是有些训练问题没有讨论：</p>
<ul>
<li>1）图像上采样是必须的么？为什么一般640x480的图像会上采样到800x1200？能否在预训练时就使用小stride？</li>
<li>2）从图像分类模型fine-tuning物体检测器，在图像缩放之后物体尺寸需要被固定么？或者需要覆盖所有的物体尺寸么？</li>
</ul>
<p>Section3，研究输入不同尺寸图片对现有网络用于ImageNet分类的影响，修改CNN结构用于分布不同尺寸图像。实验表面上采样对检测小物体很重要</p>
<p>为了分析尺寸变化的影响，在Section5比较尺寸特定和尺寸不变设计检测器性能。</p>
<p>尺寸特定，对不同尺寸范围分别训练检测器，能够减少domain-shift影响，但是由于数据量减少对性能降低。尺度不变设计更加困难。</p>
<p>Section6，我们提出新的训练范式SNIP。通过图像金字塔实现尺度不变，选择合适的物体尺寸。为了减少主干CNN的域偏差，我们只反向传播与预训练模型具有相同分辨率的RoI或anchor。这样SNIP实现了所有物体均进行训练。</p>
<p><strong>SNIP实际就是将ROI或anchor缩放到同一尺寸后进行训练，小物体放大，大物体缩小。</strong></p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><h2 id="3-多尺度图像分布"><a href="#3-多尺度图像分布" class="headerlink" title="3. 多尺度图像分布"></a>3. 多尺度图像分布</h2><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-SNIP/f3.png" width="60%"/>

<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-SNIP/f4.png"/>

<p>本节研究域偏移的影响，我们研究这个问题因为先进检测器一般在在800x1200训练，1400x2000测试检测小物体</p>
<p>1）对ImageNet下采样到48，64，80，96，128，然后上采样到224进行训练，CNN-B（ResNet-101），结果如图3和图4(a)所示。随着训练和测试分辨率偏差的增大，性能逐步下降。</p>
<p>2）CNN-S 在48x48上训练并测试，将卷积核变为3x3，stride为1，在96x96上训练改为5x5，2。应用数据增强技术，随机裁剪，颜色增强。结果如图4所示。可以发现上述变化改进了小物体检测效果</p>
<p>3）CNN-B-FT，将下采样的图片上采样后进行训练，其效果优于CNN-S，说明在高分率图像上训练也能帮助在低分辨率图像上的识别。因此相比于减少stride，增大图像尺寸更好。</p>
<p>训练物体检测器可选择对不同分辨率训练不同检测器，或训练单一检测器，由于使用预训练的模型是很方便的，因此采用上采样图像并在高分辨率下预训练模型是更好的选择。目前主流的方法也是这一范式，我们的分析也支持这种选择。</p>
<h2 id="4-背景"><a href="#4-背景" class="headerlink" title="4. 背景"></a>4. 背景</h2><p>介绍一些baseline方法，尤其是Deformable RFCN</p>
<h2 id="5-数据变换or正确尺寸？"><a href="#5-数据变换or正确尺寸？" class="headerlink" title="5. 数据变换or正确尺寸？"></a>5. 数据变换or正确尺寸？</h2><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-SNIP/f5.png"/>

<p>GPU显存的限制，很多方法在低分辨率训练，高分辨率测试，这导致了第3节所说的尺度域偏移问题。不同设置训练检测器，在1400x2000上测试小物体检测（32x32以下物体）。结果如表1所示。</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-SNIP/t1.png"/>

<ul>
<li>800all 图像分辨率为800x1200进行训练</li>
<li>1400all 缩放为1400x2000进行训练，提升有限，可能是大尺寸物体太多</li>
<li>800all&lt;80px 仅使用低于80px的目标进行训练，但是结果发现性能变差，可能是减少了很多训练样本。</li>
<li>MST，多尺寸训练，缩放图片，随机选择，效果仍然变成，可能是由于出现了极大或极小的物体。</li>
</ul>
<p>因此我们认为，应在训练时选择合适的物体尺寸进行进行训练。</p>
<h2 id="6-图像金字塔上物体检测"><a href="#6-图像金字塔上物体检测" class="headerlink" title="6. 图像金字塔上物体检测"></a>6. 图像金字塔上物体检测</h2><p>目的是训练数据不仅包含最多样的外观和姿势，同时保证合适的尺寸约束。这种方法称为SNIP。并且我们还讨论了训练时GPU显存的限制。</p>
<h3 id="6-1-SNIP"><a href="#6-1-SNIP" class="headerlink" title="6.1 SNIP"></a>6.1 SNIP</h3><p>SNIP是MST的变种，其使得物体的分辨率接近预训练数据集。MST中大尺寸图像中的大目标和小尺寸图像的小目标不容易分类。为了避免这种情况，仅使用合适尺寸范围内的物体进行训练，其他的在反向传播时忽略。SNIP不仅减少了域偏差同时保存了所有多样的外观和姿势。表1显示SNIP对于小物体的有效性。<br>首先会排除掉尺寸不在范围之内的gt，然后anchor如果跟gt的iou大于0.3则也会被排除。测试阶段，对每个分辨率进行proposal和分类，如图6所示。并且不会选择分辨率在之外的检测结果。最后，缩放回正常尺寸，使用soft-NMS组合检测结果得到最后的结果。<br>ROI的分辨率在池化后接近预训练模型，使得模型更容易进行学习。对于位置感知的R-FCN等方法尤为重要。</p>
<h3 id="6-2-采样子图"><a href="#6-2-采样子图" class="headerlink" title="6.2 采样子图"></a>6.2 采样子图</h3><p>在高分辨率图像上训练深度网络需要更多的显存，因此我们裁剪图像来满足GPU显存限制，1000x1000能够覆盖图像中所有小物体。每张图随机生成50个1000x1000的chips，即crop images，然后选择覆盖物体最多的chips。这个过程不会影响训练结果。</p>
<h2 id="7-数据集和评估"><a href="#7-数据集和评估" class="headerlink" title="7. 数据集和评估"></a>7. 数据集和评估</h2><p>COCO，123000图训练、验证，20288测试。由于COCO测试服务器没有召回率，我们使用118000训练，5000测试召回率。小物体面积为32x32以下，中物体32x32到96x96，大物体大于96x96。</p>
<h3 id="7-1-训练细节"><a href="#7-1-训练细节" class="headerlink" title="7.1 训练细节"></a>7.1 训练细节</h3><p>分别训练RPN和RCN，RPN训练6epochs，RCN训练7epochs。RCN使用OHEM，8个P6000 GPU，每个minibatch一种分辨率，因此一张图可能前向传播多次。如果没有正样本，当前图片被忽略。<br>尺寸范围：[0,80] at 1400x2000，[40,160] at 800x1200，[120,inf] at 480x800。<br>RPN在第一个epoch之后使用SNIP，RCN在第三个epochs之后，SNIP会增加一倍训练时间。</p>
<h3 id="7-2-RPN细节"><a href="#7-2-RPN细节" class="headerlink" title="7.2 RPN细节"></a>7.2 RPN细节</h3><p>作者对RPN进行了一些改进</p>
<h3 id="7-3-实验"><a href="#7-3-实验" class="headerlink" title="7.3 实验"></a>7.3 实验</h3><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-SNIP/t2.png"/>

<p>表2显示了SNIP的提升相比MS和MST都是很明显的。</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-SNIP/t3.png"/>
表3显示了SNIP对RPN的改进，以及对不同尺寸物体的召回率

<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-SNIP/t4.png"/>
表4显示了SNIP对IPN（Image Pyramid Network）的影响
说明SNIP对RPN和分类均有作用
使用更好的主干网络能够获得更好的结果
全部使用DPN-92作为RPN，然后平均各网络的分类得分，得到的结果最优。]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Object Detection</tag>
        <tag>SNIP</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记 Data Distillation: Towards Omni-Supervised Learning</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-Data-Distillation-Towards-Omni-Supervised-Learning/</url>
    <content><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>Omni-supervised learning: 全方位学习：一种特殊的半监督学习，包括所有可学习的数据和来自Internet的为标记数据</li>
<li>Data Distillation: 数据蒸馏：一种集成预处理方法，将未标记的数据进行多种变化，使用单一模型自动生成新的训练标记</li>
<li>作者主张，目前的视觉识别模型已经具有足够的精度进行自学习（self-learning）,从而挑战真实世界的数据</li>
<li>实验表面，使用<strong>数据蒸馏</strong>训练的模型超过完全使用标注数据训练的方法</li>
</ul>
<span id="more"></span>

<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Omni-supervised learning，利用所以可利用的训练数据，包括标记和未标记的。</p>
<p>semi-supervised learning，研究主要针对一个完整数据集，将其分为标记和未标记的，然后进行训练，这类研究具有上限，即全部使用标记数据训练。omni-supervised learning 没有上限，只有下限，使用全部数据训练是其下限。</p>
<p>为了解决Omni-supervised learning，本文提出数据蒸馏概念，即使用大量标记数据训练一个模型对未标记数据进行标记，然后使用多出来的数据进行训练。<strong>但是使用自己标注的数据进行训练往往没有意义</strong>。为了解决这个问题，我们集成单一模型在多种变换上的结果。</p>
<p>self-learning的研究可以追溯到1960s，但是由于近几年监督模型的快速改进，允许我们相信其对于未标记数据的预测，降低了对数据清洗的需要。</p>
<p>为了测试数据蒸馏，我们使用人形骨架检测任务，COCO。训练模型为Mask R-CNN，提升了2个百分点。</p>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><h2 id="3-Data-Distillation"><a href="#3-Data-Distillation" class="headerlink" title="3. Data Distillation"></a>3. Data Distillation</h2><p>四步：</p>
<ol>
<li>在手工标记数据上训练一个模型</li>
<li>将该模型应用于未标记数据的多种变换上</li>
<li>将多种预测集成，使预测称为标记</li>
<li>联合所有数据再次训练</li>
</ol>
<h4 id="Multi-transform-inference"><a href="#Multi-transform-inference" class="headerlink" title="Multi-transform inference"></a>Multi-transform inference</h4><p>变换输入来增强精度是一种常见策略，例如：图像裁剪、多尺度等。我们对单一模型使用多种变换推理。</p>
<h4 id="Generating-labels-on-unlabeled-data"><a href="#Generating-labels-on-unlabeled-data" class="headerlink" title="Generating labels on unlabeled data"></a>Generating labels on unlabeled data</h4><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Data_Distillation/f2.png"></p>
<p>集成多种变换的预测结果经常能获得优于单一模型预测结果。如图2所示。我们发现集成后的预测能产生新的知识，并用于训练模型本身。</p>
<p>例如对于分类问题，可以对一个图片生成平均类别概率向量[^18] ，但是会存在两个问题。1）“soft”标签不能直接用于训练模型，需要修改loss。2）结构化输出的问题，物体检测或姿势估计，平均结果是不合理的。</p>
<p>我们简单的组合这些多变换预测，生成“hard”标签，与手工标记数据中具有相同的结构或类型。生成硬标签需要一些任务特定的逻辑能表征问题结构（例如NMS）。之后即可进行训练。该过程尽管需要推理多次，但是也不训练多个模型更高效。</p>
<h3 id="Knowledge-distillation"><a href="#Knowledge-distillation" class="headerlink" title="Knowledge distillation"></a>Knowledge distillation</h3><p>1）每个minibatch包含手工标记数据和自动标记数据</p>
<p>2）训练schedule需要延长，配合数据量的增加</p>
<h2 id="4-数据蒸馏用于骨架检测"><a href="#4-数据蒸馏用于骨架检测" class="headerlink" title="4. 数据蒸馏用于骨架检测"></a>4. 数据蒸馏用于骨架检测</h2><h4 id="Data-transformations"><a href="#Data-transformations" class="headerlink" title="Data transformations"></a>Data transformations</h4><p>本文使用几何变换，其能够满足集成预测结果的需求，在集成时需要进行反变换，再集成。本文使用两种常用的变化：尺度变化、水平翻转（<em>数据增强而已</em>）我们对为标记的图片进行缩放，短边从[400,1200]，步长100像素。在验证集上应用相同策略，选择一个最优的变换使得模型提升最大，并将该模型作为teacher模型。</p>
<h4 id="Retraining"><a href="#Retraining" class="headerlink" title="Retraining"></a>Retraining</h4><p>学生模型在联合数据集上训练，每个minibatch比例为6:4。学生模型可以采用teacher model初始化也可以用ImageNet初始化，我们发现使用ImageNet初始化的效果总是更好，说明teacher 模型进入了局部最优</p>
<h2 id="5-Keypoint检测实验"><a href="#5-Keypoint检测实验" class="headerlink" title="5. Keypoint检测实验"></a>5. Keypoint检测实验</h2><h3 id="5-1-数据分割"><a href="#5-1-数据分割" class="headerlink" title="5.1 数据分割"></a>5.1 数据分割</h3><ul>
<li>co-80: COCO 80K张训练图像</li>
<li>co-35，2014 val的子集 35k张图</li>
<li>co-115，上面的组合为：2017 COCO 训练集，115k张图</li>
<li>un-120，COCO 2017提供了120k张未标记的图片</li>
<li>s1m-180，从Sports-1M[^19] 随机选择180k个视频，每个视频随机选择1帧</li>
</ul>
<h3 id="5-2-主要结果"><a href="#5-2-主要结果" class="headerlink" title="5.2 主要结果"></a>5.2 主要结果</h3><p>1）co-35作为标记数据，co-80作为未标记数据</p>
<p>2）co-115作为标记数据，分布相似的un-120作为未标记数据</p>
<p>3）co-115作为标记数据，分布不相似的s1m-180作为未标记数据</p>
<p>结果如表1所示</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Data_Distillation/t1.png" width="70%" />

<h4 id="小数据量"><a href="#小数据量" class="headerlink" title="小数据量"></a>小数据量</h4><p>这里的实验类似经典的半监督问题策略，即使用一部分标注和完全标注的性能对比，完全标注的结果作为上界。本文提出的方法主要还是用于对实际未标注的数据进行提升更具挑战性。（但是半监督的研究也可以应用于本文的问题，作者为什么没有进行比较？）</p>
<h4 id="大数据量-相似分布"><a href="#大数据量-相似分布" class="headerlink" title="大数据量-相似分布"></a>大数据量-相似分布</h4><p>使用co-115和un-120进行训练，使用DD方法进行标注。均匀不同提升。在论文[^27]中使用率1.5倍的全标注数据进行训练AP提升了3个百分点，本文的方法使用DD也提升了2个百分点。</p>
<h4 id="大数据量-不相似分布"><a href="#大数据量-不相似分布" class="headerlink" title="大数据量-不相似分布"></a>大数据量-不相似分布</h4><p>使用co-115和s1m-180训练，尽管数据分布不一致，但是AP仍然具有提升，说明DD对数据分布差异具有鲁棒性。</p>
<h3 id="5-3-消融实验"><a href="#5-3-消融实验" class="headerlink" title="5.3 消融实验"></a>5.3 消融实验</h3><h4 id="迭代次数"><a href="#迭代次数" class="headerlink" title="迭代次数"></a>迭代次数</h4><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Data_Distillation/t2.png" width="70%" />

<p>由于数据量增加，因此迭代次数也应该相应增加。为了表面我们的提升不是来自迭代次数的增加，实验结果如表2所示。</p>
<p>对于全监督baseline，130k迭代达到最优，相比默认的90k来说。但是增加到130k时下降，说明过耦合。</p>
<p>另外，DD增加数据量后，360k迭代还未完全耦合。</p>
<h4 id="未标记数据量"><a href="#未标记数据量" class="headerlink" title="未标记数据量"></a>未标记数据量</h4><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Data_Distillation/t5.png" width="70%" />

<p>如图5所示，使用不同比例的未标记数据p，minibatch的比例为1:p，迭代次数也相应增加1+p倍（基准次数为130k）。从图5所示，随着数据的增加，AP持续提升。</p>
<h4 id="teacher-模型的影响"><a href="#teacher-模型的影响" class="headerlink" title="teacher 模型的影响"></a>teacher 模型的影响</h4><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Data_Distillation/f6.png" width="70%" />

<p>不同的teacher AP的提升，导致student AP的提升</p>
<h4 id="测试时增强"><a href="#测试时增强" class="headerlink" title="测试时增强"></a>测试时增强</h4><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Data_Distillation/t3.png" width="70%" />

<p>在测试时使用多变换推理也会改进结果。</p>
<h2 id="6-物体检测实验"><a href="#6-物体检测实验" class="headerlink" title="6. 物体检测实验"></a>6. 物体检测实验</h2><h3 id="6-1-实现"><a href="#6-1-实现" class="headerlink" title="6.1 实现"></a>6.1 实现</h3><p>本文选择物体检测器为Faster R-CNN[^30] 使用FPN[^23] 为backbone，并使用RoIAlign改进[^15] 。我们采用如[^31] 中描述的端到端训练。</p>
<p>使用bbox投票[^10] 方法进行组合union训练集。</p>
<p>物体检测涉及多种类别，对每个类别设定得分阈值。我们选择阈值使得未标记图片集中平均每张图片选择的类别数量与标记的数据中每个类别的数量匹配。</p>
<p>图7显示u-120中生成的标记。</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Data_Distillation/f7.png" width="70%" />

<h3 id="6-2-物体检测结果"><a href="#6-2-物体检测结果" class="headerlink" title="6.2 物体检测结果"></a>6.2 物体检测结果</h3><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Data_Distillation/t4.png" width="70%" />

<p>表4显示了结果：</p>
<p>1）小数据量：通过类似半监督的方法，将co-80作为无标记数据进行训练。</p>
<p>2）大数据量：co-115作为标记数据，un-120作为非标记数据</p>
<h4 id="小数据量-1"><a href="#小数据量-1" class="headerlink" title="小数据量"></a>小数据量</h4><p>co-35是下界，co-115是上界，我们发现DD方法更接近下界，这个问题在未来将进行更进一步的研究</p>
<h4 id="大数据量"><a href="#大数据量" class="headerlink" title="大数据量"></a>大数据量</h4><p>co-115是下界，应用DD之后数据都有不同的增长。</p>
<p>表4显示使用为标记的数据训练物体检测是一个更具挑战性的问题，但是DD策略仍然能够改进。</p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Omni Supervised Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记 Learning non-maximum suppression</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-Learning-non-maximum-suppression/</url>
    <content><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>目标检测器受益于端到端的训练范式：推荐、特征、分类器开始成为一个神经网络。另一个独立不可缺的组件是NMS，一个后处理算法将属于同一目标的bbox合并。NMS仍然是手工设计，策略简单，基于固定阈值的贪心聚类算法，导致召回率和精度之间权衡。我们提出一个新的网络架构来实现NMS，仅基于其box和得分。我们在PETS上验证行人检测和COCO上验证目标检测。结果表明该方法能够改进定位和遮挡问题。</p>
<span id="more"></span>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p><strong>聚类检测</strong> 贪心NMS作为标准算法从VJ一直应用于目前SOTA的算法。也有一些其他的聚类算法用于NMS，但是没有显示明显的提升：mean-shift 聚类[^6],[^35]， agglomerative 聚类[^2], affinity 聚类[^17], 启发式变体[^25]。 在[^27],[^23]中提出全局优化的原则聚类函数，尽管其还未用于贪心NMS。</p>
<p><strong>面向像素检测</strong> Hough投票建立了检测结果和图像证据直接的关系，这能够避免过度使用图像内容。但是整体上说hough投票的效果较差。论文[^37],[^5]结合语义标签，论文[^36]将检测作为labelling问题。这些方法依赖更多的信息，但是我们的系统纯粹基于检测结果。</p>
<p><strong>同现</strong> 有些工作提出检测目标群，而不是检测独立的个体来解决较强的遮挡问题，这个问题比NMS更复杂。论文[^22] 基于密度估计来抑制检测结果。我们的方法既不用图片信息也不用手工标记的成对目标。</p>
<p><strong>自语义</strong> 一些方法使用局部[^30],[^4]或全局[^31]图像信息重打分。这些方法会产生一些分离的双检测结果并且改进全局的检测治理，但是仍然需要NMS。我们将NMS作为一个重打分的问题，并且取消了后处理。</p>
<p><strong>图神经网络</strong> 一组检测结果可以被认为是一个图，具有重叠的窗口可以被认为是图中的边。论文[^18]在图上操作，但是其要求前处理过程定义节点顺序，在我们的问题中是模糊的。</p>
<p><strong>端到端的学习</strong> 很少有工作将NMS纳入端到端学习中。一种想法是在训练时包含NMS[^32],[^12]，使其能够得到实际测试时的结果，这样做更合理，但是并没有使NMS可学习。另一个想法是直接生成稀疏的结果使得NMS没有必要，论文[^26]在图像重叠patch上生成检测结果，但是在patch边缘仍然需要NMS。论文[^13]设计一个卷积网络结合NMS和不同的iou阈值进行决策，其网络选择需要NMS的局部区域。但是上述方法都没有完全消除NMS直接生成稀疏的结果。我们的网络有能力直接进行抑制操作，而不是选择一些子集再经过最后的抑制步骤。</p>
<h2 id="3-检测和非极大值抑制"><a href="#3-检测和非极大值抑制" class="headerlink" title="3. 检测和非极大值抑制"></a>3. 检测和非极大值抑制</h2><p>本节我们回归非极大值抑制为何必须，并且我们指出为何当前的检测器不能对一个目标输出一个结果，提出两个检测器必要的部件。</p>
<p>目前检测器不返回所有检测结果，而是使用NMS作为后处理步骤去除冗余的结果。为了实现端到端学习的检测器，我们对没有后处理方法的检测器感兴趣。为了理解为何需要NMS，很有必要审视检测任务及其如何评估的。</p>
<p><strong>目标检测</strong> 目标检测任务即在一张图上映射一组boxs：每个box对应一个目标并与其边缘贴紧。这意味着检测器应该对每个目标返回一个结果。由于检测过程的内在不确定性，评估时以检测结果的置信得分进行评估。得分高的错误检测比低得分的错误检测惩罚高，使得错误结果的得分低。</p>
<p><strong>检测器不生成我们期望的结果</strong> 检测任务可以简化为分类任务，在图像中所有可能的地方进行分类。这个观点使得“假设和得分”检测器兴起，其构建一个搜索空间（滑窗，推荐）然后独立的估计每个检测是结果的类别概率。结果就是当两个重叠很大box对应相同的目标其得分均非常高，因为看起来几乎一样。一般来说，一个目标周围会生成多个高得分结果，得分跟目标的重叠率相关。</p>
<p><strong>贪心NMS</strong> 为了实现一个目标一个结果，假设高重叠的检测属于一个目标，即选择置信分最高的结果，然后抑制与其IoU率超过阈值的结果。</p>
<p><strong>贪心NMS不够好</strong> 该算法表现好有两个前提，1）能够抑制同一目标导致的多个检测结果；2）不会抑制接近的其他检测结果。如果目标分散条件2很容易达到，并且条件1也能工作很好。在拥挤场景，目标间具有很强的遮挡，宽抑制和窄抑制存在对抗。换句话说，一张图一个目标NMS是不重要的，但是对高遮挡问题，需要更换的NMS算法。</p>
<h3 id="3-1-不需要NMS的未来"><a href="#3-1-不需要NMS的未来" class="headerlink" title="3.1 不需要NMS的未来"></a>3.1 不需要NMS的未来</h3><p>在没有手工算法的情况下实现真正端到端系统，我们不禁要问：为什么需要手工设计的后处理步骤？为什么检测器不能对一个目标输出一个结果？</p>
<p>重叠高的窗口具有较高的得分是鲁棒性的要求：相似的输入得到相似的输出。因此，检测器对一个目标输出一个检测结果那么需要结合其他检测结果。冗余的结果需要合并，从而使得检测器能分辨这些是重复的结果，只输出一个高得分。一般用IoU的阈值来定义正负样本，因此位置微小偏移时检测结果仍然认为是正样本。这样的训练数据增强方法是为了增加检测器的鲁棒性。但是这样的策略不会倾向于一个目标产生一个检测结果，而且产生多个高得分的结果。</p>
<p>因此，我们发现两个关键组件：</p>
<ol>
<li>一种损失函数能够惩罚冗余检测结果</li>
<li>联合处理周围的结果，使得检测器获得足够的信息来判断该目标是否被检测多次</li>
</ol>
<p>本文，我们设计一个网络结合上述两个组件。为了验证我们的网络能够实现NMS，仅对检测结果和得分进行操作，而不采用图像特征。</p>
<h2 id="4-使用网络进行NMS"><a href="#4-使用网络进行NMS" class="headerlink" title="4. 使用网络进行NMS"></a>4. 使用网络进行NMS</h2><p>我们的方法避免硬决定，并且不会抛弃一些检测结果。除此之外，我们将NMS作为重打分任务。在重打分之后，简单的阈值选择就能够减少检测结果。我们将所有重打分的结果进行评估，不进行任何后处理过程。</p>
<h3 id="4-1-损失"><a href="#4-1-损失" class="headerlink" title="4.1 损失"></a>4.1 损失</h3><p>在计算损失的时候加入benchmark的评估中采用匹配方法。匹配上的作为正样本，否则作为负样本。<br>$$<br>L(s_i,y_i)&#x3D;\sum^N_{i&#x3D;1}w_{y_i}\cdot log(1+exp(-s_i\cdot y_i))<br>$$<br>$s_i$是检测得分，$y_i$是预测类别。权重$w_{y_i}$用于平衡检测任务的样本不平衡。面对多类别问题时，检测结果有得分和类别，在处理时我们仍然将每个类别当作二元分类问题。</p>
<p>表征检测得分我们使用热编码：一个零向量在该类别对应的位置包含其得分。由于mAP计算是为考虑样本的数量，因此我们实际的权重也是将其当做均匀分布的。</p>
<h3 id="4-2-“Chatty”windows-”交流窗口"><a href="#4-2-“Chatty”windows-”交流窗口" class="headerlink" title="4.2 “Chatty”windows ”交流窗口"></a>4.2 “Chatty”windows ”交流窗口</h3><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2017-LearningNMS/f3.png" width="100%" />

<p>为了最小化上述loss，我们需要网络处理结果联系。为此，我们设计了一个循环结构称为block（如图3）。一个block给每个检测结果与其周围结果的表示，并更新自己的表示。堆叠多个block后意味着网络能够允许检测结果之间交流，并更新自己的特征表示。我们成其为<strong>GossipNet（Gnet）</strong>。</p>
<p>这里有两个非标准的操作是关键。1. 构建成对检测结果的特征表示层；这导致一个关键问题：每个结果周围的结果输了是不一定的；2.因此我们后续使用池化来解决。</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2017-LearningNMS/f2.png" width="70%" />

<p><strong>检测特征</strong> block的输入是检测结果的特征向量，输出是更新向量，如图2所示。特征纬度是128维。最后输出每个检测结果的更新得分。第一个block以全零向量作为输入，检测结果信息输入网络的pairwise计算步骤。</p>
<p><strong>Pairwise 检测语义</strong> 每个mini-batch包含n个检测结果，每个结果用c维向量描述，因此数据量为$n\times c$ 。对每个检测结果 $d_i$ ，生成与其相交$(IoU&gt;0.2)$的成对结果$(d_i,d_j)$。 特征两个d的特征和g维的成对特征则特征纬度为：$l&#x3D;2c+g$ 。如果一个d周围一k个结果，那么生成的batc尺寸为$K\times l$。k的数量可能不一样，因此采用全局最大池化合并为一个结果，然后使用全连接层更新结果。</p>
<p><strong>检测对特征</strong> 监测对的语义特征包含几个属性：1) IoU；2)-3) 归一化的x，y距离；4)-5) 宽高变化；6) 宽高比变化; 7)-8) 两个检测得分。如果是多类问题，得分是一个向量而不是标量。我们将这些特征输入3个全连接层，用于学习g检测对特征。</p>
<p><strong>Block</strong> block的作用是让检测结果基于他们的邻居更新表示。其包含降维，成对检测语义层，2个全连接成，池化层，全连接层最后一层用于增加维度。最后提供三个fc层用于预测新得分。</p>
<p><strong>参数</strong> 网络包含16个block，检测特征128维，在检测对时缩减为32维，最后输出又扩增为128维。若我们改变特征维度，我们帮助每层的特征比例一致。</p>
<p><strong>信息传递</strong> 前向过程通过block可以解释为信息传递。每个检测结果传递信息给邻近结果，并降低得分，为了替代手工设计信息传递算法，我们使得网络潜在自动学习。</p>
<h3 id="4-3-Remarks"><a href="#4-3-Remarks" class="headerlink" title="4.3 Remarks"></a>4.3 Remarks</h3><p>Gnet没有使用NMS但是效果与采用了NMS的Tnet接近。Gnet网络更大，因此采用训练样本更多的妨害。</p>
<p>Gnet是个纯NMS功能网络，不需要图像特征，仅在检测结果上进行操作。意味着Gnet不能当做网络层加入检测器。而事实上是可以这么整合的，我们在未来会开展研究。</p>
<p>我们的目标是联合图像上所有检测结果进行重打分。通过允许检测结果观察旁边的结果来更新自身的表示。实验表面模型对参数的鲁棒性，并且随着深度的增加性能变优。</p>
<h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5. 实验"></a>5. 实验</h2><p>数据集PETS和COCO。评估指标average precision，AP。</p>
<h3 id="5-1-PET数据集"><a href="#5-1-PET数据集" class="headerlink" title="5.1 PET数据集"></a>5.1 PET数据集</h3><p>该数据集一般用于单一尺度行人检测，包含不同的遮挡程度。首先按照论文[^13]和论文[^28]的设置进行训练和测试。</p>
<p>训练模型包含8个block，128维输入特征，30k迭代，学习率0.001，每10k次衰减0.1。</p>
<p>NMS和Tnet作为baseline</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2017-LearningNMS/f4.png" width="100%" />

<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2017-LearningNMS/f5.png" width="100%" />

<p>图4显示了结果，NMS阈值高被抑制的结果多，Gnet相比于最后的NMS提升了4.8个AP。图5显示了对不同遮挡级别的结果，Gnet比Tnet稍好。在遮挡高的情况下Gnet比NMS提升7.3AP。</p>
<h2 id="5-2-COCO：行人检测"><a href="#5-2-COCO：行人检测" class="headerlink" title="5.2 COCO：行人检测"></a>5.2 COCO：行人检测</h2><p>Faster R-CNN作为基准，Gnet使用ADAM训练2*1000K次迭代，学习率0.0001，每1000K次减少到0.00001。速度14ms&#x2F;image，评价67.3检测结果。</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2017-LearningNMS/f6.png" width="100%" />

<p>图6显示了block数量和性能的关系，随着网络月神，性能鲁棒性越好。我们能够Gnet能够替代NMS，</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2017-LearningNMS/t1.png" width="100%" />

<p>表1显示了Gnet和NMS的量化对比。</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2017-LearningNMS/t2.png" width="100%" />

<p>图7显示了Gnet对多类别检测时的作用，大部分类别有提示，少部分降低。</p>
<h2 id="6-结论"><a href="#6-结论" class="headerlink" title="6. 结论"></a>6. 结论</h2><p>本文提出Gnet用于替代NMS，其主要包含两个关键组件：1）惩罚同一目标生成两个结果；2）联合处理所有结果。该网络能够实现端到端训练。结果显示相比于NMS，Gnet的后处理效果更好。目前Gnet需要大量训练数据，未来会研究通过数据增强或更好的预训练初始化模型。以及配合图像特征进行处理。我们相信本文提出的想法和结果能够使得未来检测器和NMS的分离消失。</p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>NMS</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记 Repulsion Loss: Detecting Pedestrians in a Crowd</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Pedestrian-Detection/Note-Repulsion-Loss-Detecting-Pedestrians-in-a-Crowd/</url>
    <content><![CDATA[<p>本文设计了一种loss函数用于加强bbox的回归精度，这个特性适用于目标密集且相互遮挡较多的场景，例如行人检测问题。该文的想法与另一篇文章《<a href="http://www.xzhewei.com/Note-%E7%AC%94%E8%AE%B0/Pedestrian-Detection/Note-Individualness-and-Determinantal-Point-Processes-for-Pedestrian-Detection/">Individualness and Determinantal Point Processes for Pedestrian Detection</a>》类似，后者通过行列式处理，前者设计loss函数。</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>在人群中检测个体行人仍然是一个具有挑战性的问题，由于在真实世界场景中行人通常聚集在一起并且相互遮挡。本文我们首次通过实验研究人群间的相互遮挡如何影响行人检测器的性能，对人群间遮挡的问题进行了分析。然后我们提出了一个针对人群场景设计的外接矩形回归损失函数，称为<strong>repulsion loss</strong>。该损失包含两个部分：与目标的<strong>attraction</strong>项和与周围物体的<strong>repulsion</strong>项。<strong>repulsion</strong>项避免矩形框偏移到旁边的物体上，因此对群体中的定位更加鲁棒。基于repulsion损失训练的检测器在遮挡场景下相对其它检测器有显著提升。</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Repulsion_Loss/f1.png" width="70%"/>

<span id="more"></span>

<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>遮挡是目标检测领域一直需要解决的问题，这里有类间和类内两种。类内遮挡我们可以称为crowd遮挡。在行人检测中，大部分遮挡都是crowd遮挡。因子大部分行人检测场景，监控、自动驾驶，行人通常一起出现。例如在CityPerson数据集中，48.8%的行人相互遮挡的IoU大于0.1，26.4%的大于0.3。高频的crowd遮挡验证影响了行人检测的性能。</p>
<p>crowd遮挡的问题增加了行人定位的难度。例如当行人T与B重叠时，检测器会被相似的外观特征混淆，导致本来应该检测T目标的bbox偏移到B上，导致定位错误。并且经过NMS后，偏移的bbox可能被抑制导致漏检，因此crowd遮挡使得检测率对NMS的阈值非常敏感：高NMS虚警多，低NMS漏检多。这样的问题会影响许多实例分割框架，由于其需要非常精确的检测结果。因此如何在crowd场景中对个体行人鲁棒的定位是行人检测非常重要的问题。</p>
<p>在SOTA检测框架中，bbox回归用于目标定位，回归器能够减少proposal与gt直接的距离。然而，现有的方法仅要求proposal接近gt，而不考虑周围目标的影响。如图1所示，标准的bbox回归损失是不考虑周围的目标的。这带来一个问题：能否在检测人群中的一个目标时考虑周围目标的位置？</p>
<p>受到磁铁的启发，本文提出一种新的定位技术，repulsion 损失。其要求目标不仅接近目标T，并且远离周围的gt和bbox。换句话说，RepLoss包含两个部分：目标的吸引，非目标的排斥。如图1所示。红色bbox偏移到B会产生惩罚。因此RepLoss能够使得检测器对crowd场景更鲁棒。我们的贡献如下：</p>
<ul>
<li>我们先研究了crowd遮挡对行人的影响在CityPerson进行了量化分析</li>
<li>提出两类repulsion损失，RepGT 和 RepBox，前者避免bbox偏移到旁边的目标，后者减少NMS的影响</li>
<li>提出一个repulsion loss，端到端的行人检测器，在CityPerson和Caltech实现SOTA的性能。</li>
</ul>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p><strong>目标定位</strong></p>
<ul>
<li>RPN</li>
<li>Denbox[^15]提出一个不用anchor的全卷机网络</li>
<li>IoU Loss[^29] 最大化gt和bbox直接的IoU</li>
<li>Desai[^4] 也提出了不同类别吸引和对抗，但是没有从全局模型上解决目标类别问题</li>
</ul>
<p><strong>行人检测</strong></p>
<ul>
<li>ACF, LDCF, Checkerboard等ICF检测器</li>
<li>RPN+BF，HyperNet</li>
<li>部件模型[^23],[^27],[^34]处理遮挡问题</li>
<li>学习非极大值抑制[^13]</li>
</ul>
<h2 id="3-What-is-the-Impact-of-Crowd-Occlusion"><a href="#3-What-is-the-Impact-of-Crowd-Occlusion" class="headerlink" title="3. What is the Impact of Crowd Occlusion?"></a>3. What is the Impact of Crowd Occlusion?</h2><p>为了分析人群遮挡问题，本节我们实验研究了人群遮挡对行人检测结果的影响程度。在分析之前，首先介绍使用的数据集和基准检测器。</p>
<h3 id="3-1-Preliminaries"><a href="#3-1-Preliminaries" class="headerlink" title="3.1 Preliminaries"></a>3.1 Preliminaries</h3><h4 id="数据集和评估指标"><a href="#数据集和评估指标" class="headerlink" title="数据集和评估指标"></a>数据集和评估指标</h4><p>本文采用CityPerson数据集，包含5k张图片，35k个行人，13k个忽略区域。实验分别在reasonable train&#x2F;validation 集上训练和测试。评估指标：log miss rate in FPPI[0.01,1]</p>
<h4 id="检测器"><a href="#检测器" class="headerlink" title="检测器"></a>检测器</h4><p>我们采用最常见的Faster R-CNN作为检测器，并按照Zhang[^31]和Mao[^21]的设置对其进行修改。除此之外，我们使用R-50替代VGG16，其速度更快且参数更少。</p>
<p>另外，ResNet很少用于行人检测问题，其原因是卷积层的将采样比率过大而不能检测和定位小型人。为了解决这个问题，我们在最后的特征图上使用膨胀卷积，使得最后的缩放比例为1&#x2F;8。本文的ResNet方法实现了14.6 MR-2比CityPerson论文中的性能稍好。</p>
<h3 id="3-2-错误分析"><a href="#3-2-错误分析" class="headerlink" title="3.2 错误分析"></a>3.2 错误分析</h3><h4 id="漏检"><a href="#漏检" class="headerlink" title="漏检"></a>漏检</h4><p>首先我们在基准检测器上分析由于人群遮挡导致的漏检。CityPersons数据集提供了bbox和可见区域，因此遮挡occ可按下式计算</p>
<p>$$occ &#x3D; 1 - \frac{area(BBo{x_v})}{area(BBox)}$$</p>
<h4 id="虚警"><a href="#虚警" class="headerlink" title="虚警"></a>虚警</h4><p>我们定义$occ\ge0.1$为遮挡，$occ\ge0.1$ &amp; $IoU\ge0.1$为人群遮挡。通过统计在reasonable validation验证集中，包含1579个行人，其中810个为遮挡案例，479个为人群遮挡案例。很明显由于行人产生的遮挡超过了遮挡案例的一半以上</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Repulsion_Loss/f2.png" width="70%"/>

<p>如图2所示，我们分别评估了在不同FPPI下的检测器漏检率，并统计了漏检GT分别属于三个子集的数量。</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Repulsion_Loss/f3.png" width="70%"/>

<p>之后我们统计了所有虚警中人群站的比例，如图3(b)红线所示，大约20%。如图4所示，我们发现人群错误通常发生在预测的bbox偏移到旁边非目标GT上，或者多个GT有重叠的情况。并且，人群错误通常具有很高的置信得分导致很高的排名。因此改进检测器人群场景的鲁棒性，回归bbox时需要更具分辨性的loss。</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Repulsion_Loss/f4.png" width="70%"/>

<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><blockquote>
<p>pedestrian detectors are surprisingly tainted by crowd occlusion, as it constitutes the majority of missed detections and results in more false positives by increasing the difficulty in localization.</p>
</blockquote>
<p>提出Repulsion loss用于改进人群场景的行人检测器</p>
<h2 id="4-Repulsion-Loss"><a href="#4-Repulsion-Loss" class="headerlink" title="4. Repulsion Loss"></a>4. Repulsion Loss</h2><p>该损失受磁铁的启发，吸引和对抗包含三个部分</p>
<p>$$L&#x3D;L_{Attr}+\alpha<em>L_{RepGT}+\beta</em>L_{RepBox}$$</p>
<ul>
<li>$L_{Attr}$ 是吸引项，其要求预测的bbox接近被分配的目标</li>
<li>$L_{RepGT},L_{RepBox}$ 是对抗项，要求预测的bbox原理周围的目标和其他被分配了目标的bbox</li>
</ul>
<p>为了简单起见，先考虑两类检测问题，所有的GT都是同一类别，P和G分布是proposal和GT的bbox。P+是所有正proposal，至少有1个GT作为P+，其他的作为负样本。</p>
<p><strong>吸引项</strong> 使用一些距离目标函数来减少bbox和gt的检测，如欧式距离，SmoothL1距离，IoU。为了公平期间，本文采用SmoothL1，如大部分文章选择的方式。吸引项如式2所示，$G^p_{Attr}$选择IoU最大的gt作为该$B^p$的目标。</p>
<p>$$L_{Attr}&#x3D;\frac{\sum_{P\in P+}SmoothL1(B^p,G^p_{Attr})}{|P+|}$$</p>
<p><strong>对抗项(RepGT)</strong> 用于对抗不属于当前bbox回归目标的bbox。</p>
<p>首先找到与当前P有最大IoU的$G^p_{rep}$，然后基于IoG计算loss，$IoG(B,G)&#x3D;\frac{area(B\cap G)}{area(G)}$</p>
<p>RepGT loss为：</p>
<p>$$<br>L_{RepGT}&#x3D;\frac{\sum_{P\in P+}SmoothLn(IoG(B^p,G^p_{Attr}))}{|P+|}<br>$$</p>
<p>$$<br>SmoothLn &#x3D; \left{ {\begin{array}{<em>{20}{c}}<br>{ - \ln (1 - x)}\<br>{\frac{x - \sigma }{1 - \sigma } - \ln (1 - \sigma )}<br>\end{array}} \right.\begin{array}{</em>{20}{c}}<br>{x \le \sigma }\<br>{x &gt; \sigma }<br>\end{array}<br>$$</p>
<p>$\sigma$ 是平滑参数用于调整对抗的敏感性。当B与非目标GT越接近，则惩罚越大，因此RepLoss可以有效避免BBox偏移到周围的GT。</p>
<p><strong>对抗项(RepBox)</strong>  NMS对人群中行人检测的影响大，因此提出RepBox Loss用于对抗不同gt目标的bbox。</p>
<p>$$<br>L_{RepBox}&#x3D;\frac{\sum_{i\ne j}SmoothLn(IoU(B^p_i,B^P_j))}{\sum_{i\ne j}1[IoU(B^p_i,B^P_j)&gt;0]+\varepsilon}<br>$$</p>
<p>上述公式可知，我们期望不同目标gt的B的IoU越小越好。RepBox Loss能够减少NMS导致的BBox被抑制的可能性。</p>
<h3 id="4-1-讨论"><a href="#4-1-讨论" class="headerlink" title="4.1 讨论"></a>4.1 讨论</h3><p><strong>距离函数</strong> 值得注意的是，我们在Rep项中选择IoG和IoU作为距离函数，而不是smoothL1，因为IoG和IoU的大小在[0,1]，而smoothL1不是，因此如果用在RepGT中其会导致bbox离gt越远越好。IoG只要求其交集达到最小即可。这更符合我们的期望。采用IoG而不是IoU，因为采用IoU会引导回归的bbox尺寸变大。</p>
<p><strong>平滑参数$\sigma$</strong> 不同于论文[^29]里直接采用$-ln(IoU)$的作为损失函数，我们提出平滑的ln函数smoothln。该参数的作用是调节函数对周围bbox或gt的敏感程度。由于bbox比gt密集的多，因此平滑参数在RepBox中小，在RepGT中大。</p>
<h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5. 实验"></a>5. 实验</h2><p>首先我们介绍实验设置和实现细节，在CityPerson上评估RepGT和RepLoss，</p>
<h3 id="5-1-实验设置"><a href="#5-1-实验设置" class="headerlink" title="5.1 实验设置"></a>5.1 实验设置</h3><p><strong>数据集</strong> CityPerson，Caltech-USA。我们在Zhang提供的新的标注上进行实验。</p>
<p><strong>训练细节</strong> 我们在自己构建的快速灵活的深度学习框架上实现</p>
<ul>
<li>lr : 0.016</li>
<li>iter : 80k(CityPerson), 160k(Caltech)</li>
<li>step : 60k(CityPerson), 120k(Caltech)</li>
<li>GAMMA : 10</li>
<li>weight decay : 0.0001</li>
<li>momentum : 0.9</li>
<li>Batch per GPU : 1</li>
<li>GPU : 4</li>
<li>使用 OHEM加速收敛</li>
</ul>
<h3 id="5-2-消融实验"><a href="#5-2-消融实验" class="headerlink" title="5.2 消融实验"></a>5.2 消融实验</h3><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Repulsion_Loss/t1t2.png" width="70%"/>

<p><strong>RepGT</strong> 参数$\sigma,\alpha,\beta$，当$\sigma&#x3D;1.0$时RepGT效果最好，相当于没有进行平滑处理。图3(a)显示了增加RepGT之后的MR的变化，在reasonable-crowd集上漏检显著下降，若以0.5作为阈值，那么漏检率下降了10%。如图3(b)所示，RepGT产生的虚警更少。</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Repulsion_Loss/f6.png" width="70%"/>

<p><strong>RepBox</strong> 当$\sigma&#x3D;0$时效果最好，即完全平滑，因为BBox比GT密集的多。NMS高导致MR高，NMS低导致FP高。图6显示，在各种NMS下RepBox更好。如图7所示，在gt之间的bbox的数量更少。</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Repulsion_Loss/f7.png" width="70%"/>

<p><strong>平衡RepGT和RepBox</strong> 根据表2所示，最优的参数是0.5。</p>
<h3 id="5-3-比较SOTA方法"><a href="#5-3-比较SOTA方法" class="headerlink" title="5.3 比较SOTA方法"></a>5.3 比较SOTA方法</h3><p>为了展示在不同遮挡情况下的有效性，将数据集分为：</p>
<ul>
<li>reasonable，≤35%</li>
<li>reasonable-partial，10%&lt;occ≤35%</li>
<li>bare，occ≤10%</li>
<li>heavy，occ&gt;35%</li>
</ul>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Repulsion_Loss/t3.png" width="70%"/>

<p>表3总结了RepLoss在CityPerson上的性能，实现了13.2MR，提升了1.4%，在Heavy集上，提升了3.7%，partial集上提升1.8%，在bare上午明显提升</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Repulsion_Loss/t4.png" width="70%"/>

<p>RepLoss在caltech集上的表现如图8和表4所示。在IoU&#x3D;0.5时，RepLoss实现了5.0的MR，并且在高IoU时提升更多。</p>
<h2 id="6-拓展：一般目标检测"><a href="#6-拓展：一般目标检测" class="headerlink" title="6. 拓展：一般目标检测"></a>6. 拓展：一般目标检测</h2><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Repulsion_Loss/t5.png" width="70%"/>

<p>RepLoss也可用于一般目标检测。我们在PASCAL VOC数据集上进行实验，Faster R-CNN (ResNet-101) 作为基准，NSM阈值为0.3，在trainval07和train12训练，test07测试，相比于baseline mAP提升了3.4。如表5所示，RepLoss对整体的提升不大，但是对crowd subset，提升了2.1 mAP。说明本文提出的方法可以拓展到一般目标检测。</p>
<h2 id="7-结论"><a href="#7-结论" class="headerlink" title="7. 结论"></a>7. 结论</h2><p>本文，我们设计 replusion loss 用于行人检测，其提升了人群场景的检测性能。主要的想法是基于目标吸引项的loss对于训练一个最优的检测器是不够的，增加周围的对抗项是必要的。</p>
<p>为了实现对抗项，我们提出了两类对抗损失，在CityPerson和Caltech两个数据集上均实现了最优的性能。详细的实验证明了RepLoss的价值，并且可以用于一般目标检测，我们期望该loss能广泛用于其他目标检测任务。</p>
<p>[^21]: J. Mao, T. Xiao, Y. Jiang, and Z. Cao. What can help pedestrian detection? In CVPR, 2017.<br>[^31]: S. Zhang, R. Benenson, M. Omran, J. Hosang, and B. Schiele. How far are we from solving pedestrian detection? In CVPR, 2016.</p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Pedestrian Detection</category>
      </categories>
      <tags>
        <tag>Pedestrian Detection</tag>
        <tag>NMS</tag>
        <tag>Repulsion Loss</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记 Individualness and Determinantal Point Processes for Pedestrian Detection</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Pedestrian-Detection/Note-Individualness-and-Determinantal-Point-Processes-for-Pedestrian-Detection/</url>
    <content><![CDATA[<ul>
<li>机构：首尔大学，加州大学</li>
<li>行人检测后处理，个体性估计</li>
<li>代码:<a href="http://cpslab.snu.ac.kr/software">http://cpslab.snu.ac.kr/software</a></li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文，我们介绍了检测候选的个体性作为一个补充用于评价行人检测。个体性从目标推荐和滑窗得到的原始检测候选中分配一个检测结果给每个物体。我们表明，传统的方法，如NMS，是次优的，因为其仅基于周围检测结果的得分来抑制。我们使用行列式点过程(determinantal point process)结合个体性优化选择最后的结果。该方法使用质量和相似性对每个检测结果建模。然后，检测结果中具有高检测得分和低相关性（通过矩阵行列式计算概率）的作为最后的结果。此矩阵由quality terms作为对角元素，其他位置是相似性元素。具体地，我们专注于行人检测问题，该问题由于其频繁的遮挡和不可预知的运动使其称为最具挑战的问题之一。实验结果显示本文提出的方法比NMS和不受约束的二次优化问题更好。</p>
<span id="more"></span>

<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>目标检测的目的是在图像中定位一个已知类别的目标。这对于许多视觉任务的基础，例如跟踪、场景识别、动作识别等。在视觉跟踪中，基于检测的跟踪是在连续图像中基于连续的检测结果定位目标，是非常有效的方法[^1]。通过定位图像中目标的位置，我们能更好理解场景中发生的事情[^2]。目标检测也应用于动作识别，通过发现一系列特定相关的动作序列[^3]。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1511942386442.png" alt="1511942386442"></p>
<p>一般目标检测的框架在测试图像上进行滑窗或者通过训练的分类器推荐目标。目前已经有许多目标检测器能够将独立的目标检测好。然而，一个检测器对每个目标的检测结果周围都会产生大量的原始检测结果（raw detection responses），如图1a所示。冗余的检测结果一般通过贪心算法进行抑制得到图1b，如NMS。</p>
<p>采用上述框架，很难检测高度被遮挡的目标。因为分类器在训练时设计为用于分辨不同的类别，而不是分别类内差异。例如图1中A、B行人外接矩形高度重叠，我们检测时需要判断是FP还是FN。基于NMS，由于A被遮挡，其检测得分会低于B，因此会被抑制。因此，NMS方法不可避免的产生一些FN，当目标相互遮挡时。另一方面，如果一些先验知识能够利用，例如不同的identities，这样的问题就能避免。事实上，false rejection能够极大影响检测精度，例如原始的检测结果召回率为90%，经过NMS之后检测结果仅为50%。因此，从原始检测结果挑选是一件重要的任务。</p>
<p>本文，我们提出一个算法基于个体性（individualness）和行列式点过程（determinantal point process, DPP）用于精确检测，并且可以应用于任何目标检测器。该方法可以作为目标性检测的一个独立补充。物体性检测得到一组候选结果，个体性检测发现候选结果的关系，得到最终检测结果。我们定义个体性使用特征向量的相关性，其包含bbox中的外观和空间信息。具体的，我们关注人群场景中的多行人检测问题。</p>
<p>DPP是一个随机过程，使用量子物理理论中的互斥粒子模型，其禁止高度相关的量子态同时出现。这个特性很适合排除冗余的检测结果。为了应用DPP，需要定义质量和多样性因子，通常为unary score和pair-wise correlation。基于此，我们可以选择一个更优化的子集如图1c所示。</p>
<p>本文的贡献如下所示：</p>
<ol>
<li>现有检测框架中，从候选检测结果中选择最后结果的方法存在问题</li>
<li>引入DPP增强检测精度，通过设计质量和多样性特征设计</li>
<li>用DPP选择优化的检测结果，并在多个行人数据集上验证。</li>
<li>在PETS 2009上，DPM基于本文方法达到41.9%的准确度和99%的精度，而NMS只达到23.2%的准确度和98.2%的精度。</li>
<li>在30个行人的图像上处理超过300个候选结果的时间少于30ms</li>
</ol>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>行人检测方法：HOG、SVM、DPM、Boosting-Based，Faster R-CNN</p>
<p>融合检测结果：NMS。</p>
<ol>
<li>文献[^13]指出定位精确性与检测得分没有很强的相关性。因此其提出一个回归模型学习检测结果与GT的相对位置。</li>
<li>文献[^14]将NMS框架整合到深度学习模型中，但是NMS的参数在训练时仍然是固定的。</li>
</ol>
<p>NMS可以看为优化问题：</p>
<ol>
<li>文献[^15]提出二次无约束二元优化方法（quadratic unconstrained binary optimization，QUBO）替代NMS。QUBO的目的是找到一个二元向量，其中每个元素表示对应的检测BBox是否应该被抑制。目标函数包含一元和二元项。一元项测量BBox是行人的置信度，二元项基于BBox的重叠区域进行惩罚。目标函数使用贪心算法求解。QUBO的问题在于其估计行人之间的分布使用二元目标函数。</li>
<li>文献[^16]提出基于吸引力传播聚类（affinity propagation clustering，APC）的方法。统计BBox两辆之间的相似性，然后聚类相似性最大的。 但是APC没有显式的惩罚相互接近的物体。尽管可以增加互斥函数进行改进，但是检测精度也没用显著的增加。</li>
</ol>
<h2 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h2><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512616433920.png" alt="1512616433920"></p>
<p>提出的方法包含连个阶段：物体性检测和个体性检测。本文使用DDP建模BBox之间的关系。</p>
<h3 id="3-1-行列式点过程方程（Determinantal-Point-Process-Formulation）"><a href="#3-1-行列式点过程方程（Determinantal-Point-Process-Formulation）" class="headerlink" title="3.1 行列式点过程方程（Determinantal Point Process Formulation）"></a>3.1 行列式点过程方程（Determinantal Point Process Formulation）</h3><p>定义：</p>
<ul>
<li>DPP基于BBox的质量和相似性计算最终选择的BBox</li>
<li>有个N个BBox，$y$表示所有BBox的集合，$Y$标示最终选择的子集</li>
<li>BBox的质量为$q_i$</li>
<li>BBox的相似性为$S_{ij}$，两个BBox向量的内积</li>
<li>计算正-半正定核矩阵（positive-semidefinite kernel matrix）$L_Y&#x3D;[L_{ij}]_{i,j\in Y}$</li>
<li><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512620048441.png" alt="1512620048441"></li>
</ul>
<p>则选择BBox的过程即为最优化问题</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512619743792.png" alt="1512619743792"></p>
<p>通常来说这是一个NP-hard（non-deterministic polynomial）问题，需要对所以的可能进行评估，但是幸运的是该问题是log-submodular问题，可以通过简单的贪心算法估计。</p>
<blockquote>
<p><a href="https://baike.baidu.com/item/NP-hard">百度</a><br>NP-hard，其中，NP是指非确定性多项式（non-deterministic polynomial，缩写NP）。所谓的非确定性是指，可用一定数量的运算去解决多项式时间内可解决的问题。NP-hard问题通俗来说是其解的正确性能够被“很容易检查”的问题，这里“很容易检查”指的是存在一个多项式检查算法。相应的，若NP中所有问题到某一个问题是图灵可归约的，则该问题为NP困难问题。</p>
</blockquote>
<p>例如图，当选择$Y&#x3D;{i,j}$时，DPP选择其的得分$P_L(Y)$为</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512621004358.png" alt="1512621004358"></p>
<p>可以发现，当$q$增大时行列式的值增加，当$|S_{i,j}|$增大时，行列式的值减小。因此DPP过程自动的选择高质量和低相似性的组合。</p>
<h3 id="3-2-质量项"><a href="#3-2-质量项" class="headerlink" title="3.2 质量项"></a>3.2 质量项</h3><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512629712724.png" alt="1512629712724"></p>
<p>一般来说质量项就是检测得分，但是原始得分是相互独立的。我们提出一个检测策略考虑周围结果的关系。一个图像中的虚警可能会抑制真正行人BBox。这种问题在BBox很大时更严重，统计一个GT中具有的检测BBox数量如图3b所示。可以发现大部分GT中包含的BBox非常少（这里有个问题，是不是本来滑窗ROI的尺寸本来就比较大，然后0也被统计进去了）。基于这个发现，提出重得分函数</p>
<p>$$s_i^c&#x3D;s_i^oexp(-\lambda n_i)$$</p>
<p>$\lambda$是一个常数。并且这样还可以得到更加贴紧行人的BBox，如第四节所示。（这个想法很特别，而且似乎特别适合二元检测任务，因为根据透视原理，不可能在一个大目标前面还存在多个小目标）。</p>
<p>特别地，在相机固定的场景中，行人的高度变化不大，而且高度$h_i$与位置$(x_i,y_i)$存在函数关系，可以容易的拟合出函数的系数：</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512630353869.png" alt="1512630353869"></p>
<p>假设行人的身高成高斯分布，我们根据BBox的偏差来对其进行重打分：</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512630798943.png" alt="1512630798943"></p>
<p>最后质量项$q$表示为：</p>
<p>$$q&#x3D;\alpha s + \beta$$</p>
<p>权重的设置需要根据检测器来确定，比如DPM的平均得分为0.7，ACF的得分是33.2。</p>
<h3 id="3-3-个体性和多样性特征"><a href="#3-3-个体性和多样性特征" class="headerlink" title="3.3 个体性和多样性特征"></a>3.3 个体性和多样性特征</h3><p>个体性旨在确定两个图像是否是指同一个人。类似多相机环境中的行人身份再识别问题。但是这两个问题不同且难度不一样。</p>
<ol>
<li>重叠区域有完全相同的信息</li>
<li>两个图像快交叠靠的很近</li>
<li>需要考虑被遮挡的行人，再识别问题通常不考虑</li>
</ol>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512631315204.png" alt="1512631315204"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512633314567.png" alt="1512633314567"></p>
<p>为了克服这个问题，我们考虑测量BBox特征描述的相关性。特征应该对背景、尺度不敏感。为了实现这个目的，我们采用卷积特征。总体上，相关矩阵是块对角并且个体相关性较低，因此我们决定采用CNN特征。</p>
<p>仅只用CNN不够有效，如图4所示，对单个行人有多个聚类结果，例如图5中间男人BBox右边有个BBox，尽管两个BBox中有轮廓不一样，但是仍然是一个行人。为了解决这个问题，我们增加考虑BBox的控制位置。空间个体性设计为单个行人周围的BBox具有高相关性。则第i个检测结果的个体性表示为：</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512635866556.png" alt="1512635866556"></p>
<p>k表示像素序号，$\pi_i$表示属于检测结果i的bbox的像素集合。尽管$\varphi_i$的纬度等同于图像尺寸，但是两个BBox的空间相关性可以检测计算得到:</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512636179348.png" alt="1512636179348"></p>
<p>同时不需要存储完整的相关性向量，只需保存检测的BBox尺寸和重叠面积：</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512636263251.png" alt="1512636263251"></p>
<p>（晕，这不就是IOU么）</p>
<p>得到了BBox的物体性特征$\phi_i$和$\varphi_i$，如何融合两个特征为一个多样性特征？可以采用平均值，但是其要求特征具有相同纬度。我们提出一个更通用有效的方法设计多样性特征，并直接构造正-半正定相似性矩阵S。$S^c$，$S^s$分别为有两个特征构建，换句话说$S_{ij}^c&#x3D;\phi_i^T\phi_j$。然后使用一个平和权重（0.8）融合：</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512636884079.png" alt="1512636884079"></p>
<h3 id="3-4-Mode-Finding"><a href="#3-4-Mode-Finding" class="headerlink" title="3.4 Mode Finding"></a>3.4 Mode Finding</h3><p>这一节描述如何求解上面的最优化函数。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512637471602.png" alt="1512637471602"></p>
<ol>
<li>每次选择最大的j</li>
<li>加入到Y集合中</li>
<li>计算P，若增加倍数超过阈值，则继续，并将j从y中删除</li>
<li>直到P不再增加，或y为空</li>
</ol>
<h3 id="3-5-与QUBO的关系"><a href="#3-5-与QUBO的关系" class="headerlink" title="3.5 与QUBO的关系"></a>3.5 与QUBO的关系</h3><p>QUBO使用DPP目标函数转换为相似的形式。DPP是寻找最大化$L_Y$的行列式。QUBO有两个缺点</p>
<ol>
<li>QUBO cannot deal with positively correlated items. 这句话不理解</li>
<li>QUBO 更加惩罚高度相关的BBox，可能不适合遮挡的行人。</li>
</ol>
<p>假设$L_y&#x3D;[2,-0.8;-0.8,1.4]$，QUBO将不选择第二个检测结果因为$-0.8-0.8+1.4&#x3D;-0.2&lt;0$，在DPP中$det(Y_L)&#x3D;2.16&gt;2$ 因此会选择第二个结果。</p>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h2><p>数据集</p>
<ul>
<li>INRIA：288个图像</li>
<li>PETS2009：S1.L1的190帧，平均每帧33个行人，行人较密</li>
<li>EPFL Terrace：5010帧，每25帧取1帧，平均每帧5人</li>
</ul>
<p>模型：DPM，ACF，Faster RCNN</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512654724175.png" alt="1512654724175"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512654762753.png" alt="1512654762753"></p>
<p>表1和图7显示结果，可以看见DPP都比NMS好或一致，并且在PET2009这类数据集上有显著提升。并且RCNN这类方法在INRIA上有较好的效果，但是在PET2009上效果不好，可能是因为区域推荐的方法会生成很多重叠的框，而不是独立个体的框。上述实验不会使用先验知识。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/IndividualnessDPP/1512655923604.png" alt="1512655923604"></p>
<p>图8a里面有贪心NMS，另一种为特殊NMS$\frac{area(d_g\bigcap d_e)}{min(area(d_g),area(d_e))}&gt;0.65$ 能够实现更好的精度。非贪心NMS不排除被抑制的BBox。QUBO的精度与非贪心NMS相似。并且DPP比其他的都显著改善。</p>
<p>图8b表示了CNN网络层的有效性。显示，检测结果对不同的网络层不敏感。我们使用4096维向量用于计算多样性特征。</p>
<p>图8c显示了计算速度。最多300个proposal，只统计算法1的计算时间。卷积计算时间248ms。计算平台Intel Xeon 2.3 GHz，128 RAM，TITAN X D5 12GB CPU。处理过程平均少于30ms。</p>
<p>BBox定位精度使用$\frac{|d_e\bigcap d_g|}{|d_e|}$测量。PET2009中DPP是0.81，NMS是0.76。图9显示一些结果示例。DPP产生更贴紧的BBox，如EPFL-84帧，NMS和QUBO都漏检了中间穿白衣服的人，DPP成功检测。</p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h2><p>我们提出一种基于个体性改进检测性能的算法。个体性评价两个检测候选的相似性，物体性基于得分生成候选框。每个BBox外观和空间信息用于考虑个体性。然后行列式点处理过程结合得分和相似性得到最后结果。实验结果表明DPP比NMS和QUBO更好。并且在平均包含30个行人的图像上处理300个候选框的速度小于30ms。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[^13]: Liu, S., Lu, C., Jia, J.: Box aggregation for proposal decimation: last mile of object detection. In ICCV, pp. 2569–2577 (2015)<br>[^14]: Wan, L., Eigen, D., Fergus, R.: End-to-end integration of a convolution network, deformable partsmodel and non-maximum suppression. In CVPR, pp. 851–859 (2015)<br>[^15]: Rujikietgumjorn, S., Collins, R.T.: Optimized pedestrian detection for multiple and occluded people. In CVPR (2013)<br>[^16]: Rothe, R.,Guillaumin,M., Gool, L.V.: Non-maximum suppression for object detec- tion by passing messages between windows. In ACCV (2014)</p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Pedestrian Detection</category>
      </categories>
      <tags>
        <tag>Post Processing</tag>
      </tags>
  </entry>
  <entry>
    <title>Note-How Far are We from Solving Pedestrian Detection?</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Pedestrian-Detection/Note-How-Far-are-We-from-Solving-Pedestrian-Detection/</url>
    <content><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>在近期行人检测进展的鼓舞下，我们研究了现有方法与“完美单帧检测器”之间的差距。基于Caltech数据集创建了一个人工的基准，并且手工聚合了在顶级检测器中经常发生的错误以实现研究分析工作。研究结果刻画了定位错误、背景vs前景方面错误（虚警和漏检）。</p>
<p>针对定位错误，我们研究了训练集标记噪声对检测器性能的影响，结果表明仅使用小部分干净的训练数据都能使检测器的性能得到提升。针对虚警&#x2F;漏检的情况，我们研究了应用在行人检测中的卷积神经网络，并且讨论了哪些因素影响了它们的性能。</p>
<p>除了深入的分析，我们汇报了Caltech数据集的最佳性能，并且提出了一个新的、纯净的训练&#x2F;测试标注集。</p>
<span id="more"></span>

<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>目标检测近年来收到了很大的关注。行人检测是其中一个典型的子问题，且由于多方面的应用，它仍然是研究的一个热点方向。</p>
<p>尽管在行人检测方面存在大量的研究工作，但是近期的论文仍然表现出很大的进展程度，这说明研究工作的饱和点仍未达到。在本文中，我们分析了现有技术与新创建的人工基准（section 3.1）之间的差距。结果表明，仍可以成十倍地提高技术来达到人工标注的性能。我们致力于探究哪些因素可以帮助缩小这一差距。</p>
<p>我们分析了拥有顶级性能行人检测器的失败案例，判断进一步推进性能需要改进的部分。分析方法有多种，包括人工检查，对有问题的案例进行自动分析，以及oracle实验（section3.2）。分析结果表明定位是高置信度虚警的重要来源。我们通过人工清理Caltech训练标注集和使用算法来处理剩余的训练样本，来提高训练集校准的质量以解决这一问题（section 3.3 and 4.1）。针对前景和背景的区别，我们研究了行人检测的卷积神经网络并且讨论了哪些因素影响了它们的性能（section 4.2）。</p>
<h3 id="1-1-Related-work"><a href="#1-1-Related-work" class="headerlink" title="1.1. Related work"></a>1.1. Related work</h3><p>随着ICF（integralchannel feature，积分通道特征）检测器[6,5]的成功，许多变体[22,23,16,18]被人们提出并且显示出很大的提高。近期行人检测相关综述总结道被改进的特征一直在促进性能的提高并且很有可能继续发挥这样的作用。同时指出光流和环境信息作为图像特征的补充，将能够促进检测准确度的提高。</p>
<p>通过对使用外部数据进行预训练的模型进行微调，卷积神经网络也可以达到现有的最先进性能[15,20]。</p>
<p>近期大部分论文着眼于介绍新颖的，更好的结果，却忽视了对产生结果的系统进行的分析。有些分析工作可以建立在通用目标检测之上；与此相反，在行人检测领域，此类分析工作很少有人进行。在2008年，[21]提供了对INRIA数据集的错误分析（小部分）。在2012年的Caltech数据集调查[7]中提到的最佳方法的虚警率是本文方法的十几倍，召回率为20%，而且当时没有方法的检测率能达到95%。</p>
<p>由于近年来行人检测取得了较大成果，为了更好的理解未来研究的最佳切入点，基于现有检测器的一种更加深入、全面的分析是很有价值的。</p>
<h3 id="1-2-Contributions"><a href="#1-2-Contributions" class="headerlink" title="1.2. Contributions"></a>1.2. Contributions</h3><p>（a）提供了对现有行人检测系统的详细分析，洞察了失败案例。</p>
<p>（b）为Caltech行人基准提供了人工基准（human baseline），以及纯净版本的标注，用于为基准的训练和测试集提供新的、高质量的真实值。数据将会被公开。</p>
<p>（c）分析了训练数据的质量对检测器效果的影响大小。更具体的说，我们量化了更好地校准和更少的标注错误对于检测性能的提升。</p>
<p>（d）通过洞察分析，我们探索了几种获得最佳性能方法的变体：滤波通道特征检测器（filteredchannel feature detector）[23]和R-CNN检测器[13,15]，并且在基线之上显现出了性能的提升。</p>
<h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2. Preliminaries"></a>2. Preliminaries</h2><p>数据集、度量、基准检测器的描述</p>
<h3 id="2-1-Caltech-USA-pedestrian-detectionbenchmark"><a href="#2-1-Caltech-USA-pedestrian-detectionbenchmark" class="headerlink" title="2.1. Caltech-USA pedestrian detectionbenchmark"></a>2.1. Caltech-USA pedestrian detectionbenchmark</h3><p>最流行的数据集：Caltech-USA[7]、KITTI[11]</p>
<p>本次研究工作针对Caltech-USA基准数据集：</p>
<ul>
<li>2.5 hours 30Hz采集自LA的街道</li>
<li>共有350,000个标注框，约2300个不同的行人</li>
<li>检测方法基于包含4024帧的测试集进行评估</li>
<li>根据标注尺寸、遮挡程度和长宽比，分为不同子集</li>
<li>规定的训练过程是每隔30帧取一帧，一共有4250帧，约1600个行人</li>
</ul>
<p>最近有方法[16,23]采用了更精细的视频采样方法从而可以使用更多的数据进行训练，与普通的设置相比，产生的训练数据高达十倍。</p>
<h3 id="2-2-Filtered-channel-features-detector"><a href="#2-2-Filtered-channel-features-detector" class="headerlink" title="2.2. Filtered channel features detector"></a>2.2. Filtered channel features detector</h3><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image002.jpg"></p>
<ul>
<li>截止到CVPR2015，最好的方法是Checkerboards</li>
<li>Checkerboards是ICF的一种，将HOG+LUV特征通道过滤后送入增强的随机森林。</li>
</ul>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image004.jpg"></p>
<ul>
<li>目前最好的卷积神经网络方法[15,20]对底层检测方法很敏感，所以首先关注优化滤波通道特征检测器的方法。（详见section 4.2）</li>
</ul>
<p><strong>Rotatedfilters</strong></p>
<p>关于训练新模型的实验（section 4.1），我们基于LDCF实现了自己的Checkerboards。为了减少训练耗时，我们将滤波器的数量从原来实现版本中的61减少到了9。RotatedFileters是LDCF的简化版本，而且从上方表格中可以看出它相比LDCF提升了很多，并且只比Checkerboards方法的漏检率高了一个百分点，然而训练和测试时间快了六倍。</p>
<p><strong>Additionalcues</strong></p>
<p>综述[3]表明环境上下文信息和光流信息对检测也有所帮助。然而，随着检测器质量的提升，从这两种特征获得的结果会受到侵蚀。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image006.jpg"></p>
<h2 id="3-Analysing-thestate-of-the-art"><a href="#3-Analysing-thestate-of-the-art" class="headerlink" title="3. Analysing thestate of the art"></a>3. Analysing thestate of the art</h2><h3 id="3-1-Are-we-reaching-saturation"><a href="#3-1-Are-we-reaching-saturation" class="headerlink" title="3.1.Are we reaching saturation?"></a>3.1.Are we reaching saturation?</h3><p>问：目前的基准还有多大的发展空间？</p>
<p>为了回答这个问题，我们提出了一种人工的基线作为下界。我们让领域专家在Caltech-USA测试集中手工“检测”行人；如此，机器检测算法应当至少达到人工检测的性能并且最终达到超过这一性能的水平。</p>
<p><strong>Humanbaseline protocol</strong></p>
<p>为了确保与现有检测器之间比较的公平性，我们着眼于单帧单目检测的设置。这些视频帧以随机的（且不连续）顺序被标注，标注者只能依靠行人外观和提供的仅一帧图像中的环境上下文信息来进行标注。</p>
<ul>
<li>Caltech基准对检测框的长宽比进行了统一</li>
<li>手工标注：从人的头顶画一条直线至两脚中间，然后自动生成标注框，这样可以确保标注框和检测对象的中心一致</li>
</ul>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image008.jpg"></p>
<ul>
<li>为了检查两组标注是否一致，对测试图像的子集（约10%）中的标注进行了复制，再分别进行评估。采用IoU≥0.5的匹配标准，结果与采用单个标注框相同。</li>
</ul>
<p><strong>Conclusion</strong></p>
<p>我们将人工基线与其他有顶级性能的方法在测试数据的不同子集上进行了比较。我们发现人工基线在所有的情况下都远比先进检测器的性能更好，说明自动检测方法仍然有很大的提升空间。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image010.jpg"></p>
<h3 id="3-2-Failure-analysis"><a href="#3-2-Failure-analysis" class="headerlink" title="3.2.Failure analysis"></a>3.2.Failure analysis</h3><p>问：为什么自动检测方法会失败？</p>
<p>本节分析Checkerboards检测发生错误的原因（代表其他高性能检测器），因为大部分顶级方法都是来自ICF族的，所以它们的情况应该相似，包括基于ICF检测器的卷积神经网络方法。</p>
<h4 id="3-2-1Error-sources"><a href="#3-2-1Error-sources" class="headerlink" title="3.2.1Error sources"></a>3.2.1Error sources</h4><p>检测器产生的错误类型：</p>
<ul>
<li>false positive虚警</li>
<li>false negatives漏检</li>
</ul>
<p>在这次分析中，关注FPPI为0.1条件下的虚警和漏检情况，并且人工将它们聚类为不同的组别：</p>
<ul>
<li>402个虚警</li>
<li>148个漏检</li>
<li>这些虚警和漏检情况按照错误的类型分至了不同的类别下</li>
</ul>
<p><strong>Falsepositives</strong></p>
<p>402个虚警分为了以下11个类别，产生原因概括为定位&#x2F;背景&#x2F;标注错误三种：</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image012.jpg"></p>
<ul>
<li>其中背景造成的虚警占比最大，具有垂直结构的物体造成干扰最大。这说明检测器需要扩展具有更好的垂直上下文，对大物体提供更好的可视性以及粗略的高度估计。</li>
<li>定位错误中重复检测造成虚警数占比最大，这说明改进后的检测器需要更多的定位反馈和&#x2F;或另一种非极大值抑制方法。在3.3节和4.1节中探究了如何提升检测器的定位能力。</li>
</ul>
<p><strong>Falsenegatives</strong></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image014.jpg"></p>
<p>典型原因有行人尺寸过小、行人处于侧身状态。假设side-view和cyclists得分低是由于数据集的偏差，但是训练集中这两种情况的对象不多（大部分人都是在行走的），也许向训练集中补充有这些特定对象的外部图像是一个有效的策略</p>
<p>为了更好地理解关于小尺寸行人的问题，我们测量了大小、模糊度和对比度。观察到：小尺寸的行人通常都存在曝光过度或不足的情况，而且十分模糊，我们假设这是检测效果差的潜在原因（除像素数太少这个因素外）。但是我们的研究结果表明这个假设是错误的：</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image016.jpg"></p>
<p>从上图看来，较低的检测得分和低对比度之间并无联系，针对模糊度的情况结果也是如此。所以检测小尺寸行人难度大的真正原因是像素的数量不足。所以要提升小尺寸对象检测率需要尽可能利用所有可以利用的像素，不仅是检测窗口内的像素，还有周围环境以及随时间变化可利用的像素。</p>
<p><strong>Conclusion</strong></p>
<p>已经确定来源的虚警问题可以按照上文提及的方法针对性的解决</p>
<p>部分漏检问题在上文中提出了解决方案，但是小尺寸和被遮挡行人的检测依然存在很大的困难需要克服。 </p>
<h4 id="3-2-2-Oracle-test-cases"><a href="#3-2-2-Oracle-test-cases" class="headerlink" title="3.2.2 Oracle test cases"></a>3.2.2 Oracle test cases</h4><p>An oracle experiment is used to compareyour actual system to how your system would behave if some component of italways did the right thing.</p>
<p>本节通过oracle实验衡量定位&#x2F;背景vs前景错误对检测质量指标（log-average miss-rate）的影响</p>
<ul>
<li>针对定位错误：所有与真实值重叠的虚警都被忽略，不计入评估</li>
<li>针对前景vs背景错误：所有<strong>未与</strong>真实值重叠的虚警都被忽略</li>
</ul>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image018.jpg"></p>
<p>从上图可以看出修复了定位错误可以在FPPI低的区域提升性能；修复了背景错误可以在FPPI高的区域提升性能。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image020.jpg"></p>
<p>上图反映了8种顶级性能检测器在修正了定位或背景问题情况下检测效果的提升。在对比这几种检测器的过程中，我们发现大部分方法通过对这两种问题都进行修复，会在很大程度上增强检测器的性能。</p>
<p><strong>Conclusion</strong></p>
<p>对于大多数拥有顶级性能的方法来说，定位问题和背景&#x2F;前景问题对于检测的质量同等重要。</p>
<h3 id="3-3-Improved-Caltech-USA-annotations"><a href="#3-3-Improved-Caltech-USA-annotations" class="headerlink" title="3.3 Improved Caltech-USA annotations"></a>3.3 Improved Caltech-USA annotations</h3><p>改进Caltech数据集标注方法：</p>
<ul>
<li>对现有检测器进行更好地评估</li>
<li>评估改进的标注方法能在多大程度上提升检测器性能</li>
</ul>
<p><strong>New annotation protocol</strong></p>
<ul>
<li>对训练集和测试集都进行了标注</li>
<li>在human baseline中忽略的区域和遮挡都被标注了</li>
<li>标注者允许通过整个视频来判断对象是不是人</li>
<li>允许对同一图像进行多次修改</li>
</ul>
<p>纠正了原有标注的错误：</p>
<ul>
<li>未校准</li>
<li>漏标</li>
<li>虚警</li>
<li>使用“ignore”不恰当</li>
</ul>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image022.jpg"></p>
<p><strong>Betteralignment</strong></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image024.jpg"></p>
<p>（O代表原有标注，N代表改进的标注）</p>
<p>由上图可见：</p>
<ul>
<li>只有使用原有标注进行训练的模型有更好的效果</li>
<li>改进的标注方法有更好的校准效果</li>
</ul>
<h2 id="4-改进现有技术"><a href="#4-改进现有技术" class="headerlink" title="4. 改进现有技术"></a>4. 改进现有技术</h2><p>在这一节中我们利用分析的结果来改进基线检测器的定位和背景与前景的辨别。</p>
<h3 id="4-1-训练标注的影响"><a href="#4-1-训练标注的影响" class="headerlink" title="4.1. 训练标注的影响"></a>4.1. 训练标注的影响</h3><p>我们想要利用新的标注来了解标注质量对检测质量的影响。首先使用不同的训练集来训练ACF[5]和RotatedFilters模型（在2.2节中介绍），并分别在原始的和新的标注上进行评估(例如$MR_{-2}^O,MR_{-4}^O,MR_{-2}^N,MR_{-4}^N$)。注意两个检测器都使用boosting进行训练，因此对标注噪声很敏感。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image028.jpg"></p>
<p><strong>修剪收益</strong> 表4展示了使用原始的，新的和修剪的标注训练的结果（在整个训练集上进行5&#x2F;6和1&#x2F;6的训练和验证划分）。如预期一样，在原始的&#x2F;新的标注上训练并在原始的&#x2F;新的标注上测试的模型性能优于在不同的标注上训练和测试的模型。为了更好的了解新的标注的效果，我们构建了一组混合的标注。修剪的标注是允许减弱移除错误和改进校准的效果的中间点。</p>
<p>通过匹配新的和原始的标注来生成修剪标注（IoU $\ge$ 0.5）, 并将存在在原始标注中却不在新的标注中的部分记为忽略区域，同时添加存在在新的标注中却不在原始标注中的部分。</p>
<p>从原始标注到修剪标注，主要做的改变是移除标注错误，而新的标注相对于修建的标注有了更好的校准。从表4中可以看出，ACF和RotatedFilters都通过移除标注错误提高了性能。这表明新的训练集比原始训练集更加纯净。</p>
<p>从 $M_{-2}^N$ 中可以看出更强大的检测器可以更好的从数据中受益，并可以通过移除标注错误来获得最大的检测器质量提升。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image034.jpg"></p>
<p><strong>校准收益</strong> ICF族的检测器可以通过增加的训练数据来获得提升[16, 23]，使用10倍的数据要比使用1倍的数据要好（见2.1节）。为了使用新的1倍的标注来利用9倍的剩余数据，我们在新标注上训练了一个模型并使用该模型对9倍部分的原始标注进行重新校准。因为新标注有更好的校准，我们希望这个模型能改进原始标注中轻微的位置和尺寸错误。图8展示了该过程的一些结果。可以在补充材料中获取更多的细节。</p>
<p>表5展示了使用自动校准过程的结果和一些退化案例：使用原始的10×，使用通过原始的10×训练的模型自校准原始的10×，仅使用新标注的一部分校准原始10×（不更换1×部分）。结果表明使用检测器来改进总体数据校准十分有效，校准更好的训练数据会有更好的检测质量（在$MR^O$和 $MR^N$ 上都是如此）。这符合3.2节的分析。使用在1&#x2F;2新标注上训练的模型来校准比使用原始标注获得的模型更强大。</p>
<p>我们将使用新的标注和校准的9×数据训练的RotatedFilters模型称为Rotated-Filters-New10×。该模型在表3中也达到了high median true positives IoU，表明它在测试时确实获取了更高精度的检测。</p>
<p><strong>总结</strong> 无论是改进校准还是减少标注错误来提高标注质量，使用高质量的标注进行训练都会改进整体检测质量。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image040.jpg"></p>
<h3 id="4-2-用于行人检测的卷积神经网络"><a href="#4-2-用于行人检测的卷积神经网络" class="headerlink" title="4.2. 用于行人检测的卷积神经网络"></a>4.2. 用于行人检测的卷积神经网络</h3><p>3.2节的结果表明核心背景与前景的辨别任务（对象检测的分类部分）还有改进的余地。近来的工作[15,20]展示了使用卷积神经网络（convnets）来进行行人检测可以达到有竞争力的性能。我们也分析了卷积神经网络，并且探索了由检测提案质量影响的性能范围。</p>
<p><strong>AlexNet</strong> 和 <strong>VGG</strong> 我们考虑了两种卷积神经网络。1）[15]中的AlexNet和2）[12]中的VGG16模型。这两个模型都在ImageNet上进行了预训练并且通过使用SquaresChnFtrs提案的Cattech10×（原始标注）进行微调。两个网络都死开源的，并且都是R-CNN架构[13]的实例。尽管他们的训练&#x2F;测试时架构略有不同（R-CN与Fast R-CNN），我们希望结果差异由其各自的辨别力来支配（VGG在Pacval检测任务中[13]通过AlexNet提高了8pp mAP）。</p>
<p>表6表示随着检测提案质量的提升，AlexNet却无法提供一致的提升，最终使ICF检测器的结果恶化（[15]中有相似的观测结果）。同样，VGG在较弱的提案上有大的提升，但随着提案的改进，卷积神经网络重新获得的提升最终停止。</p>
<p>仔细检查所得曲线后（见补充材料），我们注意到AlexNet和VGG都为背景实例分配更低的分数，同时生成大量高分虚警。ICF检测器可以提供高recall提案，其中对象周围的虚警会分配低分值（见[15,supp,material,fig.9]），然而，卷积神经网络很难对正样本周围的这些窗口分配低分值。换句话说，尽管进行了调整，卷积神经网络的得分图仍比proposal ones模糊。由于AlexNet和VGG架构的内部特征池，我们假设这是他们的内在限制。从卷积神经网络获取峰值相应很可能需要使用不同的架构，可能更像是用于语义标注或者边界估计任务这些需要精确像素输出的架构。</p>
<p>幸运的是，我们可以使用边界框回归来弥补卷积神经网络评分中缺乏的空间分辨率。在VGG上添加边界回归，并应用两轮非极大值抑制（第一次在提案上，第二次在回归的框上）有收缩（contract）得分图的效果。生成多个健壮虚警前的相邻提案现在收缩成单个高分值检测。在第二个NMS上使用通用的IoU<img src="file:///C:/Users/Zavix/AppData/Local/Temp/msohtmlclip1/01/clip_image030.gif"> 0.5的合并标准。</p>
<p>表6最后一列表明，即使对于最好的检测器RotatedFilters-New0×，在输入提案上使用边界框回归+NMS也能获得有效的提升。在原始标注RotatedFilters-New10×+VGG上达到14.7% $MR_{-2}^O$，其改善超过[15, 20]。</p>
<p>图9在卷积神经网络的结果上重复了Oracle测试。可以看出VGG明显降低了背景错误，同时些微增加了定位错误。</p>
<p><strong>总结</strong> 即使卷积神经网络在图像分类和通常的目标检测上与强大的结果，它们似乎在小物体周围产生良好的定位检测分值上有局限性。边框回归是在目前架构上回避这一限制的关键因素。即使使用强大的卷积神经网络，背景与前景的辨别仍是错误的主要来源；这表明神经网络的原始分类能力还有待改进。</p>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p>这篇论文致力于分析在Caltech数据集上顶级性能的检测器的错误。人工基准（human baseline）定义了性能提升的下限。但差距还需要缩小10多倍。为了在检测过程中更好的度量下一步，我们提供了新的纯净的Caltech训练和测试集标注。</p>
<p>在最佳性能方法上的失败分析表明大部分错误都有很好的表征。错误特征引导了设计更好的检测器的具体建议（在3.2节提到；例如用于人侧视图的数据增加，或者在垂直轴上扩展检测器接收区域）</p>
<p>通过衡量更好的标注对定位准确性的影响，以及通过调查使用卷积神经网络来改善前景和背景的区分，部分解决了一些问题。结果表明正确训练的ICF检测器可以实现更好的校准，且对于行人检测，卷积神经网络在定位上能力不强，但是可以通过边界框回归来部分解决。无论在原始的和新的标注上，描述的检测方法都达到了顶级性能，具体见表7.</p>
<p>我们希望该工作提供的见解和数据可以指导行人检测任务中对机器和人工之间差距的缩小。</p>
<p><strong>补充材料</strong></p>
<p><strong>A. 内容</strong></p>
<p>补充材料提供了主要论文中提到的一些方面的更详细的见解。</p>
<ul>
<li>Section B提供了实验中使用到的RotatedFilters检测器的细节（2.2节）</li>
<li>Section C提供了汇总不同的测试子集后的具体的曲线（图3和3.1节）</li>
<li>Section D显示了分析检测器的每个错误类型的示例，讨论了尺寸，模糊和对比度评估，并更详细地回顾了oracle案例实验（3.2节）</li>
<li>Section E展示了新的训练标注如何改进原始标注的示例（3.3节）</li>
<li>Section F讨论了新的标注对现有方法的影响（MP排名和recall-versus-IoUcurves）（4.1节）</li>
<li>Section G展示了使用1×数据自动校准10×数据的效果（4.1节）</li>
<li>图26总结了原始的和新的标注的最终检测结果</li>
</ul>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image045.jpg"></p>
<p><strong>B. Rotated filters检测器</strong></p>
<p>实验中使用LDCF[16]代码库重新实现了过滤通道特征Checkerboards检测器[23]。由于具有大量过滤器（每个通道61个过滤器），训练过程十分缓慢。为了加快训练和测试的过程，我们为每个通道设计9个过滤器，仍然取得了良好的性能。将新的过滤通道特征检测器称为RotatedFilters（见图11d）。</p>
<p>旋转过滤器由LDCF的滤波器（将PCA应用于每个特征通道获取）启发。每个特征通道的前三个过滤器是正交方向上的常量过滤器和两个阶梯函数，并具有定向梯度通道也有旋转过滤器的特性（见图11b）。旋转过滤器是LDCF的程序化版本。得到的RotatedFilters过滤器在某种程度上是直观的，而Checkerboards过滤器的功能没有其系统化和清晰。</p>
<p>为了整合更丰富的本地信息，与SquaresChnFtrs[3]一样，我们在每个通道上使用多种尺寸重复每个过滤器。</p>
<p>在Caltech验证集上，RotatedFilters使用一种尺寸（4×4）获取了31.6% $MR_{-2}^O$；使用三种尺寸（4×4，8×8和16×16）获取了28.9% $MR_{-2}^O$。因此在实验中选择了3尺寸的结构。在测试集上，RotatedFilters的性能是19.2% $MR_{-2}^O$，比Checkerboards减少了1%的损失，但在特征计算上比它快6倍。</p>
<p>在本文中，我们使用RotatedFilters进行所有涉及训练新模型的实验。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image048.jpg"></p>
<p><strong>C. 每个测试子集的结果</strong></p>
<p>图12包含主论文（“子集条形图”）中图3的详细曲线。可以看到Checkerboards和RotatedFilters在所有子集上都有很好的性能。在其排名不在前列的案例中（例如图12e和图12h）所有的方法都表现出较低的检测质量，因此都具有相似的差评。</p>
<p>图12显示在Caltech数据集中Checkerboards对于很多普通案例并不是最优的，但其在各种情况下都表现出了良好的性能，因此是一个有趣的分析方法。</p>
<p><strong>D. Checkerboards错误分析</strong></p>
<p><strong>错误示例</strong> 图17，18，19和20展示了主论文分析中考虑的每个错误类型的4个示例（包括虚警和负样本）。</p>
<p><strong>模糊和对比度测量</strong> 为了能够分析模糊和对比度，我们定义了两种自动化措施。使用[10]中的方法测量模糊，并使用行人顶部和底部灰度值的位数差来计算对比度。</p>
<p>图15和16展示了使用测量的模糊和对比度的行人排名。可以看出质量测量与模糊和对比度的定性概念有很好的相关性。</p>
<p><strong>尺寸，模糊或者对比度?</strong> 负样本错误的主要来源是小尺寸，但是小的行人通常具有低对比度或者模糊度。为了分析三个因素，我们观察了大小&#x2F;对比度&#x2F;模糊度与分数之间的相关性，如图14所示。可以看出虚警和正样本的重叠在不同层次的对比度和模糊度之间均匀分布；但是重叠在小尺寸上相当密集。因此小尺寸是影响检测质量的主要因子；而模糊度和对比度的测量并不能为检测任务提供信息。</p>
<p><strong>D.1. Oracle案例</strong></p>
<p>图21展示了现有方法的标准评测和oracle评测的曲线。在定位预测中，与真实值重叠的虚警不予考虑；在背景和前景辨别预测中，与真实值没有重叠的虚警不予考虑。基于这个曲线有如下发现：</p>
<ul>
<li>所有方法在每个oracle评估中都有显着改善。</li>
<li>所有方法的排名在每个oracle案例下都保持相对稳定</li>
<li>定位与背景有前景的oracle测试在$MR_{-4}^O$上的改进是相当的；检测性能可以通过固定两个问题中的一个来提高。</li>
</ul>
<p>图13中还展示了有相似分数的一些对象的示例。在低分组和高分组中，我们都可以看到行人和背景对象，这表明检测器不能充分地对前景和背景进行排序。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image052.jpg"></p>
<p><strong>D.2. 对数比例视觉失真</strong></p>
<p>本文展示的oracle实验的结果模拟了一种我们不会产生的错误：删除了触摸标注的行人错误（定位预测）或者位于背景上的错误（背景预测）。</p>
<p>要注意这是仅有的两种虚警。如果我们移除了这两种类型的错误，结果将是一个有非常低错误率的水平线。</p>
<p>由于Caltech性能曲线上的双对数比例看起来像是两个oracles都可以稍微改进性能，但是情况并非如此，大部分的错误都是由不同类型的错误引起的。</p>
<p>图22说明了双对数尺寸歪曲面积的多少。我们经常将平均错误率视为曲线下的面积，因此可以通过其类型对图中的虚警进行颜色编码：该图显示了每个点上定位（蓝色）和背景（绿色）错误之间的比率，这也是整个曲线的比率。22b和22c的两个曲线都展示了同样的数据，唯一的区别在于左侧展示了定位，右侧展示了背景。由于双对数尺寸，绘制在左边的错误类型似乎主导了度量</p>
<p><strong>E. 改进的标注</strong></p>
<p>图23在测试集中的示例帧上展示了原始（红色）和新的（绿色）标注。从比较中可以看出，新的注释与行人更加一致。这是由于头和脚更靠近新的边界框的中心。</p>
<p><strong>F. 原始标注和新的标注的评估</strong></p>
<p><strong>排名</strong> 图25显示了在 $MR_{-2}^O$ （原始标注）或 $MR_{-2}^N$（提出的新标注）上进行评估时，所有已有的Caltech方法在CVPR 2015之前的排名。虽然排名有一些变化（例如JointDeep与SDN），总体趋势仍然保持不变。改进的标注与之前的标注不是完全相反的，这是一个好的迹象。如论文（以及补充材料的其他部分）所讨论的，改进的标注对于未来的方法（MR进一步下降）以及曲线的低FPPI区域（高信度错误）至关重要。</p>
<p><strong>RotatedFilters</strong> 图26a和26b展示了RotatedFilters，Rotated-Filters-New10x和RotatedFilters-New10x-+VGG方法分别在原始的和新的标注上的结果。在训练期间使用改进的标注（-New10x）确实提高了原始和新标注的结果。</p>
<p><strong>MR</strong>与 <strong>IoU</strong>主论文的3.3节（和表3）讨论了新标注如何更好地校准的经验性措施。我们在这楼里提供更多的细节。图24绘制了顶级性能方法的 $MR_{-2}^O$ 和$MR_{-2}^N$ 和将检测作为正样本的重叠标准（IoU阈值）。标准评测使用0.5的IoU阈值。图中在INRIA上训练的方法是连续的线，在Caltech上训练的方法是虚线。</p>
<p>在图24（原始标注）中，由于重叠阈值十分严格，方法的排序保持稳定。（与[7]中观察结果一致）。有趣的是，图24b（新标注）可以观察到不同的趋势。在评测 $MR_{-2}^N$（新的标注）时，我们发现在INRIA上训练的方法，虽然在IoU&#x3D;0.5时表现不佳，但在较高的IoU下表现相对较好，最终超过了原始Caltech数据训练的所有方法。我们认为INRIA训练数据质量更好（校准更好的训练样本），因此检测器能更好的学习定位。原始和新标注之间的这种差异使得改进的标注在定位方面表现更好。主文章中的表3提供了图24的总结版本。</p>
<p><strong>G. 校准Caltech10×的影响</strong></p>
<p>从图24b中可以看出，使用半自动校准的Caltech10×训练数据显著提升了定位质量。从RotatedFilters到RotatedFilters-New10×，$MR_{-2}^N$在整个IoU范围内均有改进。图27显示了在10×训练数据上进行校准过程的定性结果。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image059.jpg"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image061.jpg"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image063.jpg"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image065.jpg"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image067.jpg"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image069.jpg"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image071.jpg"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image073.jpg"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image075.jpg"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image077.jpg"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image079.jpg"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image081.jpg"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2016-How_Far/clip_image083.jpg"></p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Pedestrian Detection</category>
      </categories>
      <tags>
        <tag>Pedestrian Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Note Seq-NMS for Video Object Detection</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Video-Object-Detection/Note-Seq-NMS-for-Video-Object-Detection/</url>
    <content><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>用于视频目标检测的NMS方法</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>视频目标检测困难的原因：</p>
<ul>
<li>较大的尺度变化</li>
<li>遮挡</li>
<li>运动模糊</li>
</ul>
<p>本文，我们提出单帧检测的一个简单拓展来帮助解决上述问题。</p>
<p>单帧检测完全忽略了时间维度，本文，我们在后处理阶段融合时序信息，以此优化每帧的检测结果。对于给定的时间序列上的ROI和类别得分，我们使用简单的重叠标准来连接邻近帧的BBox，使得序列的得分最大化。之后抑制附近的BBox，然后对BBox重打分。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515727263191.png" alt="1515727263191"></p>
<p>贡献：</p>
<ul>
<li>提出Seq-NMS改进用于视频视频数据的物体检测流程。特别地，我们改进了后处理阶段，使用前后帧的高分物体结果增强弱检测结果。</li>
<li>Seq-NMS在ImageNet VID上的表现超过先进的单帧检测结果。</li>
<li>方法在ILSVRC2015上排名第3</li>
</ul>
<span id="more"></span>

<h2 id="2-我们的方法"><a href="#2-我们的方法" class="headerlink" title="2. 我们的方法"></a>2. 我们的方法</h2><h3 id="2-1-Seq-NMS"><a href="#2-1-Seq-NMS" class="headerlink" title="2.1 Seq-NMS"></a>2.1 Seq-NMS</h3><p>NMS经常会选错BBox，选择的BBox通常较大，且与GT与较小的IOU。大BBox经常有较高的物体得分，可能是在ROI pooling时大BBox能提取更多的信息。为了解决这个问题，我们尝试使用时序信息对bbox重排序。我们假设邻近帧有相同的物体具有相似的位置和尺寸。</p>
<p>我们提出一些经验性的方法：1）序列选择，2）序列重打分，3）抑制。重复此步骤直到没有剩余序列为止。图1显示了此过程。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515729313541.png" alt="1515729313541"></p>
<h4 id="序列选择"><a href="#序列选择" class="headerlink" title="序列选择"></a>序列选择</h4><p>当IoU超过一定阈值后，第一帧的一个BBox与第二帧的BBox连接，我们首先选择潜在的可能的所有连接。然后尝试找到得分最大的序列。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515729999290.png" alt="1515729999290"></p>
<p>可以通过简单的动态规划算法求解。</p>
<h4 id="序列重打分"><a href="#序列重打分" class="headerlink" title="序列重打分"></a>序列重打分</h4><p>尝试使用average或max函数对序列打分</p>
<h4 id="抑制"><a href="#抑制" class="headerlink" title="抑制"></a>抑制</h4><p>选择后的BBox重候选BBox中移除，并且对IoU超过阈值的BBox抑制。</p>
<h2 id="3-数据集"><a href="#3-数据集" class="headerlink" title="3. 数据集"></a>3. 数据集</h2><p>ImageNet VID数据集，30个类别</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515730390416.png" alt="1515730390416"></p>
<h2 id="4-结果"><a href="#4-结果" class="headerlink" title="4. 结果"></a>4. 结果</h2><h3 id="4-1-RPN和分类器训练细节"><a href="#4-1-RPN和分类器训练细节" class="headerlink" title="4.1 RPN和分类器训练细节"></a>4.1 RPN和分类器训练细节</h3><p>首先，在VID训练集上迭代400K次，Fast R-CNN训练迭代200K次。最后固定卷积层，训练400K次，发现RPN在VID验证集上实现90%的召回率。</p>
<p>对于分类器，我们考虑ZF和VGG16两个网络，ZF在VID的训练集上训练，VGG16在2015DET的训练和验证集上预训练。然后VGG在VID上训练，去掉多余的类别单元，然后固定其他层。</p>
<h3 id="4-2-定量结果"><a href="#4-2-定量结果" class="headerlink" title="4.2 定量结果"></a>4.2 定量结果</h3><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515731237073.png" alt="1515731237073"></p>
<p>NMS表示单帧检测，best是表示每个类别选择其最优的策略，然后求其平均。表2显示检测结果。</p>
<p>图3显示了每个类别的mAP的提升，图4显示了每个类别的提升。可以发现，摩托车、海龟、小熊猫、斑马、羊的提升较大。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515731407526.png" alt="1515731407526"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515731433180.png" alt="1515731433180"></p>
<p>表3显示了VID比赛的结果，我们提交的最好结果是48.7%。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515731904233.png" alt="1515731904233"></p>
<p>T-CNN的方法包含以下技术：1）很强的单帧检测器；2）bbox抑制和传播；3）轨迹&#x2F;tubelet 重打分；4）模型组合。其单帧检测器的mAP可达到67.7%如果仅考虑后面两个技术的提升，我们的方法提升更大（7.3% vs. 6.7%）。</p>
<h3 id="4-3-定性分析"><a href="#4-3-定性分析" class="headerlink" title="4.3 定性分析"></a>4.3 定性分析</h3><p>Seq-NMS可以把一些低得分的物体重新找回，但是也可能带入一些虚警。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515732534734.png" alt="1515732534734"></p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/Seq-NMS/1515732575950.png" alt="1515732575950"></p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Video Object Detection</category>
      </categories>
      <tags>
        <tag>Video Object Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>修改Matlab默认编码格式为UTF-8</title>
    <url>/Tutorial-%E6%95%99%E7%A8%8B/%E4%BF%AE%E6%94%B9Matlab%E9%BB%98%E8%AE%A4%E7%BC%96%E7%A0%81%E6%A0%BC%E5%BC%8F%E4%B8%BAUTF-8/</url>
    <content><![CDATA[<p>输入<code>feature(&#39;lcoal&#39;)</code>查看当前环境编码格式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">           ctype: &#x27;zh_CN.GBK&#x27;</span><br><span class="line">         collate: &#x27;zh_CN.GBK&#x27;</span><br><span class="line">            time: &#x27;zh_CN.GBK&#x27;</span><br><span class="line">         numeric: &#x27;en_US_POSIX.UTF-8&#x27;</span><br><span class="line">        monetary: &#x27;zh_CN.GBK&#x27;</span><br><span class="line">        messages: &#x27;zh_CN.GBK&#x27;</span><br><span class="line">        encoding: &#x27;GBK&#x27;</span><br><span class="line">terminalEncoding: &#x27;GBK&#x27;</span><br><span class="line">     jvmEncoding: &#x27;GBK&#x27;</span><br><span class="line">          status: &#x27;MathWorks locale management system initialized.&#x27;</span><br><span class="line">         warning: &#x27;&#x27;</span><br></pre></td></tr></table></figure>

<p>进入matlab安装路径，打开lcdata.xml文件</p>
<p><code>D:\Program Files\MATLAB\R2014b\bin\lcdata.xml</code></p>
<p>将<code>UTF-8</code>与<code>GBK</code>编码对应</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;encoding name=”UTF-8”&gt;</span><br></pre></td></tr></table></figure>

<p>修改为</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;encoding name=&quot;UTF-8&quot;&gt;</span><br><span class="line">    &lt;encoding_alias name=&quot;utf8&quot;/&gt;</span><br><span class="line">    &lt;encoding_alias name=&quot;GBK&quot;/&gt;</span><br><span class="line">&lt;/encoding&gt;</span><br></pre></td></tr></table></figure>

<p>然后修改</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;locale name=&quot;zh_CN&quot; encoding=&quot;GBK&quot; xpg_name=&quot;zh_CN.GBK&quot;&gt;</span><br></pre></td></tr></table></figure>

<p>为</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;locale name=&quot;zh_CN&quot; encoding=&quot;UTF-8&quot; xpg_name=&quot;zh_CN.UTF-8&quot;&gt;</span><br></pre></td></tr></table></figure>

<p>重启matlab</p>
]]></content>
      <categories>
        <category>Tutorial 教程</category>
      </categories>
      <tags>
        <tag>Matlab</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记 Road detection using convolutional neural networks</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Note-Road-detection-using-convolutional-neural-networks/</url>
    <content><![CDATA[<ul>
<li>会议：European Conference on Artificial Life (ECAL)</li>
<li>机构：Aberystwyth University (英国)</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>解决自动驾驶的问题：使用卷积神经网络预测图像中路面的位置和宽度。</li>
<li>实验了两种网络架构，六种色彩模型</li>
<li>使用机器人Pioneer 3-AT，离线测试5个不同的路段</li>
</ul>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul>
<li>自动驾驶车辆都需要判断前方的路面和非路面区域</li>
<li>当道路线清晰，标准的图像处理+视觉方法可以直接解决</li>
<li>当路面不清晰时就会变得复杂</li>
<li>图像处理应该足够鲁棒，满足动态复杂路面检测的需求</li>
<li>如果能判断前方路面的位置的形状，能够使得问题变简单</li>
</ul>
<span id="more"></span>

<ul>
<li><p>Alvarez[^1]，Thrun[^2] 尝试通过路面颜色分布的差异进行分割，这种方法不依赖道路标识，但是道路的颜色分布不是一直静止的，局部和动态的变化（阴影，水坑，纹理）都可以降低精度。此外，有些颜色通道，路面和非路面的分布基本没差异。</p>
</li>
<li><p>Ososinski[^3] 提出 Adaptive Statistical Colour Based（ASC），在图像上投影体形来定义路面区域。评估路面检测，采用Mahalanobis距离<a href="https://en.wikipedia.org/wiki/Mahalanobis_distance">^wiki</a> 评估在梯形内和外的像素。在多个道路上测试，大部分情况较好，但还是存在识别案例，以及有些具有系统性偏差。依赖前一帧颜色通道，当发生突变时可能存在问题。</p>
<blockquote>
<p>本文主要参考这篇文章的工作框架</p>
</blockquote>
</li>
</ul>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><ul>
<li>Zhou，提出支持向量机分割输入图像。其假设路面的结构路面，为了能够使用不同路面及减少分类错误，其不断加入新帧中的像素，删除旧帧中的像素。但是在复杂场景中该方法失效。</li>
<li>Pomerleau，ALVINN项目，最早使用神经网络控制无人车在室外驾驶。三层前馈网络，输入灰度图像，输出车辆方向命令。网络的训练数据通过一个人在一个模拟道路环境中驾驶生成。之后在户外的一个运行车辆上让驾驶员假设车辆来训练网络。该网络能够在已训练过的路上进行假设，但是不能迁移到新的道路环境。</li>
<li>为了克服上述问题，Jochem，提出MANIAC模块，其包含几个独立的NN，分别对应基准不同的道路类型，然后整合到一个模块中，基于所有NN的输出，最后得出结果。该方法无法覆盖所以道路类型，需要大量NN，而且每加入一个新NN，整个系统需要重新训练。</li>
<li>Shinzato，提出一种多网络方法适应多种路段。首先，每个图像被分为多块，每个块10x10像素，每个块被标记为可行或不可信，然后从block提取特征输入NN，然后综合结果输出预测。NN等方法对训练过的路段较好，但是对其他路段较差。</li>
</ul>
<p>近几年神经网络发展迅速，在检测、识别相关的视觉问题上，卷积神经网络能实现更高的精度。神经网络能获得高层语义特征，对路面检测来说，提取高级语义能够实现更好的鲁棒性。</p>
<ul>
<li>Hadsell，使用CNN实现了野地自动驾驶</li>
<li>Chen，Liu，使用赛车模拟器视频训练CNN，其效果比使用Gabor的baseline方法好。该方法还是主要限制在城市道路场景。</li>
<li>Huval，使用CNN检测高速公路的车道线，训练数据采用大量采集的视频数据。</li>
<li>Badrinarayanan，提出反卷积网络用于图片语义分割</li>
<li>据作者所知，上述方法没有在非城市道路评估，我们评估了Bad等人的方法在各种不同路面上的效果，如图1所示，效果较差。</li>
</ul>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/2017 - Narayan et al. - Road detection using convolutional neural networks/1521976193935.png" width="50%"/>

<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><ul>
<li>两个网络架构</li>
<li>10组道路序列图像，满足多样的道路环境</li>
<li>6个色彩模型（RGB，HSV，YUV，YCbCr，lab，CbCra），看是否会影响CNN的性能</li>
<li>评估方法类似Ososinski，结果显示最好的网络性能与ASC基本一致，在一两个环境下CNN更好</li>
</ul>
<h4 id="Road-Shap"><a href="#Road-Shap" class="headerlink" title="Road Shap"></a>Road Shap</h4><ul>
<li><p>路面模型，梯形，两个可变参数，位置$x$，宽度$w$，如图2</p>
</li>
<li><p>增加高度$h$和角度$\theta$，能够实现更好匹配，但是会增加网络任务的复杂度。</p>
</li>
<li><p>对机器人来说位置x已足够让其保持在路面上，宽度w使其满足不同速度需求，使其保证行动的顺滑，不会产生突变。</p>
<blockquote>
<p>个人觉得这个模型太简单</p>
</blockquote>
</li>
</ul>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/2017 - Narayan et al. - Road detection using convolutional neural networks/1521619448741.png" width="70%"/>

<h4 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h4><ul>
<li>是否更深的网络能得到更精确的结果</li>
<li>作者设计了一个4层的网络LCNN，输入图像是50x50x3的图像，两层网络（见图3中红框），分别有1000和600个神经元，最后两个节点输出参数x和w</li>
<li>平均位置偏差大于20个像素</li>
</ul>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/2017 - Narayan et al. - Road detection using convolutional neural networks/1521621510078.png" width="100%"/>

<ul>
<li>另一个模型采用AlexNet</li>
<li>参数随机初始化</li>
<li>学习率0.0001</li>
<li>dropout：0.2（conv）0.5（fc）</li>
<li>Caffe</li>
<li>使用ImageNet初始化，然后去掉最后两个全连接层，使用随机初始化两个输出节点</li>
<li>迭代30次</li>
</ul>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/2017 - Narayan et al. - Road detection using convolutional neural networks/1521622327058.png">

<h4 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h4><p>自己采集标注了数据集，下载地址：<a href="https://www.aber.ac.uk/en/cs/research/ir/dss/#road-driving">https://www.aber.ac.uk/en/cs/research/ir/dss/#road-driving</a></p>
<h3 id="离线检测"><a href="#离线检测" class="headerlink" title="离线检测"></a>离线检测</h3><p>从6种色彩模型中选择选择每个模型最好的，好的标准是平均像素偏差。可以发现RGB不区分明度和颜色，YUV和YCbCr是相似的颜色模型，也具有相对更低的偏差。AlexNet跨颜色模型的效果更好。但是如果选择和是的色彩模型LCNN也可以与AlexNet比较。</p>
<p>![1521642366208](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/2017">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/2017</a> - Narayan et al. - Road detection using convolutional neural networks&#x2F;1521642366208.png)</p>
<p>选择两个表现最好的ASC的色彩模型（HSV和lab）与LCNN和AlexNet进行比较。比较结果如下面两个图所示。</p>
<p>![1521642885864](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/2017">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/2017</a> - Narayan et al. - Road detection using convolutional neural networks&#x2F;1521642885864.png)</p>
<p>![1521642902451](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/2017">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/2017</a> - Narayan et al. - Road detection using convolutional neural networks&#x2F;1521642902451.png)</p>
<blockquote>
<p>Boxplots是一种统计图标，<a href="https://en.wikipedia.org/wiki/Box_plot">wiki</a>，其关键是理解四分位数，Q1：25%数据小于等于它，Q2：50%数据小于等于它，即中位数，Q3，75%数据小于等于它，四分卫距IQR&#x3D;Q3-Q1。Q3+1.5IQR就是外限，Q1-1.5IQR就是外限。超过外限的则显示出来。</p>
<img src="http://www.physics.csbsju.edu/stats/complex.box.defs.gif" width="50%"/>
</blockquote>
<p>可以发现AlexNet的性能比ASC要好，同时也比LCNN好。Lakeside数据集，NN方法偏右，ACS在KITTI数据集的效果较差，如图7所示。</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/2017 - Narayan et al. - Road detection using convolutional neural networks/1521644566071.png"  width="70%"/>

<h2 id="Robot-Trials"><a href="#Robot-Trials" class="headerlink" title="Robot Trials"></a>Robot Trials</h2><p>在室内进行多种颜色环境的训练，25条线路，5种不同道路环境。如图8所示。</p>
<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/2017 - Narayan et al. - Road detection using convolutional neural networks/1521644918748.png" width="50%"/>

<img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/note/2017 - Narayan et al. - Road detection using convolutional neural networks/1521644977589.png" width="50%"/>

<p>25条线路，只有两个没有成功。Red-Blue路线，Robot很难保持在中间，如图8所示，会往右边的绿色区域偏离。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>CNN可能不容易分解为清晰的操作原则，但是它们不受人主观意见的干扰，导致系统的鲁棒性受限。我们以ASC方法为baseline，AlexNet展现了更好的效果，并且其简化版LCNN也能达到相似等级的精度。我们在室内环境验证了方法的可行性。为了我们会在更广泛的数据上测试，并将在我们的robot移动平台上嵌入一个GPU模块，使其满足运行CNN，实现实时的控制需求。</p>
]]></content>
      <categories>
        <category>Note 笔记</category>
      </categories>
      <tags>
        <tag>Road Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记 Seeing Small Faces from Robust Anchors Perspective</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Note-Seeing-Small-Faces-from-Robust-Anchors-Perspective/</url>
    <content><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文针对基于anchor的人脸检测提出了一种新的anchor设计方法，具有出色的尺度不变性，尤其是针对小脸的检测效果很好。</p>
<p>以往基于anchor的检测器存在的问题：对小尺寸的脸部检测性能降低，比如小于16×16pixels的脸部。</p>
<p>作者的发现：</p>
<ul>
<li><p>当前的anchor设计方法不能保证小脸与anchor boxes之间的高重合度，增加了训练的难度</p>
</li>
<li><p><u>EMO</u></p>
</li>
</ul>
<p>效果：比baseline anchor-based检测器表现要好，在具有挑战性的人脸检测数据集上达到了state-of-the-art，而且运行速度也具有竞争力。</p>
<span id="more"></span>

<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>目前的最先进人脸检测器是一些基于anchor的深度卷积神经网络方法，这些方法受到了主流目标检测器的启发，比如SSD&#x2F;Faster R-CNN&#x2F;R-FCN。</p>
<p>虽然基于anchor的检测器在解决形状和外观不变性的问题上取得了成功，但是它们解决尺度不变性的能力却不让人满意。而现在非常小的脸部在人脸检测领域是极具挑战性的问题之一。</p>
<p>![52127364131](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Face_Anchor/Recall">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Face_Anchor/Recall</a> Rate-Face Scale.png)</p>
<p>从图中（IoU&#x3D;0.5）可以看出：</p>
<ul>
<li><p>大于64×64pixels的脸部几乎能够100%被检测出来，而当脸部的尺寸在小于16×16范围时，召回率有了明显的下降。</p>
</li>
<li><p>其原因在于：在分类和调整anchor boxes之后，这些anchors与那些足够小脸部的重合度还不够高。进一步分析：在训练之前，对每一张脸计算它与重叠anchors的最高IoU，然后根据尺寸将faces进行分组，再对每一组计算平均最高IoU值，如下图：  </p>
<p>![52127432957](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Face_Anchor/Average">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Face_Anchor/Average</a> IoU-Face Scale.png)</p>
<p>召回率与脸部尺寸呈现正相关的关系，而那些与小脸的IoU很低的anchor boxes难以被调整到GT的位置，导致了小脸的召回率很低</p>
</li>
</ul>
<p>本文针对于设计一种新的anchor方案来支持基于anchor的检测器，以获得更好的尺度不变性。如下图所示，我们新提出的anchors相比于以往的anchors与脸部有更高的IoU：</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Face_Anchor/baseline_anchors_vs_ours.png" alt="52127498686"></p>
<p>红色的anchors比黄色的anchors更容易调整到GT的位置，从而使得网络比之前更容易学习如何将anchors回归到GT faces。因此作者深入研究了faces是如何与不同配置的anchors进行匹配的，并且提出了一种新的EMO得分来描述anchors与faces取得高IoU的能力。具体来说：</p>
<ul>
<li>给定一个已知尺寸的face，以及一组anchors，先计算face与这组anchors的最大IoU，<u>假设该face的位置来自于图像上的一个二维分布</u>。然后EMO得分能从理论上解释出“为什么大尺寸的脸部更容易被anchors高重合程度地覆盖，而且分布密集的anchors更有可能覆盖到脸部”</li>
<li>EMO得分启发了几种简单却有效的anchor设计策略，以获得更高的face IoU得分，同时又不会向网络引入过多复杂度。</li>
</ul>
<p>具体来说，作者提出：</p>
<ul>
<li>减少各种网络架构设计anchor的步长</li>
<li>增加不在中心位置的anchors，使anchors的分布更加密集</li>
<li>随机移动face boxes，增加获得更高IoU的概率</li>
<li>将重叠率低的face boxes与多种anchors匹配</li>
</ul>
<p>通过<strong>消融实验</strong>证明了本文提出的方法比以往基于anchors的检测器有很大的提升，在Wider Face，AFW，PASCAL Faces，FDDB上都取得了目前最先进的结果</p>
<p>主要贡献：</p>
<ol>
<li>使用新提出的EMO得分对不同条件下的anchor匹配机制进行了深度分析</li>
<li>为了更高的IoU，尤其针对小脸提出了几种新的anchor设计方法，包括anchor步长的减小；额外平移过的anchors；随机faces平移，并证明了这些方法比baseline有很大提升效果</li>
<li>在Wider Face，AFW，PASCAL Faces，FDDB上达到了目前最先进水平，而且运行速度有竞争力</li>
</ol>
<p>##2. Related Work</p>
<h2 id="3-Expected-Max-Overlapping-Scores"><a href="#3-Expected-Max-Overlapping-Scores" class="headerlink" title="3. Expected Max Overlapping Scores"></a>3. Expected Max Overlapping Scores</h2><h3 id="3-1-Overview-of-Anchor-Based-Detector"><a href="#3-1-Overview-of-Anchor-Based-Detector" class="headerlink" title="3.1. Overview of Anchor-Based Detector"></a>3.1. Overview of Anchor-Based Detector</h3><p>基于anchors的检测方法将anchor boxes进行分类和回归来检测目标。</p>
<p>Anchors是一组多尺寸多宽高比的预定义boxes，规律地平铺在图像平面上。在训练过程中，这些anchors根据IoU重叠率与GT boxes进行匹配，匹配的条件有以下两种情况：</p>
<ul>
<li>该anchor与某GT box的IoU是最高的</li>
<li>该anchor与某GT box的IoU比阈值$T_h$高</li>
</ul>
<blockquote>
<p>如果某个anchor与所有的GT boxes的IoU都低于阈值$T_l$，那么该anchor会被标记为背景</p>
</blockquote>
<p>Anchors会与特征图联系起来，特征图决定了anchors的位置和步长。</p>
<p>特征图是尺寸为$c×h×w$的张量（tensor），$s_F$为特征的步长，即相邻位置之间的距离，$\frac{H}{h}&#x3D;\frac{W}{w}&#x3D;s_F$，anchors将这些位置作为中心点，使用这种联合的表征来计算置信度得分和bounding boxes回归，所以anchor的步长与特征的步长是相等的，$s_A&#x3D;s_F$。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Face_Anchor/EMO.png" alt="52142838391"></p>
<h3 id="3-2-Anchor-Setup-and-Matching"><a href="#3-2-Anchor-Setup-and-Matching" class="headerlink" title="3.2. Anchor Setup and Matching"></a>3.2. Anchor Setup and Matching</h3><p>$S$代表一个预定义的尺度集合，表示anchor boxes的尺度；$R$代表一个预定义的宽高比集合，表示anchors的宽高比；于是不同anchor boxes的总数为$|S×R|&#x3D;|S||R|$，$×$为两个集合的笛卡尔乘积，$|*|$代表集合的基数。</p>
<p>举例：有3种尺度和1种宽高比的anchor boxes如Fig2(a)所示，让$L​$代表一种按规律分布位置的合集，即图中的“+”位置，相邻两个“+”之间的距离为anchor的步长$s_A​$。$A​$代表所有anchors的集合，表示以“+”位置为中心平铺各种尺度和宽高比的anchor boxes，$A&#x3D;S×R×L​$。</p>
<p>Fig2(b)中，假设某face box表示为$B_f$（绿色），与anchor box $B_a$ （红色虚线）有最大的IoU，max IoU此时可由以下等式(1)计算得到：</p>
<p>​                                                          $\max \limits_{α\in{A}}\frac{|B_f\cap{B_a}|}{|B_f\cup{B_a}|}$                                                              (1)</p>
<h3 id="3-3-Computing-the-EMO-Score"><a href="#3-3-Computing-the-EMO-Score" class="headerlink" title="3.3. Computing the EMO Score"></a>3.3. Computing the EMO Score</h3><p>![52151028737](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Face_Anchor/computing">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Face_Anchor/computing</a> EMO score.png)</p>
<p>在图(c)中，用$(x,y)$表示一个face的中心位置，$p(x,y)$表示face位置的概率密度分布函数，且满足$\int_{0}^{H}\int_{0}^{W}p(x,y)dxdy&#x3D;1$，再根据等式(1)，EMO得分的计算方式为等式(2)：</p>
<p>​                                       $EMO&#x3D;\int_{0}^{H}\int_{0}^{W}p(x,y)\max \limits_{α\in{A}}\frac{|B_f\cap{B_a}|}{|B_f\cup{B_a}|}dxdy$                              (2)</p>
<p>Fig4显示了不同尺度face和anchor步长下的EMO得分：</p>
<p>![52152753475](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Face_Anchor/EMO">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Face_Anchor/EMO</a> scores.png)   </p>
<p>由图可见尺度较大的脸部与anchors的IoU更高；当face的尺度固定时，anchors的步长越小，EMO得分越高。   </p>
<h2 id="4-Strategies-of-New-Anchor-Design"><a href="#4-Strategies-of-New-Anchor-Design" class="headerlink" title="4. Strategies of New Anchor Design"></a>4. Strategies of New Anchor Design</h2><p>目标是从提高EMO得分为切入点，针对小脸提高平均IoU。基于之前的分析，作者提出：通过减小anchor的步长来增加平均IoU，同时减小face与anchor中心之间的距离。</p>
<h3 id="4-1-Stride-Reduction-with-Enlarged-Feature-Maps"><a href="#4-1-Stride-Reduction-with-Enlarged-Feature-Maps" class="headerlink" title="4.1. Stride Reduction with Enlarged Feature Maps"></a>4.1. Stride Reduction with Enlarged Feature Maps</h3><p>由于anchor步长与特征步长一致，所以可以通过放大feature map来减小anchor的步长。</p>
<ul>
<li>双线性上采样将feature map扩大</li>
</ul>
<h3 id="4-2-Extra-Shifted-Anchors"><a href="#4-2-Extra-Shifted-Anchors" class="headerlink" title="4.2 Extra Shifted Anchors"></a>4.2 Extra Shifted Anchors</h3><p>通过扩大feature maps的方法来减小anchor strides并没有改变$s_A&#x3D;s_F$这一条件，在本节中作者将会通过增加额外的辅助性anchors来减小$s_A$使得$s_A&lt;s_F$，这些anchors被称为shifted anchors，它们并不以滑窗的中心位置为中心。这种方法可以在不改变feature maps分辨率的情况下提高EMO得分。这些shifted anchors与那些定位在“+”中心的anchors共享特征。</p>
<p>具体来说，假设feature map的步长是$s_F$，两个相邻滑窗位置之间的距离是$s_F$，在Fig6中由黑点标记：</p>
<p>![52153156197](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Face_Anchor/shifted">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Face_Anchor/shifted</a> anchors.png)</p>
<p>在图(a)中，每个位置（黑点）都有一个以其为中心的anchor；当每个位置的右下方添加了额外的anchors（绿色的）时，如图(b)，anchor stride减少到了$s_F&#x2F;\sqrt{2}$；当如图(c)中的蓝色和品红色anchor加入时，anchor stride减少到了$s_F&#x2F;2$。实际上，只需要加入小尺寸的shifted anchors，因为大尺寸的anchors已经能够保证较高的IoU了，这样能够节约计算时间。举例来说，在扩大的feature maps上增加了三种小尺寸的anchor（16×16）对平均IoUs的提升效果如Fig7所示：</p>
<p>![52153220169](<a href="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Face_Anchor/our">http://zavix-image.oss-cn-shenzhen.aliyuncs.com/Paper/CVPR2018-Face_Anchor/our</a> anchor design.png)</p>
<p>可见小尺寸脸部的平均IoU在作者的改进方法下有了显著的提升。</p>
<h3 id="4-3-Face-Shift-Jittering"><a href="#4-3-Face-Shift-Jittering" class="headerlink" title="4.3. Face Shift Jittering"></a>4.3. Face Shift Jittering</h3><p>当对每个face计算EMO得分时，是假设face的中心是服从二维均匀分布的。然而在真实数据集中，每个face的位置是固定的，一些faces距离anchors的中心较近，所以它们更有可能与anchors有更高的IoU；而一些faces本来就距离anchors的中心较远，经常会取得较低的IoU值。为了增加后一种faces与anchors获得较高IoU的概率，作者将这些faces在训练的每次迭代过程中进行随机的平移。</p>
<p>具体来说，在每次迭代过程中，图像平移的偏移量在每次迭代中为$(\delta_x,\delta_y)$，$\delta_x$和$\delta_y$为图像向右和向下平移的像素值，所以图像中所有faces的位置增量都为$(\delta_x,\delta_y)$。偏移量服从离散均匀分布，比如$\delta_x,\delta_y\in{0,1,…,s_A&#x2F;2-1}$。即使用服从离散均匀分布的偏移量（offsets）来处理服从连续均匀分布的face locations。作者将随机偏移量的最大值设置为$s_A&#x2F;2-1$，因为重叠的一个周期距离为$s_A&#x2F;2$。</p>
<h3 id="4-4-Hard-Face-Compensation"><a href="#4-4-Hard-Face-Compensation" class="headerlink" title="4.4. Hard Face Compensation"></a>4.4. Hard Face Compensation</h3><p>由Fig7可见，即使将feature strides减半、对小尺寸的anchors做了平移，那些尺寸非常小脸部的IoU与更大的脸部相比来说还是处于较低的水平。这是因为脸部的尺寸和位置是连续的，而anchor的尺寸和位置是离散的。因此，仍然有一些脸部的尺寸和位置离anchor很远，这些离得很远的脸部难以与anchors匹配上。</p>
<p>因此作者提出了一种补偿性的策略，让很难匹配到anchors的脸部尝试与更多的anchors匹配。</p>
]]></content>
      <categories>
        <category>Note 笔记</category>
      </categories>
      <tags>
        <tag>Anchor</tag>
        <tag>Small Face</tag>
      </tags>
  </entry>
  <entry>
    <title>多用户多账户GitHub的ssh设置</title>
    <url>/uncategorized/%E5%A4%9A%E7%94%A8%E6%88%B7%E5%A4%9A%E8%B4%A6%E6%88%B7GitHub%E7%9A%84ssh%E8%AE%BE%E7%BD%AE/</url>
    <content><![CDATA[<p>我有两个github账号，在同一台电脑上，一个用于个人，一个用于团队。如何使用同时使用ssh对github进行数据推送呢？</p>
<p>场景：</p>
<p>用户名1：jobs</p>
<p>用户名2：musk</p>
<p>仓库1：<a href="mailto:&#103;&#105;&#x74;&#x40;&#103;&#105;&#x74;&#x68;&#x75;&#x62;&#46;&#99;&#111;&#109;">&#103;&#105;&#x74;&#x40;&#103;&#105;&#x74;&#x68;&#x75;&#x62;&#46;&#99;&#111;&#109;</a>:jobs&#x2F;apple.git</p>
<p>仓库2：<a href="mailto:&#x67;&#x69;&#116;&#x40;&#103;&#105;&#x74;&#104;&#117;&#98;&#x2e;&#x63;&#111;&#109;">&#x67;&#x69;&#116;&#x40;&#103;&#105;&#x74;&#104;&#117;&#98;&#x2e;&#x63;&#111;&#109;</a>:musk&#x2F;tesla.git</p>
<p>ssh密钥1：~&#x2F;.ssh&#x2F;jobs</p>
<p>ssh密钥2：~&#x2F;.ssh&#x2F;musk</p>
<p>首先，假设你已经按照官方教程将jobs.pub和musk.pub分别加入两个账户的ssh公钥中。</p>
<p>在~&#x2F;.ssh目录下找到config文件，如果没有就创建一个</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">touch config # 创建config</span><br></pre></td></tr></table></figure>

<p>然后在config中写入如下信息</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 该文件用于配置私钥对应的服务器</span><br><span class="line"># Default github user(first@mail.com)</span><br><span class="line">Host jobs.com</span><br><span class="line"> HostName github.com</span><br><span class="line"> User git</span><br><span class="line"> IdentityFile C:/Users/Administrator/.ssh/jobs</span><br><span class="line"></span><br><span class="line"> # second user(second@mail.com)</span><br><span class="line"> # 建一个github别名，新建的帐号使用这个别名做克隆和更新</span><br><span class="line">Host musk.com</span><br><span class="line"> HostName github.com</span><br><span class="line"> User git</span><br><span class="line"> IdentityFile C:/Users/Administrator/.ssh/musk</span><br></pre></td></tr></table></figure>



<p>测试一下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh -T jobs.com</span><br><span class="line">Hi jobs! You&#x27;ve successfully authenticated, but GitHub does not provide shell access.</span><br><span class="line">ssh -T musk.com</span><br><span class="line">Hi musk! You&#x27;ve successfully authenticated, but GitHub does not provide shell access.</span><br></pre></td></tr></table></figure>


<p>出现以上内容说明设置成功。</p>
<p>最后，如果用github进行发布hexo，deploy可以写为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: jobs.com/jobs.github.io.git</span><br><span class="line">  branch: master</span><br><span class="line">  message: </span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>ssh</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title>Note-DuBox: No-Prior Box Objection Detection via Residual Dual Scale Detectors</title>
    <url>/Note-%E7%AC%94%E8%AE%B0/Object-Detection/Note-DuBox-No-Prior-Box-Objection-Detection-via-Residual-Dual-Scale-Detectors/</url>
    <content><![CDATA[<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>一篇Anchor-free方法，没有预设Box尺寸。</p>
<ol>
<li><p>直接回归Box的四个点，和IoU</p>
</li>
<li><p>FPN的金字塔结构归为两个尺寸</p>
</li>
<li><p>CPRS Loss, 回归和分类的分离导致经常回归正确的框却分类错误，在loss中增加IoU门函数。但是似乎都是这么做的。</p>
</li>
<li><p>各种tirck。见表1.</p>
</li>
</ol>
<p>正片文章虽然结果优秀但是论述没有重点，每个trick都用相同的篇幅叙述，让人抓不到重点。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/picgo/DuBox-fig-1.png"></p>
<span id="more"></span>

<h2 id="Research-Objective"><a href="#Research-Objective" class="headerlink" title="Research Objective"></a>Research Objective</h2><blockquote>
<p>Traditional neural objection detection methods use multi-scale features that allow multiple detectors to perform detecting tasks independently and in parallel. At the same time, with the handling of the prior box, the algorithm’s ability to deal with scale invariance is enhanced. However, too many prior boxes and independent detectors will increase the computational redundancy of the detection algorithm.</p>
</blockquote>
<p>本文是一个anchor-free方法，但是作者是从多尺度的角度来叙述。说预设anchor和多尺度检测会增加计算开销。因此预设anchor尺寸，减少检测器提升精度和效率是目的。</p>
<h2 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h2><p>多尺度检测器中每层都预设很多anchor box，每层都有一个检测器，造成计算冗余。例如RetinaNet中有超过100K个anchor。作者提出两个疑问：</p>
<ol>
<li>使用更少的尺度，实现较好的性能</li>
<li>不使用anchor也回归精确的box</li>
</ol>
<h2 id="Method-s"><a href="#Method-s" class="headerlink" title="Method(s)"></a>Method(s)</h2><h3 id="No-prior-Box-Detection"><a href="#No-prior-Box-Detection" class="headerlink" title="No-prior Box Detection"></a>No-prior Box Detection</h3><p>只使用hook作为预设的位置，而不预设scale和ratio。在anchor-based方法中先生成anchor然后与gt匹配后选择正负样本。没有了anchor如何选择正样本呢？本文给出的方法是设定一个position range $\Theta$。当hook $(i,j)$ 在此范围中，则此hook应回归此目标box。每个位置回归一个box$(Pr_{\Delta w_1},Pr_{\Delta w_2},Pr_{\Delta h_1},Pr_{\Delta h_2})$ 和一个置信度得分$Pr_{cls}$。<br>$$<br>r &#x3D; \sqrt{(x_2-x_1)^2+(y_2-y_1)^2}&#x2F;p<br>$$</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/picgo/DuBox-fig-2.png"></p>
<h3 id="Residual-Dual-Scale-Detectors"><a href="#Residual-Dual-Scale-Detectors" class="headerlink" title="Residual Dual Scale Detectors"></a>Residual Dual Scale Detectors</h3><p>残差双尺度检测器，如图3所示。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/picgo/DuBox-fig-3.png"></p>
<p>首先原来FPN中是5层特征金字塔，作者认为太多，可以减少，因此采用p3+p4组成detector1, p5+p6组成detector2的形式，这样只有两个尺度进行预测。</p>
<h4 id="Refine-module"><a href="#Refine-module" class="headerlink" title="Refine module"></a>Refine module</h4><p>该模块是一个简单的通道和空间注意力模块[^27]，特征图得到一个注意力的加权图后与原特征图相乘。图显示其具体结构，卷积，再反卷积，得到与原特征图相同尺寸的权重图后相乘。作者说该技术能够使得检测器考虑hook周围的特征提升精度。</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/picgo/DuBox-fig-4-2.png"></p>
<h4 id="BBox-bridge-module"><a href="#BBox-bridge-module" class="headerlink" title="BBox bridge module"></a>BBox bridge module</h4><p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/picgo/DuBox-fig-4-1.png"></p>
<p>该模块是将低级和高级检测器连接在一起，低级检测器的特征图通过两个卷积后缩小4倍后跟高级检测器特征相加。这使得高级检测器在之前的基础上训练（感觉有点像两阶段）。</p>
<h3 id="Classification-Regression-Progressive-Strapped-Loss"><a href="#Classification-Regression-Progressive-Strapped-Loss" class="headerlink" title="Classification-Regression Progressive Strapped Loss"></a>Classification-Regression Progressive Strapped Loss</h3><p>使用IoU loss<br>$$<br>L{bbox}&#x3D;-\sum{i,j\in \Theta}ln(IoU(Pr^{i,j}{bbox},Gt^{i,j}{bbox}))<br>$$</p>
<p>分类损失在原来的基础上增加一个IoU门函数$\sigma$<br>$$<br>L_{cls}&#x3D;-\sum_{i,j\in \Theta}CE(Pr^{i,j}<em>{cls},T^{i,j}</em>{cls})\sigma(Pr^{i,j}{bbox},Gt^{i,j}{bbox})-\sum_{i,j\notin \Theta}CE(Pr^{i,j}<em>{cls},T^{i,j}</em>{cls})<br>$$<br>当门函数大于0.5时取1，否则为0 （这里其实跟anchor-based方法中match方法一样）</p>
<h3 id="Reducing-Redundancy-Strategy"><a href="#Reducing-Redundancy-Strategy" class="headerlink" title="Reducing Redundancy Strategy"></a>Reducing Redundancy Strategy</h3><p>一些Trick：</p>
<ol>
<li>detector1中p取10，detector2中p取9，且r最小为3</li>
<li>当目标的面积&#x2F;图片面积大于0.3时，此时不统计此box的回归损失</li>
</ol>
<h3 id="Data-Augment-and-Sample-Balance"><a href="#Data-Augment-and-Sample-Balance" class="headerlink" title="Data Augment and Sample Balance"></a>Data Augment and Sample Balance</h3><h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h4><ul>
<li>随机裁剪</li>
<li>随机photo-metric distortion</li>
<li>翻转</li>
</ul>
<h4 id="批次平衡"><a href="#批次平衡" class="headerlink" title="批次平衡"></a>批次平衡</h4><p>为每个类别建立一个图片列表，每次batch选择一个类别为主要类别，然后选择相关的图片进行训练，这样保证每个类别的有相同的概率被选中。</p>
<h4 id="正负样本平衡"><a href="#正负样本平衡" class="headerlink" title="正负样本平衡"></a>正负样本平衡</h4><p>1:3的比例</p>
<h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>实验结果如表1所示，每个模块和trick对结果的影响</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/picgo/DuBox-table-1.png"></p>
<p>在VOC数据集上的比较</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/picgo/DuBox-table-3.png"></p>
<p>在COCO数据集上的比较</p>
<p><img src="http://zavix-image.oss-cn-shenzhen.aliyuncs.com/picgo/DuBox-table-4.png"></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Anchor-based目前不是唯一的选择，Dubox不预设box也能work。</p>
<p>双尺度比多尺度启发式选择更有效，且高、低级别级联检测。</p>
<p>一些Trick能有效减少冗余结果，提升性能。</p>
<p>[^27]: S.Woo, J. Park, J.-Y. Lee, and I. So Kweon. Cbam: Convolutional block attention module. In The European Conference on Computer Vision (ECCV), September 2018.</p>
]]></content>
      <categories>
        <category>Note 笔记</category>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Anchor Free</tag>
      </tags>
  </entry>
</search>
